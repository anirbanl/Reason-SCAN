{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1. Syntactic Dependency Analysis of gSCAN\n",
    "Based on current outline of gSCAN, we suspect that gSCAN is not grounded syntactically. This prevents models trained with gSCAN have true reasoning powers and true systematicity. All these codes are run against the original codebase released by authors of gSCAN (i.e., no new codes added in gSCAN), which ensures fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '../multimodal_seq2seq_gSCAN/'))\n",
    "import random\n",
    "\n",
    "from seq2seq.gSCAN_dataset import GroundedScanDataset\n",
    "from seq2seq.model import Model\n",
    "from seq2seq.train import train\n",
    "from seq2seq.predict import predict_and_save, predict_single\n",
    "from tqdm import tqdm, trange\n",
    "from GroundedScan.dataset import GroundedScan\n",
    "\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from seq2seq.gSCAN_dataset import Vocabulary\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.DEBUG,\n",
    "                    datefmt=\"%Y-%m-%d %H:%M\")\n",
    "logger = logging.getLogger(__name__)\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "use_cuda = True if torch.cuda.is_available() and not isnotebook() else False\n",
    "device = \"cuda\" if use_cuda else \"cpu\"\n",
    "\n",
    "if use_cuda:\n",
    "    logger.info(\"Using CUDA.\")\n",
    "    logger.info(\"Cuda version: {}\".format(torch.version.cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(flags):\n",
    "    for argument, value in flags.items():\n",
    "        logger.info(\"{}: {}\".format(argument, value))\n",
    "\n",
    "    if not os.path.exists(flags[\"output_directory\"]):\n",
    "        os.mkdir(os.path.join(os.getcwd(), flags[\"output_directory\"]))\n",
    "\n",
    "    if not flags[\"simple_situation_representation\"]:\n",
    "        raise NotImplementedError(\"Full RGB input image not implemented. Implement or set \"\n",
    "                                  \"--simple_situation_representation\")\n",
    "    # Some checks on the flags\n",
    "    if flags[\"generate_vocabularies\"]:\n",
    "        assert flags[\"input_vocab_path\"] and flags[\"target_vocab_path\"], \"Please specify paths to vocabularies to save.\"\n",
    "\n",
    "    if flags[\"test_batch_size\"] > 1:\n",
    "        raise NotImplementedError(\"Test batch size larger than 1 not implemented.\")\n",
    "\n",
    "    data_path = os.path.join(flags[\"data_directory\"], \"dataset.txt\")\n",
    "    if flags[\"mode\"] == \"train\":\n",
    "        train(data_path=data_path, **flags)\n",
    "    elif flags[\"mode\"] == \"test\":\n",
    "        assert os.path.exists(os.path.join(flags[\"data_directory\"], flags[\"input_vocab_path\"])) and os.path.exists(\n",
    "            os.path.join(flags[\"data_directory\"], flags[\"target_vocab_path\"])), \\\n",
    "            \"No vocabs found at {} and {}\".format(flags[\"input_vocab_path\"], flags[\"target_vocab_path\"])\n",
    "        splits = flags[\"splits\"].split(\",\")\n",
    "        for split in splits:\n",
    "            logger.info(\"Loading {} dataset split...\".format(split))\n",
    "            test_set = GroundedScanDataset(data_path, flags[\"data_directory\"], split=split,\n",
    "                                           input_vocabulary_file=flags[\"input_vocab_path\"],\n",
    "                                           target_vocabulary_file=flags[\"target_vocab_path\"], generate_vocabulary=False,\n",
    "                                           k=flags[\"k\"])\n",
    "            test_set.read_dataset(max_examples=None,\n",
    "                                  simple_situation_representation=flags[\"simple_situation_representation\"])\n",
    "            logger.info(\"Done Loading {} dataset split.\".format(flags[\"split\"]))\n",
    "            logger.info(\"  Loaded {} examples.\".format(test_set.num_examples))\n",
    "            logger.info(\"  Input vocabulary size: {}\".format(test_set.input_vocabulary_size))\n",
    "            logger.info(\"  Most common input words: {}\".format(test_set.input_vocabulary.most_common(5)))\n",
    "            logger.info(\"  Output vocabulary size: {}\".format(test_set.target_vocabulary_size))\n",
    "            logger.info(\"  Most common target words: {}\".format(test_set.target_vocabulary.most_common(5)))\n",
    "\n",
    "            model = Model(input_vocabulary_size=test_set.input_vocabulary_size,\n",
    "                          target_vocabulary_size=test_set.target_vocabulary_size,\n",
    "                          num_cnn_channels=test_set.image_channels,\n",
    "                          input_padding_idx=test_set.input_vocabulary.pad_idx,\n",
    "                          target_pad_idx=test_set.target_vocabulary.pad_idx,\n",
    "                          target_eos_idx=test_set.target_vocabulary.eos_idx,\n",
    "                          **flags)\n",
    "            model = model.cuda() if use_cuda else model\n",
    "\n",
    "            # Load model and vocabularies if resuming.\n",
    "            assert os.path.isfile(flags[\"resume_from_file\"]), \"No checkpoint found at {}\".format(flags[\"resume_from_file\"])\n",
    "            logger.info(\"Loading checkpoint from file at '{}'\".format(flags[\"resume_from_file\"]))\n",
    "            model.load_model(flags[\"resume_from_file\"])\n",
    "            start_iteration = model.trained_iterations\n",
    "            logger.info(\"Loaded checkpoint '{}' (iter {})\".format(flags[\"resume_from_file\"], start_iteration))\n",
    "            output_file_name = \"_\".join([split, flags[\"output_file_name\"]])\n",
    "            output_file_path = os.path.join(flags[\"output_directory\"], output_file_name)\n",
    "            output_file = predict_and_save(dataset=test_set, model=model, output_file_path=output_file_path, **flags)\n",
    "            logger.info(\"Saved predictions to {}\".format(output_file))\n",
    "    elif flags[\"mode\"] == \"predict\":\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        raise ValueError(\"Wrong value for parameters --mode ({}).\".format(flags[\"mode\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGroundedScanDataset(object):\n",
    "    \"\"\"\n",
    "    Loads a GroundedScan instance from a specified location.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path_to_data: str, save_directory: str, k: int, split=\"train\", input_vocabulary_file=\"\",\n",
    "                 target_vocabulary_file=\"\", generate_vocabulary=False):\n",
    "        logger.info(\"Initializing dummy gSCAN dataset for adverserial experiments...\")\n",
    "        assert os.path.exists(path_to_data), \"Trying to read a gSCAN dataset from a non-existing file {}.\".format(\n",
    "            path_to_data)\n",
    "        if not generate_vocabulary:\n",
    "            assert os.path.exists(os.path.join(save_directory, input_vocabulary_file)) and os.path.exists(\n",
    "                os.path.join(save_directory, target_vocabulary_file)), \\\n",
    "                \"Trying to load vocabularies from non-existing files.\"\n",
    "        if split == \"test\" and generate_vocabulary:\n",
    "            logger.warning(\"WARNING: generating a vocabulary from the test set.\")\n",
    "        # self.dataset = GroundedScan.load_dataset_from_file(path_to_data, save_directory=save_directory, k=k)\n",
    "        # pre-load just to get the grid size\n",
    "        with open(path_to_data, 'r') as infile:\n",
    "            all_data = json.load(infile)\n",
    "        \n",
    "        self.image_dimensions = all_data[\"grid_size\"]\n",
    "        self.image_channels = 3\n",
    "        self.split = split\n",
    "        self.directory = save_directory\n",
    "\n",
    "        # Keeping track of data.\n",
    "        self._examples = np.array([])\n",
    "        self._input_lengths = np.array([])\n",
    "        self._target_lengths = np.array([])\n",
    "        if generate_vocabulary:\n",
    "            logger.info(\"Generating vocabularies...\")\n",
    "            self.input_vocabulary = Vocabulary()\n",
    "            self.target_vocabulary = Vocabulary()\n",
    "            self.read_vocabularies()\n",
    "            logger.info(\"Done generating vocabularies.\")\n",
    "        else:\n",
    "            logger.info(\"Loading vocabularies...\")\n",
    "            self.input_vocabulary = Vocabulary.load(os.path.join(save_directory, input_vocabulary_file))\n",
    "            self.target_vocabulary = Vocabulary.load(os.path.join(save_directory, target_vocabulary_file))\n",
    "            logger.info(\"Done loading vocabularies.\")\n",
    "\n",
    "    def read_vocabularies(self) -> {}:\n",
    "        \"\"\"\n",
    "        Loop over all examples in the dataset and add the words in them to the vocabularies.\n",
    "        \"\"\"\n",
    "        logger.info(\"Populating vocabulary...\")\n",
    "        for i, example in enumerate(self.dataset.get_examples_with_image(self.split)):\n",
    "            self.input_vocabulary.add_sentence(example[\"input_command\"])\n",
    "            self.target_vocabulary.add_sentence(example[\"target_command\"])\n",
    "\n",
    "    def save_vocabularies(self, input_vocabulary_file: str, target_vocabulary_file: str):\n",
    "        self.input_vocabulary.save(os.path.join(self.directory, input_vocabulary_file))\n",
    "        self.target_vocabulary.save(os.path.join(self.directory, target_vocabulary_file))\n",
    "\n",
    "    def get_vocabulary(self, vocabulary: str) -> Vocabulary:\n",
    "        if vocabulary == \"input\":\n",
    "            vocab = self.input_vocabulary\n",
    "        elif vocabulary == \"target\":\n",
    "            vocab = self.target_vocabulary\n",
    "        else:\n",
    "            raise ValueError(\"Specified unknown vocabulary in sentence_to_array: {}\".format(vocabulary))\n",
    "        return vocab\n",
    "\n",
    "    def shuffle_data(self) -> {}:\n",
    "        \"\"\"\n",
    "        Reorder the data examples and reorder the lengths of the input and target commands accordingly.\n",
    "        \"\"\"\n",
    "        random_permutation = np.random.permutation(len(self._examples))\n",
    "        self._examples = self._examples[random_permutation]\n",
    "        self._target_lengths = self._target_lengths[random_permutation]\n",
    "        self._input_lengths = self._input_lengths[random_permutation]\n",
    "\n",
    "    def get_data_iterator(self, batch_size=10) -> Tuple[torch.Tensor, List[int], torch.Tensor, List[dict],\n",
    "                                                        torch.Tensor, List[int], torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Iterate over batches of example tensors, pad them to the max length in the batch and yield.\n",
    "        :param batch_size: how many examples to put in each batch.\n",
    "        :param auxiliary_task: if true, also batches agent and target positions (flattened, so\n",
    "        agent row * agent columns = agent_position)\n",
    "        :return: tuple of input commands batch, corresponding input lengths, situation image batch,\n",
    "        list of corresponding situation representations, target commands batch and corresponding target lengths.\n",
    "        \"\"\"\n",
    "        for example_i in range(0, len(self._examples), batch_size):\n",
    "            if example_i + batch_size > len(self._examples):\n",
    "                batch_size = len(self._examples) - example_i\n",
    "            examples = self._examples[example_i:example_i + batch_size]\n",
    "            input_lengths = self._input_lengths[example_i:example_i + batch_size]\n",
    "            target_lengths = self._target_lengths[example_i:example_i + batch_size]\n",
    "            max_input_length = np.max(input_lengths)\n",
    "            max_target_length = np.max(target_lengths)\n",
    "            input_batch = []\n",
    "            target_batch = []\n",
    "            situation_batch = []\n",
    "            situation_representation_batch = []\n",
    "            derivation_representation_batch = []\n",
    "            agent_positions_batch = []\n",
    "            target_positions_batch = []\n",
    "            for example in examples:\n",
    "                to_pad_input = max_input_length - example[\"input_tensor\"].size(1)\n",
    "                to_pad_target = max_target_length - example[\"target_tensor\"].size(1)\n",
    "                padded_input = torch.cat([\n",
    "                    example[\"input_tensor\"],\n",
    "                    torch.zeros(int(to_pad_input), dtype=torch.long, device=device).unsqueeze(0)], dim=1)\n",
    "                # padded_input = torch.cat([\n",
    "                #     torch.zeros_like(example[\"input_tensor\"], dtype=torch.long, device=device),\n",
    "                #     torch.zeros(int(to_pad_input), dtype=torch.long, device=devicedevice).unsqueeze(0)], dim=1) # TODO: change back\n",
    "                padded_target = torch.cat([\n",
    "                    example[\"target_tensor\"],\n",
    "                    torch.zeros(int(to_pad_target), dtype=torch.long, device=device).unsqueeze(0)], dim=1)\n",
    "                input_batch.append(padded_input)\n",
    "                target_batch.append(padded_target)\n",
    "                situation_batch.append(example[\"situation_tensor\"])\n",
    "                situation_representation_batch.append(example[\"situation_representation\"])\n",
    "                derivation_representation_batch.append(example[\"derivation_representation\"])\n",
    "                agent_positions_batch.append(example[\"agent_position\"])\n",
    "                target_positions_batch.append(example[\"target_position\"])\n",
    "\n",
    "            yield (torch.cat(input_batch, dim=0), input_lengths, derivation_representation_batch,\n",
    "                   torch.cat(situation_batch, dim=0), situation_representation_batch, torch.cat(target_batch, dim=0),\n",
    "                   target_lengths, torch.cat(agent_positions_batch, dim=0), torch.cat(target_positions_batch, dim=0))\n",
    "\n",
    "    def process(self, example):\n",
    "\n",
    "        empty_example = {}\n",
    "        input_commands = example[\"input_command\"]\n",
    "        target_commands = example[\"target_command\"]\n",
    "        #equivalent_target_commands = example[\"equivalent_target_command\"]\n",
    "        situation_image = example[\"situation_image\"]\n",
    "        self.image_dimensions = situation_image.shape[0]\n",
    "        self.image_channels = situation_image.shape[-1]\n",
    "        situation_repr = example[\"situation_representation\"]\n",
    "        input_array = self.sentence_to_array(input_commands, vocabulary=\"input\")\n",
    "        target_array = self.sentence_to_array(target_commands, vocabulary=\"target\")\n",
    "        #equivalent_target_array = self.sentence_to_array(equivalent_target_commands, vocabulary=\"target\")\n",
    "        empty_example[\"input_tensor\"] = torch.tensor(input_array, dtype=torch.long, device=device).unsqueeze(\n",
    "            dim=0)\n",
    "        empty_example[\"target_tensor\"] = torch.tensor(target_array, dtype=torch.long, device=device).unsqueeze(\n",
    "            dim=0)\n",
    "        #empty_example[\"equivalent_target_tensor\"] = torch.tensor(equivalent_target_array, dtype=torch.long,\n",
    "        #                                                         device=device).unsqueeze(dim=0)\n",
    "        empty_example[\"situation_tensor\"] = torch.tensor(situation_image, dtype=torch.float, device=device\n",
    "                                                         ).unsqueeze(dim=0)\n",
    "        empty_example[\"situation_representation\"] = situation_repr\n",
    "        empty_example[\"derivation_representation\"] = example[\"derivation_representation\"]\n",
    "        empty_example[\"agent_position\"] = torch.tensor(\n",
    "            (int(situation_repr[\"agent_position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "            int(situation_repr[\"agent_position\"][\"column\"]), dtype=torch.long,\n",
    "            device=device).unsqueeze(dim=0)\n",
    "        empty_example[\"target_position\"] = torch.tensor(\n",
    "            (int(situation_repr[\"target_object\"][\"position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "            int(situation_repr[\"target_object\"][\"position\"][\"column\"]),\n",
    "            dtype=torch.long, device=device).unsqueeze(dim=0)\n",
    "        return empty_example\n",
    "\n",
    "    def read_dataset(self, max_examples=None, simple_situation_representation=True) -> {}:\n",
    "        \"\"\"\n",
    "        Loop over the data examples in GroundedScan and convert them to tensors, also save the lengths\n",
    "        for input and target sequences that are needed for padding.\n",
    "        :param max_examples: how many examples to read maximally, read all if None.\n",
    "        :param simple_situation_representation: whether to read the full situation image in RGB or the simplified\n",
    "        smaller representation.\n",
    "        \"\"\"\n",
    "        logger.info(\"Converting dataset to tensors...\")\n",
    "        for i, example in enumerate(self.dataset.get_examples_with_image(self.split, simple_situation_representation)):\n",
    "            if max_examples:\n",
    "                if len(self._examples) > max_examples:\n",
    "                    return\n",
    "            empty_example = {}\n",
    "            input_commands = example[\"input_command\"]\n",
    "            target_commands = example[\"target_command\"]\n",
    "            #equivalent_target_commands = example[\"equivalent_target_command\"]\n",
    "            situation_image = example[\"situation_image\"]\n",
    "            if i == 0:\n",
    "                self.image_dimensions = situation_image.shape[0]\n",
    "                self.image_channels = situation_image.shape[-1]\n",
    "            situation_repr = example[\"situation_representation\"]\n",
    "            input_array = self.sentence_to_array(input_commands, vocabulary=\"input\")\n",
    "            target_array = self.sentence_to_array(target_commands, vocabulary=\"target\")\n",
    "            #equivalent_target_array = self.sentence_to_array(equivalent_target_commands, vocabulary=\"target\")\n",
    "            empty_example[\"input_tensor\"] = torch.tensor(input_array, dtype=torch.long, device=device).unsqueeze(\n",
    "                dim=0)\n",
    "            empty_example[\"target_tensor\"] = torch.tensor(target_array, dtype=torch.long, device=device).unsqueeze(\n",
    "                dim=0)\n",
    "            #empty_example[\"equivalent_target_tensor\"] = torch.tensor(equivalent_target_array, dtype=torch.long,\n",
    "            #                                                         device=device).unsqueeze(dim=0)\n",
    "            empty_example[\"situation_tensor\"] = torch.tensor(situation_image, dtype=torch.float, device=device\n",
    "                                                             ).unsqueeze(dim=0)\n",
    "            empty_example[\"situation_representation\"] = situation_repr\n",
    "            empty_example[\"derivation_representation\"] = example[\"derivation_representation\"]\n",
    "            empty_example[\"agent_position\"] = torch.tensor(\n",
    "                (int(situation_repr[\"agent_position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "                int(situation_repr[\"agent_position\"][\"column\"]), dtype=torch.long,\n",
    "                device=device).unsqueeze(dim=0)\n",
    "            empty_example[\"target_position\"] = torch.tensor(\n",
    "                (int(situation_repr[\"target_object\"][\"position\"][\"row\"]) * int(situation_repr[\"grid_size\"])) +\n",
    "                int(situation_repr[\"target_object\"][\"position\"][\"column\"]),\n",
    "                dtype=torch.long, device=device).unsqueeze(dim=0)\n",
    "            self._input_lengths = np.append(self._input_lengths, [len(input_array)])\n",
    "            self._target_lengths = np.append(self._target_lengths, [len(target_array)])\n",
    "            self._examples = np.append(self._examples, [empty_example])\n",
    "\n",
    "    def sentence_to_array(self, sentence: List[str], vocabulary: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert each string word in a sentence to the corresponding integer from the vocabulary and append\n",
    "        a start-of-sequence and end-of-sequence token.\n",
    "        :param sentence: the sentence in words (strings)\n",
    "        :param vocabulary: whether to use the input or target vocabulary.\n",
    "        :return: the sentence in integers.\n",
    "        \"\"\"\n",
    "        vocab = self.get_vocabulary(vocabulary)\n",
    "        sentence_array = [vocab.sos_idx]\n",
    "        for word in sentence:\n",
    "            sentence_array.append(vocab.word_to_idx(word))\n",
    "        sentence_array.append(vocab.eos_idx)\n",
    "        return sentence_array\n",
    "\n",
    "    def array_to_sentence(self, sentence_array: List[int], vocabulary: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Translate each integer in a sentence array to the corresponding word.\n",
    "        :param sentence_array: array with integers representing words from the vocabulary.\n",
    "        :param vocabulary: whether to use the input or target vocabulary.\n",
    "        :return: the sentence in words.\n",
    "        \"\"\"\n",
    "        vocab = self.get_vocabulary(vocabulary)\n",
    "        return [vocab.idx_to_word(word_idx) for word_idx in sentence_array]\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return len(self._examples)\n",
    "\n",
    "    @property\n",
    "    def input_vocabulary_size(self):\n",
    "        return self.input_vocabulary.size\n",
    "\n",
    "    @property\n",
    "    def target_vocabulary_size(self):\n",
    "        return self.target_vocabulary.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Sequence to sequence models for Grounded SCAN\")\n",
    "\n",
    "# General arguments\n",
    "parser.add_argument(\"--mode\", type=str, default=\"run_tests\", help=\"train, test or predict\", required=True)\n",
    "parser.add_argument(\"--output_directory\", type=str, default=\"output\", help=\"In this directory the models will be \"\n",
    "                                                                           \"saved. Will be created if doesn't exist.\")\n",
    "parser.add_argument(\"--resume_from_file\", type=str, default=\"\", help=\"Full path to previously saved model to load.\")\n",
    "\n",
    "# Data arguments\n",
    "parser.add_argument(\"--split\", type=str, default=\"test\", help=\"Which split to get from Grounded Scan.\")\n",
    "parser.add_argument(\"--data_directory\", type=str, default=\"data/uniform_dataset\", help=\"Path to folder with data.\")\n",
    "parser.add_argument(\"--input_vocab_path\", type=str, default=\"training_input_vocab.txt\",\n",
    "                    help=\"Path to file with input vocabulary as saved by Vocabulary class in gSCAN_dataset.py\")\n",
    "parser.add_argument(\"--target_vocab_path\", type=str, default=\"training_target_vocab.txt\",\n",
    "                    help=\"Path to file with target vocabulary as saved by Vocabulary class in gSCAN_dataset.py\")\n",
    "parser.add_argument(\"--generate_vocabularies\", dest=\"generate_vocabularies\", default=False, action=\"store_true\",\n",
    "                    help=\"Whether to generate vocabularies based on the data.\")\n",
    "parser.add_argument(\"--load_vocabularies\", dest=\"generate_vocabularies\", default=True, action=\"store_false\",\n",
    "                    help=\"Whether to use previously saved vocabularies.\")\n",
    "\n",
    "# Training and learning arguments\n",
    "parser.add_argument(\"--training_batch_size\", type=int, default=50)\n",
    "parser.add_argument(\"--k\", type=int, default=0, help=\"How many examples from the adverb_1 split to move to train.\")\n",
    "parser.add_argument(\"--test_batch_size\", type=int, default=1, help=\"Currently only 1 supported due to decoder.\")\n",
    "parser.add_argument(\"--max_training_examples\", type=int, default=None, help=\"If None all are used.\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001)\n",
    "parser.add_argument('--lr_decay', type=float, default=0.9)\n",
    "parser.add_argument('--lr_decay_steps', type=float, default=20000)\n",
    "parser.add_argument(\"--adam_beta_1\", type=float, default=0.9)\n",
    "parser.add_argument(\"--adam_beta_2\", type=float, default=0.999)\n",
    "parser.add_argument(\"--print_every\", type=int, default=100)\n",
    "parser.add_argument(\"--evaluate_every\", type=int, default=1000, help=\"How often to evaluate the model by decoding the \"\n",
    "                                                                     \"test set (without teacher forcing).\")\n",
    "parser.add_argument(\"--max_training_iterations\", type=int, default=100000)\n",
    "parser.add_argument(\"--weight_target_loss\", type=float, default=0.3, help=\"Only used if --auxiliary_task set.\")\n",
    "\n",
    "# Testing and predicting arguments\n",
    "parser.add_argument(\"--max_testing_examples\", type=int, default=None)\n",
    "parser.add_argument(\"--splits\", type=str, default=\"test\", help=\"comma-separated list of splits to predict for.\")\n",
    "parser.add_argument(\"--max_decoding_steps\", type=int, default=30, help=\"After 30 decoding steps, the decoding process \"\n",
    "                                                                       \"is stopped regardless of whether an EOS token \"\n",
    "                                                                       \"was generated.\")\n",
    "parser.add_argument(\"--output_file_name\", type=str, default=\"predict.json\")\n",
    "\n",
    "# Situation Encoder arguments\n",
    "parser.add_argument(\"--simple_situation_representation\", dest=\"simple_situation_representation\", default=True,\n",
    "                    action=\"store_true\", help=\"Represent the situation with 1 vector per grid cell. \"\n",
    "                                              \"For more information, read grounded SCAN documentation.\")\n",
    "parser.add_argument(\"--image_situation_representation\", dest=\"simple_situation_representation\", default=False,\n",
    "                    action=\"store_false\", help=\"Represent the situation with the full gridworld RGB image. \"\n",
    "                                               \"For more information, read grounded SCAN documentation.\")\n",
    "parser.add_argument(\"--cnn_hidden_num_channels\", type=int, default=50)\n",
    "parser.add_argument(\"--cnn_kernel_size\", type=int, default=7, help=\"Size of the largest filter in the world state \"\n",
    "                                                                   \"model.\")\n",
    "parser.add_argument(\"--cnn_dropout_p\", type=float, default=0.1, help=\"Dropout applied to the output features of the \"\n",
    "                                                                     \"world state model.\")\n",
    "parser.add_argument(\"--auxiliary_task\", dest=\"auxiliary_task\", default=False, action=\"store_true\",\n",
    "                    help=\"If set to true, the model predicts the target location from the joint attention over the \"\n",
    "                         \"input instruction and world state.\")\n",
    "parser.add_argument(\"--no_auxiliary_task\", dest=\"auxiliary_task\", default=True, action=\"store_false\")\n",
    "\n",
    "# Command Encoder arguments\n",
    "parser.add_argument(\"--embedding_dimension\", type=int, default=25)\n",
    "parser.add_argument(\"--num_encoder_layers\", type=int, default=1)\n",
    "parser.add_argument(\"--encoder_hidden_size\", type=int, default=100)\n",
    "parser.add_argument(\"--encoder_dropout_p\", type=float, default=0.3, help=\"Dropout on instruction embeddings and LSTM.\")\n",
    "parser.add_argument(\"--encoder_bidirectional\", dest=\"encoder_bidirectional\", default=True, action=\"store_true\")\n",
    "parser.add_argument(\"--encoder_unidirectional\", dest=\"encoder_bidirectional\", default=False, action=\"store_false\")\n",
    "\n",
    "# Decoder arguments\n",
    "parser.add_argument(\"--num_decoder_layers\", type=int, default=1)\n",
    "parser.add_argument(\"--attention_type\", type=str, default='bahdanau', choices=['bahdanau', 'luong'],\n",
    "                    help=\"Luong not properly implemented.\")\n",
    "parser.add_argument(\"--decoder_dropout_p\", type=float, default=0.3, help=\"Dropout on decoder embedding and LSTM.\")\n",
    "parser.add_argument(\"--decoder_hidden_size\", type=int, default=100)\n",
    "parser.add_argument(\"--conditional_attention\", dest=\"conditional_attention\", default=True, action=\"store_true\",\n",
    "                    help=\"If set to true joint attention over the world state conditioned on the input instruction is\"\n",
    "                         \" used.\")\n",
    "parser.add_argument(\"--no_conditional_attention\", dest=\"conditional_attention\", default=False, action=\"store_false\")\n",
    "\n",
    "# Other arguments\n",
    "parser.add_argument(\"--seed\", type=int, default=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train a model on demo data to make sure the pipeline is running correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_flags = vars(parser.parse_args(args=['--mode', 'train', \n",
    "                                           '--data_directory', '../multimodal_seq2seq_gSCAN/data/demo_dataset/', \n",
    "                                           '--output_directory', './syntactic_dependency-results/', \n",
    "                                           '--attention_type', 'bahdanau', \n",
    "                                           '--max_training_iterations', '1000',\n",
    "                                           '--generate_vocabularies']))\n",
    "input_flags\n",
    "# main(flags=input_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data_path = os.path.join('../multimodal_seq2seq_gSCAN/data/demo_dataset/', \n",
    "                              \"dataset.txt\")\n",
    "preprocessor = DummyGroundedScanDataset(demo_data_path, '../multimodal_seq2seq_gSCAN/data/demo_dataset/', \n",
    "                                        input_vocabulary_file=\"training_input_vocab.txt\", \n",
    "                                        target_vocabulary_file=\"training_target_vocab.txt\",\n",
    "                                        generate_vocabulary=False,\n",
    "                                        k=0)\n",
    "# get a single example\n",
    "demo_dataset = GroundedScan.load_dataset_from_file(demo_data_path, save_directory='./syntactic_dependency-results/', k=0)\n",
    "raw_example = None\n",
    "for _, example in enumerate(demo_dataset.get_examples_with_image('train', True)):\n",
    "    raw_example = example\n",
    "    break\n",
    "single_example = preprocessor.process(raw_example)\n",
    "# create the model\n",
    "model = Model(input_vocabulary_size=preprocessor.input_vocabulary_size,\n",
    "              target_vocabulary_size=preprocessor.target_vocabulary_size,\n",
    "              num_cnn_channels=preprocessor.image_channels,\n",
    "              input_padding_idx=preprocessor.input_vocabulary.pad_idx,\n",
    "              target_pad_idx=preprocessor.target_vocabulary.pad_idx,\n",
    "              target_eos_idx=preprocessor.target_vocabulary.eos_idx,\n",
    "              **input_flags)\n",
    "model = model.cuda() if use_cuda else model\n",
    "_ = model.load_model(\"./syntactic_dependency-results/model_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: ['a', 'circle', 'red', 'to', 'walk']\n",
      "true: ['turn right', 'walk']\n",
      "pred: ['turn right', 'walk']\n"
     ]
    }
   ],
   "source": [
    "# regular input\n",
    "print(\"in:\", raw_example['input_command'])\n",
    "print(\"true:\", raw_example['target_command'])\n",
    "# to feed this example, we need to modify the current pipeline a little bit\n",
    "single_example = preprocessor.process(raw_example)\n",
    "output = predict_single(single_example, model=model, \n",
    "                        max_decoding_steps=30, \n",
    "                        pad_idx=preprocessor.target_vocabulary.pad_idx, \n",
    "                        sos_idx=preprocessor.target_vocabulary.sos_idx,\n",
    "                        eos_idx=preprocessor.target_vocabulary.eos_idx, \n",
    "                        device=device)\n",
    "pred_command = preprocessor.array_to_sentence(output[3], vocabulary=\"target\")\n",
    "print(\"pred:\", raw_example['target_command'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: ['to', 'a', 'circle', 'walk', 'red']\n",
      "true: ['turn right', 'walk']\n",
      "pred: ['turn right', 'walk']\n"
     ]
    }
   ],
   "source": [
    "# corrupted input\n",
    "def corrupt_example(raw_example):\n",
    "    random.shuffle(raw_example['input_command'])\n",
    "    return raw_example\n",
    "corrupt_example = corrupt_example(raw_example)\n",
    "print(\"in:\", corrupt_example['input_command'])\n",
    "print(\"true:\", corrupt_example['target_command'])\n",
    "# to feed this example, we need to modify the current pipeline a little bit\n",
    "single_example = preprocessor.process(corrupt_example)\n",
    "output = predict_single(single_example, model=model, \n",
    "                        max_decoding_steps=30, \n",
    "                        pad_idx=preprocessor.target_vocabulary.pad_idx, \n",
    "                        sos_idx=preprocessor.target_vocabulary.sos_idx,\n",
    "                        eos_idx=preprocessor.target_vocabulary.eos_idx, \n",
    "                        device=device)\n",
    "pred_command = preprocessor.array_to_sentence(output[3], vocabulary=\"target\")\n",
    "print(\"pred:\", raw_example['target_command'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: ['a', 'to', 'circle', 'walk', 'red']\n",
      "true: ['turn right', 'walk']\n",
      "pred: ['turn right', 'walk']\n"
     ]
    }
   ],
   "source": [
    "# corrupted input\n",
    "def corrupt_example(raw_example):\n",
    "    random.shuffle(raw_example['input_command'])\n",
    "    return raw_example\n",
    "corrupt_example = corrupt_example(raw_example)\n",
    "print(\"in:\", corrupt_example['input_command'])\n",
    "print(\"true:\", corrupt_example['target_command'])\n",
    "# to feed this example, we need to modify the current pipeline a little bit\n",
    "single_example = preprocessor.process(corrupt_example)\n",
    "output = predict_single(single_example, model=model, \n",
    "                        max_decoding_steps=30, \n",
    "                        pad_idx=preprocessor.target_vocabulary.pad_idx, \n",
    "                        sos_idx=preprocessor.target_vocabulary.sos_idx,\n",
    "                        eos_idx=preprocessor.target_vocabulary.eos_idx, \n",
    "                        device=device)\n",
    "pred_command = preprocessor.array_to_sentence(output[3], vocabulary=\"target\")\n",
    "print(\"pred:\", raw_example['target_command'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wow!** even with very overfitted example (trained with very small demo), we can see that if we corrupt the input, the output command stays the same! this is a strong evidence that current gSCAN does not push neural models to acquire grounding skills over world knowledge at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

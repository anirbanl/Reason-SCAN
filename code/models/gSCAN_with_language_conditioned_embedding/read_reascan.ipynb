{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "flags = {\n",
    "    \"dataset_path\": \"../../../data-files/ReaSCAN-compositional/data-compositional-splits.txt\",\n",
    "    \"output_file\": \"../../../data-files/ReaSCAN-compositional/parsed_dataset.txt\",\n",
    "    \"save_data\": True\n",
    "}\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.DEBUG,\n",
    "                    datefmt=\"%Y-%m-%d %H:%M\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sparse_situation(situation_representation: dict, grid_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Each grid cell in a situation is fully specified by a vector:\n",
    "    [_ _ _ _ _ _ _   _       _      _       _   _ _ _ _]\n",
    "     1 2 3 4 r g b circle square cylinder agent E S W N\n",
    "     _______ _____ ______________________ _____ _______\n",
    "       size  color        shape           agent agent dir.\n",
    "    :param situation_representation: data from dataset.txt at key \"situation\".\n",
    "    :param grid_size: int determining row/column number.\n",
    "    :return: grid to be parsed by computational models.\n",
    "    \"\"\"\n",
    "    num_object_attributes = len([int(bit) for bit in situation_representation[\"target_object\"][\"vector\"]])\n",
    "    # Object representation + agent bit + agent direction bits (see docstring).\n",
    "    num_grid_channels = num_object_attributes + 1 + 4\n",
    "\n",
    "    # Initialize the grid.\n",
    "    grid = np.zeros([grid_size, grid_size, num_grid_channels], dtype=int)\n",
    "\n",
    "    # Place the agent.\n",
    "    agent_row = int(situation_representation[\"agent_position\"][\"row\"])\n",
    "    agent_column = int(situation_representation[\"agent_position\"][\"column\"])\n",
    "    agent_direction = int(situation_representation[\"agent_direction\"])\n",
    "    agent_representation = np.zeros([num_grid_channels], dtype=np.int)\n",
    "    agent_representation[-5] = 1\n",
    "    agent_representation[-4 + agent_direction] = 1\n",
    "    grid[agent_row, agent_column, :] = agent_representation\n",
    "\n",
    "    # Loop over the objects in the world and place them.\n",
    "    for placed_object in situation_representation[\"placed_objects\"].values():\n",
    "        object_vector = np.array([int(bit) for bit in placed_object[\"vector\"]], dtype=np.int)\n",
    "        object_row = int(placed_object[\"position\"][\"row\"])\n",
    "        object_column = int(placed_object[\"position\"][\"column\"])\n",
    "        grid[object_row, object_column, :] = np.concatenate([object_vector, np.zeros([5], dtype=np.int)])\n",
    "    return grid\n",
    "\n",
    "\n",
    "def data_loader(file_path: str) -> Dict[str, Union[List[str], np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Loads grounded SCAN dataset from text file and ..\n",
    "    :param file_path: Full path to file containing dataset (dataset.txt)\n",
    "    :returns: dict with as keys all splits and values list of example dicts with input, target and situation.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as infile:\n",
    "        all_data = json.load(infile)\n",
    "        grid_size = int(all_data[\"grid_size\"])\n",
    "        splits = list(all_data[\"examples\"].keys())\n",
    "        logger.info(\"Found data splits: {}\".format(splits))\n",
    "        loaded_data = {}\n",
    "        for split in splits:\n",
    "            loaded_data[split] = []\n",
    "            logger.info(\"Now loading data for split: {}\".format(split))\n",
    "            for data_example in all_data[\"examples\"][split]:\n",
    "                input_command = data_example[\"command\"].split(',')\n",
    "                target_command = data_example[\"target_commands\"].split(',')\n",
    "                situation = parse_sparse_situation(situation_representation=data_example[\"situation\"],\n",
    "                                                   grid_size=grid_size)\n",
    "                loaded_data[split].append({\"input\": input_command,\n",
    "                                           \"target\": target_command,\n",
    "                                           \"situation\": situation.tolist()})  # .tolist() necessary to be serializable\n",
    "            logger.info(\"Loaded {} examples in split {}.\\n\".format(len(loaded_data[split]), split))\n",
    "    return loaded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-18 01:50 Found data splits: ['train', 'novel_inside_of_as_yellow_box', 'gscan_yellow_square_command_target_only', 'gscan_yellow_square_command', 'gscan_red_box_visual', 'novel_green_circle_box_coexist_box_shape', 'gscan_small_cylinder_command_target_only', 'novel_yellow_square_blue_circle_coexist_shape', 'novel_same_shape_is_inside_coexist_relation', 'few_shot_single_clause_logic', 'dev', 'test']\n",
      "2021-05-18 01:50 Now loading data for split: train\n",
      "2021-05-18 01:52 Loaded 315421 examples in split train.\n",
      "\n",
      "2021-05-18 01:52 Now loading data for split: novel_inside_of_as_yellow_box\n",
      "2021-05-18 01:52 Loaded 15950 examples in split novel_inside_of_as_yellow_box.\n",
      "\n",
      "2021-05-18 01:52 Now loading data for split: gscan_yellow_square_command_target_only\n",
      "2021-05-18 01:52 Loaded 21801 examples in split gscan_yellow_square_command_target_only.\n",
      "\n",
      "2021-05-18 01:52 Now loading data for split: gscan_yellow_square_command\n",
      "2021-05-18 01:52 Loaded 76979 examples in split gscan_yellow_square_command.\n",
      "\n",
      "2021-05-18 01:52 Now loading data for split: gscan_red_box_visual\n",
      "2021-05-18 01:52 Loaded 52433 examples in split gscan_red_box_visual.\n",
      "\n",
      "2021-05-18 01:52 Now loading data for split: novel_green_circle_box_coexist_box_shape\n",
      "2021-05-18 01:52 Loaded 13700 examples in split novel_green_circle_box_coexist_box_shape.\n",
      "\n",
      "2021-05-18 01:52 Now loading data for split: gscan_small_cylinder_command_target_only\n",
      "2021-05-18 01:52 Loaded 31867 examples in split gscan_small_cylinder_command_target_only.\n",
      "\n",
      "2021-05-18 01:52 Now loading data for split: novel_yellow_square_blue_circle_coexist_shape\n",
      "2021-05-18 01:52 Loaded 8450 examples in split novel_yellow_square_blue_circle_coexist_shape.\n",
      "\n",
      "2021-05-18 01:52 Now loading data for split: novel_same_shape_is_inside_coexist_relation\n",
      "2021-05-18 01:52 Loaded 4698 examples in split novel_same_shape_is_inside_coexist_relation.\n",
      "\n",
      "2021-05-18 01:52 Now loading data for split: few_shot_single_clause_logic\n",
      "2021-05-18 01:52 Loaded 49511 examples in split few_shot_single_clause_logic.\n",
      "\n",
      "2021-05-18 01:52 Now loading data for split: dev\n",
      "2021-05-18 01:52 Loaded 19282 examples in split dev.\n",
      "\n",
      "2021-05-18 01:52 Now loading data for split: test\n",
      "2021-05-18 01:52 Loaded 3716 examples in split test.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data_loader(flags[\"dataset_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flags[\"save_data\"]:\n",
    "    with open(flags[\"output_file\"], 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dt in data.items():\n",
    "    with open('parsed_dataset/' + split + '.json', 'w') as f:\n",
    "        for line in dt:\n",
    "            f.write(json.dumps(line) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

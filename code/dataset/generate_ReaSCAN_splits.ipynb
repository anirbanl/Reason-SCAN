{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrips for generating splits\n",
    "This script assums you have the main ReaSCAN generated by the generate_ReaSCAN.py script. After that, you can use this file to generate/extrapolate different splits. In the future, we may consolidate two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, OrderedDict\n",
    "import os\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "if isnotebook():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO,\n",
    "                    datefmt=\"%Y-%m-%d %H:%M\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from world import *\n",
    "from vocabulary import Vocabulary as ReaSCANVocabulary\n",
    "from object_vocabulary import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1: gSCAN Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 17:24 Reading dataset from file: ../../data-files-updated/ReaSCAN-compositional-p1/data-train.txt...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "121500"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p1/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p1_path_to_data}...\")\n",
    "p1_data_json = json.load(open(p1_path_to_data, \"r\"))\n",
    "\n",
    "p1_all_fake_train = p1_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p1_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p1_id_example_map = OrderedDict({})\n",
    "p1_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p1_data_json[\"examples\"][\"train\"]:\n",
    "    p1_id_example_map[index] = example\n",
    "    p1_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_splits_distribution = OrderedDict({})\n",
    "p1_splits_assignment = OrderedDict({})\n",
    "for index, splits in p1_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p1_splits_distribution.keys():\n",
    "            p1_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p1_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p1_splits_assignment:\n",
    "            p1_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p1_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p1_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p1_all_fake_train)*0.052)\n",
    "p1_all_example_id = p1_splits_assignment[\"train\"]\n",
    "random.shuffle(p1_all_example_id)\n",
    "p1_train_example_id = p1_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p1_dev_example_id = p1_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p1_test_example_id = p1_all_example_id[-gscan_dev_size:]\n",
    "p1_splits_assignment[\"train\"] = p1_train_example_id\n",
    "p1_splits_assignment[\"dev\"] = p1_dev_example_id\n",
    "p1_splits_assignment[\"test\"] = p1_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train split, we have 113967 examples.\n",
      "for dev split, we have 6318 examples.\n",
      "for test split, we have 1215 examples.\n"
     ]
    }
   ],
   "source": [
    "for split, all_ids in p1_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p1_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p1_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p1_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p1/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p1_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P2: Single Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 17:26 Reading dataset from file: ../../data-files-updated/ReaSCAN-compositional-p2/data-train.txt...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "363523"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p2/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p2_path_to_data}...\")\n",
    "p2_data_json = json.load(open(p2_path_to_data, \"r\"))\n",
    "\n",
    "p2_all_fake_train = p2_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p2_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p2_id_example_map = OrderedDict({})\n",
    "p2_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p2_data_json[\"examples\"][\"train\"]:\n",
    "    p2_id_example_map[index] = example\n",
    "    p2_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_splits_distribution = OrderedDict({})\n",
    "p2_splits_assignment = OrderedDict({})\n",
    "for index, splits in p2_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p2_splits_distribution.keys():\n",
    "            p2_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p2_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p2_splits_assignment:\n",
    "            p2_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p2_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p2_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p2_all_fake_train)*0.052)\n",
    "p2_all_example_id = p2_splits_assignment[\"train\"]\n",
    "random.shuffle(p2_all_example_id)\n",
    "p2_train_example_id = p2_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p2_dev_example_id = p2_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p2_test_example_id = p2_all_example_id[-gscan_dev_size:]\n",
    "p2_splits_assignment[\"train\"] = p2_train_example_id\n",
    "p2_splits_assignment[\"dev\"] = p2_dev_example_id\n",
    "p2_splits_assignment[\"test\"] = p2_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train split, we have 340985 examples.\n",
      "for dev split, we have 18903 examples.\n",
      "for test split, we have 3635 examples.\n"
     ]
    }
   ],
   "source": [
    "for split, all_ids in p2_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p2_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p2_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p2_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p2/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p2_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3: Double Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 17:33 Reading dataset from file: ../../data-files-updated/ReaSCAN-compositional-p3/data-train.txt...\n"
     ]
    }
   ],
   "source": [
    "p3_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p3/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_path_to_data}...\")\n",
    "p3_data_json = json.load(open(p3_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585963"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3_all_fake_train = p3_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p3_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p3_id_example_map = OrderedDict({})\n",
    "p3_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p3_data_json[\"examples\"][\"train\"]:\n",
    "    p3_id_example_map[index] = example\n",
    "    p3_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_splits_distribution = OrderedDict({})\n",
    "p3_splits_assignment = OrderedDict({})\n",
    "for index, splits in p3_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p3_splits_distribution.keys():\n",
    "            p3_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p3_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p3_splits_assignment:\n",
    "            p3_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p3_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p3_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p3_all_fake_train)*0.052)\n",
    "p3_all_example_id = p3_splits_assignment[\"train\"]\n",
    "random.shuffle(p3_all_example_id)\n",
    "p3_train_example_id = p3_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p3_dev_example_id = p3_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p3_test_example_id = p3_all_example_id[-gscan_dev_size:]\n",
    "p3_splits_assignment[\"train\"] = p3_train_example_id\n",
    "p3_splits_assignment[\"dev\"] = p3_dev_example_id\n",
    "p3_splits_assignment[\"test\"] = p3_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train split, we have 549634 examples.\n",
      "for dev split, we have 30470 examples.\n",
      "for test split, we have 5859 examples.\n"
     ]
    }
   ],
   "source": [
    "for split, all_ids in p3_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p3_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p3_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p3_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p3/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p3_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3: Double Clause (combing with P3 sharding, this section is shared across all patterns as well!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "pattern = \"p4\"\n",
    "special_condition = \"\"\n",
    "if special_condition != \"\":\n",
    "    prefix = f\"{pattern}-{special_condition}\"\n",
    "else:\n",
    "    prefix = pattern\n",
    "sharding_dir = f\"../../data-files-{prefix}/\"\n",
    "if pattern == \"p3\":\n",
    "    upper_limit = 3375\n",
    "elif pattern == \"p2\":\n",
    "    upper_limit = 2025\n",
    "elif pattern == \"p1\":\n",
    "    upper_limit = 675\n",
    "elif pattern == \"p4\":\n",
    "    upper_limit = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobid=45, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-45/data-train.txt\n",
      "jobid=32, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-32/data-train.txt\n",
      "jobid=38, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-38/data-train.txt\n",
      "jobid=19, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-19/data-train.txt\n",
      "jobid=13, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-13/data-train.txt\n",
      "jobid=3, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-3/data-train.txt\n",
      "jobid=21, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-21/data-train.txt\n",
      "jobid=9, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-9/data-train.txt\n",
      "jobid=4, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-4/data-train.txt\n",
      "jobid=26, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-26/data-train.txt\n",
      "jobid=14, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-14/data-train.txt\n",
      "jobid=35, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-35/data-train.txt\n",
      "jobid=42, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-42/data-train.txt\n",
      "jobid=48, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-48/data-train.txt\n",
      "jobid=31, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-31/data-train.txt\n",
      "jobid=46, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-46/data-train.txt\n",
      "jobid=10, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-10/data-train.txt\n",
      "jobid=22, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-22/data-train.txt\n",
      "jobid=0, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-0/data-train.txt\n",
      "jobid=28, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-28/data-train.txt\n",
      "jobid=25, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-25/data-train.txt\n",
      "jobid=7, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-7/data-train.txt\n",
      "jobid=17, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-17/data-train.txt\n",
      "jobid=41, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-41/data-train.txt\n",
      "jobid=36, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-36/data-train.txt\n",
      "jobid=49, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-49/data-train.txt\n",
      "jobid=43, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-43/data-train.txt\n",
      "jobid=34, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-34/data-train.txt\n",
      "jobid=27, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-27/data-train.txt\n",
      "jobid=5, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-5/data-train.txt\n",
      "jobid=15, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-15/data-train.txt\n",
      "jobid=12, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-12/data-train.txt\n",
      "jobid=18, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-18/data-train.txt\n",
      "jobid=8, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-8/data-train.txt\n",
      "jobid=20, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-20/data-train.txt\n",
      "jobid=2, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-2/data-train.txt\n",
      "jobid=39, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-39/data-train.txt\n",
      "jobid=33, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-33/data-train.txt\n",
      "jobid=44, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-44/data-train.txt\n",
      "jobid=37, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-37/data-train.txt\n",
      "jobid=40, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-40/data-train.txt\n",
      "jobid=6, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-6/data-train.txt\n",
      "jobid=24, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-24/data-train.txt\n",
      "jobid=16, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-16/data-train.txt\n",
      "jobid=11, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-11/data-train.txt\n",
      "jobid=29, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-29/data-train.txt\n",
      "jobid=1, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-1/data-train.txt\n",
      "jobid=23, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-23/data-train.txt\n",
      "jobid=47, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-47/data-train.txt\n",
      "jobid=30, status=complete=True\n",
      "scanning for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-30/data-train.txt\n"
     ]
    }
   ],
   "source": [
    "unique_command = set([])\n",
    "for subdir, dirs, files in os.walk(sharding_dir):\n",
    "    if \"jobid\" in subdir:\n",
    "        \n",
    "        # Completeness check!\n",
    "        logging_file = os.path.join(subdir, \"generator.log\")\n",
    "        with open(logging_file) as f:\n",
    "            content = f.readlines()\n",
    "        # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "        content = [x.strip() for x in content]\n",
    "        completed = False\n",
    "        for c in content:\n",
    "            if \"==FINISH==\" in c:\n",
    "                completed = True\n",
    "                break\n",
    "        jobid = logging_file.split(\"/\")[-2].split(\"-\")[-1]\n",
    "        print(f\"jobid={jobid}, status=complete={completed}\")\n",
    "        if not completed:\n",
    "            break\n",
    "        \n",
    "        # Uniqueness check!\n",
    "        data_file_path = os.path.join(subdir, \"data-train.txt\")\n",
    "        print(f\"scanning for file: {data_file_path}\")\n",
    "        data_file = json.load(open(data_file_path, \"r\"))\n",
    "        for example in data_file[\"examples\"][\"train\"]:\n",
    "            command_split = re.split(',a,|,the,', example['command'])\n",
    "            command_mono = \",\".join(command_split)\n",
    "            unique_command.add(command_mono)\n",
    "assert len(unique_command) > upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4484"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-45/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-32/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-38/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-19/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-13/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-3/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-21/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-9/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-4/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-26/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-14/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-35/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-42/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-48/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-31/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-46/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-10/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-22/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-0/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-28/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-25/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-7/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-17/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-41/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-36/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-49/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-43/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-34/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-27/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-5/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-15/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-12/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-18/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-8/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-20/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-2/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-39/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-33/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-44/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-37/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-40/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-6/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-24/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-16/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-11/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-29/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-1/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-23/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-47/data-train.txt\n",
      "Collecting for file: ../../data-files-p4/ReaSCAN-compositional-p4-jobid-30/data-train.txt\n"
     ]
    }
   ],
   "source": [
    "shared_example_combined = {}\n",
    "per_command_mono_count = {}\n",
    "for subdir, dirs, files in os.walk(sharding_dir):\n",
    "    if \"jobid\" in subdir:\n",
    "        data_file_path = os.path.join(subdir, \"data-train.txt\")\n",
    "        print(f\"Collecting for file: {data_file_path}\")\n",
    "        data_file = json.load(open(data_file_path, \"r\"))\n",
    "        for example in data_file[\"examples\"][\"train\"]:\n",
    "            command_split = re.split(',a,|,the,', example['command'])\n",
    "            command_mono = \",\".join(command_split)\n",
    "            if command_mono in per_command_mono_count.keys():\n",
    "                if per_command_mono_count[command_mono] == 180: # for p4 this may never hit!\n",
    "                    continue # we are not adding this example since redundant!\n",
    "                per_command_mono_count[command_mono] += 1\n",
    "            else:\n",
    "                per_command_mono_count[command_mono] = 1\n",
    "            if command_mono in shared_example_combined.keys():\n",
    "                shared_example_combined[command_mono].append(example)\n",
    "            else:\n",
    "                shared_example_combined[command_mono] = [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk!\n",
    "import random\n",
    "shared_examples = []\n",
    "commands_mono = list(shared_example_combined.keys())\n",
    "random.shuffle(commands_mono)\n",
    "for i in range(upper_limit):\n",
    "    examples_to_include = shared_example_combined[commands_mono[i]]\n",
    "    for example in examples_to_include:\n",
    "        shared_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file[\"examples\"][\"train\"] = shared_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11810\n"
     ]
    }
   ],
   "source": [
    "print(len(shared_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../data-files-{prefix}/ReaSCAN-compositional-{prefix}/data-train.txt\", \"w\") as fd:\n",
    "    json.dump(data_file, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3-RD: Double Clause with Only Random Distractors (and some contextual distractors, which are also random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 17:40 Reading dataset from file: ../../data-files-updated/ReaSCAN-compositional-p3-rd/data-train.txt...\n"
     ]
    }
   ],
   "source": [
    "p3_rd_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p3-rd/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_rd_path_to_data}...\")\n",
    "p3_rd_data_json = json.load(open(p3_rd_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607500"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3_rd_all_fake_train = p3_rd_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p3_rd_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p3_rd_id_example_map = OrderedDict({})\n",
    "p3_rd_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p3_rd_data_json[\"examples\"][\"train\"]:\n",
    "    p3_rd_id_example_map[index] = example\n",
    "    p3_rd_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_rd_splits_distribution = OrderedDict({})\n",
    "p3_rd_splits_assignment = OrderedDict({})\n",
    "for index, splits in p3_rd_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p3_rd_splits_distribution.keys():\n",
    "            p3_rd_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p3_rd_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p3_rd_splits_assignment:\n",
    "            p3_rd_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p3_rd_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p3_rd_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p3_rd_all_fake_train)*0.052)\n",
    "p3_rd_all_example_id = p3_rd_splits_assignment[\"train\"]\n",
    "random.shuffle(p3_rd_all_example_id)\n",
    "p3_rd_train_example_id = p3_rd_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p3_rd_dev_example_id = p3_rd_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p3_rd_test_example_id = p3_rd_all_example_id[-gscan_dev_size:]\n",
    "p3_rd_splits_assignment[\"train\"] = p3_rd_train_example_id\n",
    "p3_rd_splits_assignment[\"dev\"] = p3_rd_dev_example_id\n",
    "p3_rd_splits_assignment[\"test\"] = p3_rd_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train split, we have 569835 examples.\n",
      "for dev split, we have 31590 examples.\n",
      "for test split, we have 6075 examples.\n"
     ]
    }
   ],
   "source": [
    "for split, all_ids in p3_rd_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p3_rd_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p3_rd_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p3_rd_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p3-rd/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p3_rd_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1+P2+P3: Compositional Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 18:03 Reading dataset from file: ../../data-files-updated/ReaSCAN-compositional-p1/data-train.txt...\n",
      "2021-06-08 18:04 Reading dataset from file: ../../data-files-updated/ReaSCAN-compositional-p2/data-train.txt...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "p1_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p1/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p1_path_to_data}...\")\n",
    "p1_data_json = json.load(open(p1_path_to_data, \"r\"))\n",
    "\n",
    "p2_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p2/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p2_path_to_data}...\")\n",
    "p2_data_json = json.load(open(p2_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 18:24 Reading dataset from file: ../../data-files-updated/ReaSCAN-compositional-p3/data-train.txt...\n"
     ]
    }
   ],
   "source": [
    "p3_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p3/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_path_to_data}...\")\n",
    "p3_data_json = json.load(open(p3_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them into a single big train!\n",
    "p1_examples = p1_data_json[\"examples\"][\"train\"]\n",
    "p2_examples = p2_data_json[\"examples\"][\"train\"]\n",
    "p3_data_json[\"examples\"][\"train\"].extend(p1_examples)\n",
    "p3_data_json[\"examples\"][\"train\"].extend(p2_examples)\n",
    "data_json = p3_data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1070986"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us downsample it to ?K\n",
    "len(p3_data_json[\"examples\"][\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "id_example_map = OrderedDict({})\n",
    "id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in data_json[\"examples\"][\"train\"]:\n",
    "    id_example_map[index] = example\n",
    "    id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "\n",
    "    # a1_novel_color_attribute\n",
    "    if \"yellow,square\" in example['command']:\n",
    "        id_splits_map[index].add(\"a1_novel_color_attribute\")\n",
    "    \n",
    "    # a2_novel_color_attribute_visual\n",
    "    if example[\"derivation\"] == \"$OBJ_0\":\n",
    "        if \"red,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"red\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    elif example[\"derivation\"] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        if \"red,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"red\") or \\\n",
    "            (example['situation']['placed_objects']['1']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['1']['object']['color'] == \"red\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    elif example[\"derivation\"] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        if \"red,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"red\") or \\\n",
    "            (example['situation']['placed_objects']['1']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['1']['object']['color'] == \"red\") or \\\n",
    "            (example['situation']['placed_objects']['2']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['2']['object']['color'] == \"red\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # gscan_small_cylinder_command_target_only\n",
    "    if \"small,cylinder\" in example['command'] or \\\n",
    "        \"small,red,cylinder\" in example['command'] or \\\n",
    "        \"small,blue,cylinder\" in example['command'] or \\\n",
    "        \"small,yellow,cylinder\" in example['command'] or \\\n",
    "        \"small,green,cylinder\" in example['command']:\n",
    "        id_splits_map[index].add(\"a3_novel_size_attribute\")\n",
    "    \n",
    "    # novel_yellow_square_blue_circle_coexist_shape\n",
    "    if \"yellow,square\" in example['command'] and \"blue,circle\" in example['command']:\n",
    "        id_splits_map[index].add(\"b_novel_object_coexist\")\n",
    "\n",
    "    # novel_same_shape_is_inside_coexist_relation\n",
    "    if \"same,shape\" in example['command'] and \"is,inside\" in example['command']:\n",
    "        id_splits_map[index].add(\"c_novel_relation_coexist\")\n",
    "        \n",
    "    # novel_inside_of_as_yellow_box\n",
    "    if \"is,inside,of,a,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,a,small,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,small,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,a,big,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,big,yellow,box\" in example['command']:\n",
    "        id_splits_map[index].add(\"d_novel_object_relation_pair\")\n",
    "    \n",
    "    if example['grammer_pattern'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2 & $OBJ_3\":\n",
    "        id_splits_map[index].add(\"e_novel_clause_length\")\n",
    "    \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_distribution = OrderedDict({})\n",
    "splits_assignment = OrderedDict({})\n",
    "count = 0\n",
    "ccount = 0\n",
    "for index, splits in id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        count += 1\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in splits_distribution.keys():\n",
    "            splits_distribution[split] += 1\n",
    "        else:\n",
    "            splits_distribution[split] = 1\n",
    "        \n",
    "        if split in splits_assignment:\n",
    "            splits_assignment[split].append(index)\n",
    "        else:\n",
    "            splits_assignment[split] = [index]\n",
    "    else:\n",
    "        ccount += 1\n",
    "        for split in splits:\n",
    "            if split in splits_distribution.keys():\n",
    "                splits_distribution[split] += 1\n",
    "            else:\n",
    "                splits_distribution[split] = 1\n",
    "                \n",
    "            if split in splits_assignment:\n",
    "                splits_assignment[split].append(index)\n",
    "            else:\n",
    "                splits_assignment[split] = [index]\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "all_example_id = splits_assignment[\"train\"]\n",
    "gscan_dev_size = int(len(all_example_id)*0.01)\n",
    "gscan_test_size = int(len(all_example_id)*0.052)\n",
    "random.shuffle(all_example_id)\n",
    "train_example_id = all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "dev_example_id = all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "test_example_id = all_example_id[-gscan_dev_size:]\n",
    "splits_assignment[\"train\"] = train_example_id\n",
    "splits_assignment[\"dev\"] = dev_example_id\n",
    "splits_assignment[\"test\"] = test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a3_novel_size_attribute', 214892),\n",
       "             ('train', 575395),\n",
       "             ('a2_novel_color_attribute_visual', 187302),\n",
       "             ('c_novel_relation_coexist', 5940),\n",
       "             ('d_novel_object_relation_pair', 37620),\n",
       "             ('a1_novel_color_attribute', 127825),\n",
       "             ('b_novel_object_coexist', 10260)])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for a3_novel_size_attribute split, we have 214892 examples.\n",
      "for train split, we have 539722 examples.\n",
      "for a2_novel_color_attribute_visual split, we have 187302 examples.\n",
      "for c_novel_relation_coexist split, we have 5940 examples.\n",
      "for d_novel_object_relation_pair split, we have 37620 examples.\n",
      "for a1_novel_color_attribute split, we have 127825 examples.\n",
      "for b_novel_object_coexist split, we have 10260 examples.\n",
      "for dev split, we have 29920 examples.\n",
      "for test split, we have 5753 examples.\n"
     ]
    }
   ],
   "source": [
    "for split, all_ids in splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(id_example_map[_id])\n",
    "# save it to the disk\n",
    "data_json[\"examples\"] = updated_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data-files-updated/ReaSCAN-compositional/data-compositional-splits-all.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in splits_assignment.items():\n",
    "    if split == \"train\" or split == \"dev\" or split == \"test\":\n",
    "        updated_examples[split] = []\n",
    "        for _id in all_ids:\n",
    "            updated_examples[split].append(id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional/data-compositional-splits-train.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1+P2+P3: Compositional Splits Continue\n",
    "We need to make sure novel attribute splits actually require the attribute to reason, otherwise, it becomes less meaningful, and may cause accuracy inflation afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "path_to_data = \"../../data-files/ReaSCAN-compositional/data-compositional-splits-all.txt\"\n",
    "logger.info(f\"Reading dataset from file: {path_to_data}...\")\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 \n",
    "a1_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a1_novel_color_attribute']:\n",
    "    if example['has_attribute_distractor']:\n",
    "        for k, v in example['object_expression'].items():\n",
    "            if \"yellow square\" in v:\n",
    "                if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$COLOR\":\n",
    "                        a1_attribute_example_filtered += [example]\n",
    "                        attribute_change += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual examples for a1 = 22057\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual examples for a1 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a2\n",
    "a2_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a2_novel_color_attribute_visual']:\n",
    "    if \"red,square\" in example[\"command\"]:\n",
    "        if example['has_attribute_distractor']:\n",
    "            for k, v in example['object_expression'].items():\n",
    "                if \"red square\" in v:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                        if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$COLOR\":\n",
    "                            a2_attribute_example_filtered += [example]\n",
    "                            attribute_change += 1\n",
    "    else:\n",
    "        # this is for the visual part, we automatically added in.\n",
    "        a2_attribute_example_filtered += [example]\n",
    "        attribute_change += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual examples for a2 = 81349\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual examples for a2 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a3\n",
    "a3_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a3_novel_size_attribute']:\n",
    "    if example['has_attribute_distractor']:\n",
    "        for k, v in example['object_expression'].items():\n",
    "            if \"small\" in v and \"cylinder\" in v:\n",
    "                if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$SIZE\":\n",
    "                        a3_attribute_example_filtered += [example]\n",
    "                        attribute_change += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual examples for a3 = 35675\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual examples for a3 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['b_novel_object_coexist']:\n",
    "    b1_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10260"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b1_attribute_example_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['c_novel_relation_coexist']:\n",
    "    b2_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5940"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b2_attribute_example_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['d_novel_object_relation_pair']:\n",
    "    b3_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 test example count=921\n",
      "p2 test example count=2120\n",
      "p3 test example count=2712\n"
     ]
    }
   ],
   "source": [
    "p1_test_example_filtered = []\n",
    "p2_test_example_filtered = []\n",
    "p3_test_example_filtered = []\n",
    "for example in data_json[\"examples\"][\"test\"]:\n",
    "    if example['derivation'] == \"$OBJ_0\":\n",
    "        p1_test_example_filtered += [example]\n",
    "    elif example['derivation'] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        p2_test_example_filtered += [example]\n",
    "    elif example['derivation'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        p3_test_example_filtered += [example]\n",
    "print(f\"p1 test example count={len(p1_test_example_filtered)}\")\n",
    "print(f\"p2 test example count={len(p2_test_example_filtered)}\")\n",
    "print(f\"p3 test example count={len(p3_test_example_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a1_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-a1/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a2_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-a2/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a3_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-a3/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b1_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-b1/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b2_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-b2/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b3_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-b3/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p1_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p1-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p2_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p2-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p3_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p3-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Novel Clause Length Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 18:33 Reading dataset from file: ../../data-files-updated/ReaSCAN-compositional-p4/data-train.txt...\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p4/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {path_to_data}...\")\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4_test_example_filtered = data_json[\"examples\"][\"train\"]\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p4_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p4-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8375"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_json[\"examples\"][\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

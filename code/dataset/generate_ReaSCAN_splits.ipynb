{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrips for generating splits\n",
    "This script assums you have the main ReaSCAN generated by the generate_ReaSCAN.py script. After that, you can use this file to generate/extrapolate different splits. In the future, we may consolidate two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, OrderedDict\n",
    "import os\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "if isnotebook():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO,\n",
    "                    datefmt=\"%Y-%m-%d %H:%M\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from world import *\n",
    "from vocabulary import Vocabulary as ReaSCANVocabulary\n",
    "from object_vocabulary import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1: gSCAN Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_path_to_data = \"../../data-files/ReaSCAN-compositional-p1/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p1_path_to_data}...\")\n",
    "p1_data_json = json.load(open(p1_path_to_data, \"r\"))\n",
    "\n",
    "p1_all_fake_train = p1_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p1_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p1_id_example_map = OrderedDict({})\n",
    "p1_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p1_data_json[\"examples\"][\"train\"]:\n",
    "    p1_id_example_map[index] = example\n",
    "    p1_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_splits_distribution = OrderedDict({})\n",
    "p1_splits_assignment = OrderedDict({})\n",
    "for index, splits in p1_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p1_splits_distribution.keys():\n",
    "            p1_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p1_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p1_splits_assignment:\n",
    "            p1_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p1_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p1_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p1_all_fake_train)*0.052)\n",
    "p1_all_example_id = p1_splits_assignment[\"train\"]\n",
    "random.shuffle(p1_all_example_id)\n",
    "p1_train_example_id = p1_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p1_dev_example_id = p1_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p1_test_example_id = p1_all_example_id[-gscan_dev_size:]\n",
    "p1_splits_assignment[\"train\"] = p1_train_example_id\n",
    "p1_splits_assignment[\"dev\"] = p1_dev_example_id\n",
    "p1_splits_assignment[\"test\"] = p1_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p1_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p1_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p1_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p1_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p1/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p1_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P2: Single Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_path_to_data = \"../../data-files/ReaSCAN-compositional-p2/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p2_path_to_data}...\")\n",
    "p2_data_json = json.load(open(p2_path_to_data, \"r\"))\n",
    "\n",
    "p2_all_fake_train = p2_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p2_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p2_id_example_map = OrderedDict({})\n",
    "p2_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p2_data_json[\"examples\"][\"train\"]:\n",
    "    p2_id_example_map[index] = example\n",
    "    p2_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_splits_distribution = OrderedDict({})\n",
    "p2_splits_assignment = OrderedDict({})\n",
    "for index, splits in p2_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p2_splits_distribution.keys():\n",
    "            p2_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p2_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p2_splits_assignment:\n",
    "            p2_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p2_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p2_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p2_all_fake_train)*0.052)\n",
    "p2_all_example_id = p2_splits_assignment[\"train\"]\n",
    "random.shuffle(p2_all_example_id)\n",
    "p2_train_example_id = p2_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p2_dev_example_id = p2_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p2_test_example_id = p2_all_example_id[-gscan_dev_size:]\n",
    "p2_splits_assignment[\"train\"] = p2_train_example_id\n",
    "p2_splits_assignment[\"dev\"] = p2_dev_example_id\n",
    "p2_splits_assignment[\"test\"] = p2_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p2_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p2_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p2_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p2_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p2/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p2_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3: Double Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_path_to_data = \"../../data-files/ReaSCAN-compositional-p3/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_path_to_data}...\")\n",
    "p3_data_json = json.load(open(p3_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_all_fake_train = data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p3_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p3_id_example_map = OrderedDict({})\n",
    "p3_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p3_data_json[\"examples\"][\"train\"]:\n",
    "    p3_id_example_map[index] = example\n",
    "    p3_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_splits_distribution = OrderedDict({})\n",
    "p3_splits_assignment = OrderedDict({})\n",
    "for index, splits in p3_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p3_splits_distribution.keys():\n",
    "            p3_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p3_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p3_splits_assignment:\n",
    "            p3_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p3_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p3_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p3_all_fake_train)*0.052)\n",
    "p3_all_example_id = p3_splits_assignment[\"train\"]\n",
    "random.shuffle(p3_all_example_id)\n",
    "p3_train_example_id = p3_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p3_dev_example_id = p3_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p3_test_example_id = p3_all_example_id[-gscan_dev_size:]\n",
    "p3_splits_assignment[\"train\"] = p3_train_example_id\n",
    "p3_splits_assignment[\"dev\"] = p3_dev_example_id\n",
    "p3_splits_assignment[\"test\"] = p3_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p3_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p3_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p3_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p3_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p3/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p3_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3-RD: Double Clause with Only Random Distractors (and some contextual distractors, which are also random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_rd_path_to_data = \"../../data-files/ReaSCAN-compositional-p3-rd/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_rd_path_to_data}...\")\n",
    "p3_rd_data_json = json.load(open(p3_rd_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_rd_all_fake_train = p3_rd_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p3_rd_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p3_rd_id_example_map = OrderedDict({})\n",
    "p3_rd_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p3_rd_data_json[\"examples\"][\"train\"]:\n",
    "    p3_rd_id_example_map[index] = example\n",
    "    p3_rd_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_rd_splits_distribution = OrderedDict({})\n",
    "p3_rd_splits_assignment = OrderedDict({})\n",
    "for index, splits in p3_rd_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p3_rd_splits_distribution.keys():\n",
    "            p3_rd_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p3_rd_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p3_rd_splits_assignment:\n",
    "            p3_rd_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p3_rd_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p3_rd_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p3_rd_all_fake_train)*0.052)\n",
    "p3_rd_all_example_id = p3_rd_splits_assignment[\"train\"]\n",
    "random.shuffle(p3_rd_all_example_id)\n",
    "p3_rd_train_example_id = p3_rd_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p3_rd_dev_example_id = p3_rd_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p3_rd_test_example_id = p3_rd_all_example_id[-gscan_dev_size:]\n",
    "p3_rd_splits_assignment[\"train\"] = p3_rd_train_example_id\n",
    "p3_rd_splits_assignment[\"dev\"] = p3_rd_dev_example_id\n",
    "p3_rd_splits_assignment[\"test\"] = p3_rd_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p3_rd_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p3_rd_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p3_rd_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p3_rd_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p3-rd/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p3_rd_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1+P2+P3: Compositional Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "p1_path_to_data = \"../../data-files/ReaSCAN-compositional-p1/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p1_path_to_data}...\")\n",
    "p1_data_json = json.load(open(p1_path_to_data, \"r\"))\n",
    "\n",
    "p2_path_to_data = \"../../data-files/ReaSCAN-compositional-p2/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p2_path_to_data}...\")\n",
    "p2_data_json = json.load(open(p2_path_to_data, \"r\"))\n",
    "\n",
    "p3_path_to_data = \"../../data-files/ReaSCAN-compositional-p3/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_path_to_data}...\")\n",
    "p3_data_json = json.load(open(p3_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them into a single big train!\n",
    "p1_examples = p1_data_json[\"examples\"][\"train\"]\n",
    "p2_examples = p2_data_json[\"examples\"][\"train\"]\n",
    "p3_data_json[\"examples\"][\"train\"].extend(p1_examples)\n",
    "p3_data_json[\"examples\"][\"train\"].extend(p2_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us downsample it to ?K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "id_example_map = OrderedDict({})\n",
    "id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in data_json[\"examples\"][\"train\"]:\n",
    "    id_example_map[index] = example\n",
    "    id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "\n",
    "    # a1_novel_color_attribute\n",
    "    if \"yellow,square\" in example['command']:\n",
    "        id_splits_map[index].add(\"a1_novel_color_attribute\")\n",
    "    \n",
    "    # a2_novel_color_attribute_visual\n",
    "    if example[\"derivation\"] == \"$OBJ_0\":\n",
    "        if \"yellow,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"yellow\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    elif example[\"derivation\"] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        if \"yellow,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"yellow\") or \\\n",
    "            (example['situation']['placed_objects']['1']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['1']['object']['color'] == \"yellow\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    elif example[\"derivation\"] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        if \"yellow,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"yellow\") or \\\n",
    "            (example['situation']['placed_objects']['1']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['1']['object']['color'] == \"yellow\") or \\\n",
    "            (example['situation']['placed_objects']['2']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['2']['object']['color'] == \"yellow\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # gscan_small_cylinder_command_target_only\n",
    "    if \"small,cylinder\" in example['command'] or \\\n",
    "        \"small,red,cylinder\" in example['command'] or \\\n",
    "        \"small,blue,cylinder\" in example['command'] or \\\n",
    "        \"small,yellow,cylinder\" in example['command'] or \\\n",
    "        \"small,green,cylinder\" in example['command']:\n",
    "        id_splits_map[index].add(\"a3_novel_size_attribute\")\n",
    "    \n",
    "    # novel_yellow_square_blue_circle_coexist_shape\n",
    "    if \"yellow,square\" in example['command'] and \"blue,circle\" in example['command']:\n",
    "        id_splits_map[index].add(\"b_novel_object_coexist\")\n",
    "\n",
    "    # novel_same_shape_is_inside_coexist_relation\n",
    "    if \"same,shape\" in example['command'] and \"is,inside\" in example['command']:\n",
    "        id_splits_map[index].add(\"c_novel_relation_coexist\")\n",
    "        \n",
    "    # novel_inside_of_as_yellow_box\n",
    "    if \"is,inside,of,a,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,a,small,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,small,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,a,big,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,big,yellow,box\" in example['command']:\n",
    "        id_splits_map[index].add(\"d_novel_object_relation_pair\")\n",
    "    \n",
    "    if example['grammer_pattern'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2 & $OBJ_3\":\n",
    "        id_splits_map[index].add(\"e_novel_clause_length\")\n",
    "    \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_distribution = OrderedDict({})\n",
    "splits_assignment = OrderedDict({})\n",
    "for index, splits in id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in splits_distribution.keys():\n",
    "            splits_distribution[split] += 1\n",
    "        else:\n",
    "            splits_distribution[split] = 1\n",
    "        \n",
    "        if split in splits_assignment:\n",
    "            splits_assignment[split].append(index)\n",
    "        else:\n",
    "            splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        for split in splits:\n",
    "            if split in splits_distribution.keys():\n",
    "                splits_distribution[split] += 1\n",
    "            else:\n",
    "                splits_distribution[split] = 1\n",
    "                \n",
    "            if split in splits_assignment:\n",
    "                splits_assignment[split].append(index)\n",
    "            else:\n",
    "                splits_assignment[split] = [index]\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "all_example_id = splits_assignment[\"train\"]\n",
    "gscan_dev_size = int(len(all_example_id)*0.01)\n",
    "gscan_test_size = int(len(all_example_id)*0.052)\n",
    "random.shuffle(all_example_id)\n",
    "train_example_id = all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "dev_example_id = all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "test_example_id = all_example_id[-gscan_dev_size:]\n",
    "splits_assignment[\"train\"] = train_example_id\n",
    "splits_assignment[\"dev\"] = dev_example_id\n",
    "splits_assignment[\"test\"] = test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional/data-compositional-splits-all.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in splits_assignment.items():\n",
    "    if split == \"train\" or split == \"dev\" or split == \"test\":\n",
    "        updated_examples[split] = []\n",
    "        for _id in all_ids:\n",
    "            updated_examples[split].append(id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional/data-compositional-splits-train.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1+P2+P3: Compositional Splits Continue\n",
    "We need to make sure novel attribute splits actually require the attribute to reason, otherwise, it becomes less meaningful, and may cause accuracy inflation afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "path_to_data = \"../../data-files/ReaSCAN-compositional/data-compositional-splits-all.txt\"\n",
    "logger.info(f\"Reading dataset from file: {path_to_data}...\")\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 \n",
    "a1_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a1_novel_color_attribute']:\n",
    "    if example['has_attribute_distractor']:\n",
    "        for k, v in example['object_expression'].items():\n",
    "            if \"yellow square\" in v:\n",
    "                if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$COLOR\":\n",
    "                        a1_attribute_example_filtered += [example]\n",
    "                        attribute_change += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual examples for a1 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a2\n",
    "a2_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a2_novel_color_attribute_visual']:\n",
    "    if \"yellow,square\" in example[\"command\"]:\n",
    "        if example['has_attribute_distractor']:\n",
    "            for k, v in example['object_expression'].items():\n",
    "                if \"yellow square\" in v:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                        if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$COLOR\":\n",
    "                            a2_attribute_example_filtered += [example]\n",
    "                            attribute_change += 1\n",
    "    else:\n",
    "        # this is for the visual part, we automatically added in.\n",
    "        a2_attribute_example_filtered += [example]\n",
    "        attribute_change += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual examples for a2 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a3\n",
    "a3_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a3_novel_size_attribute']:\n",
    "    if example['has_attribute_distractor']:\n",
    "        for k, v in example['object_expression'].items():\n",
    "            if \"small\" in v and \"cylinder\" in v:\n",
    "                if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$SIZE\":\n",
    "                        a3_attribute_example_filtered += [example]\n",
    "                        attribute_change += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual examples for a3 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['b_novel_object_coexist']:\n",
    "    b1_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['c_novel_relation_coexist']:\n",
    "    b2_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['d_novel_object_relation_pair']:\n",
    "    b3_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a1_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-a1/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a2_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-a2/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a3_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-a3/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b1_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-b1/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b2_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-b2/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b3_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-b3/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test subsplit per command pattern!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "path_to_data = \"../../data-files/ReaSCAN-compositional/data-compositional-splits-all.txt\"\n",
    "logger.info(f\"Reading dataset from file: {path_to_data}...\")\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_test_example_filtered = []\n",
    "p2_test_example_filtered = []\n",
    "p3_test_example_filtered = []\n",
    "for example in data_json[\"examples\"][\"test\"]:\n",
    "    if example['derivation'] == \"$OBJ_0\":\n",
    "        p1_test_example_filtered += [example]\n",
    "    elif example['derivation'] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        p2_test_example_filtered += [example]\n",
    "    elif example['derivation'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        p3_test_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"p1 test example count={len(p1_test_example_filtered)}\")\n",
    "print(f\"p2 test example count={len(p2_test_example_filtered)}\")\n",
    "print(f\"p3 test example count={len(p3_test_example_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p1_test_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p1-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p2_test_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p2-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p3_test_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p3-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Novel Clause Length Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrips for generating splits\n",
    "This script assums you have the main ReaSCAN generated by the generate_ReaSCAN.py script. After that, you can use this file to generate/extrapolate different splits. In the future, we may consolidate two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, OrderedDict\n",
    "import os\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "if isnotebook():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO,\n",
    "                    datefmt=\"%Y-%m-%d %H:%M\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from world import *\n",
    "from object_vocabulary import *\n",
    "from vocabulary import *\n",
    "from grammer import *\n",
    "from simulator import *\n",
    "from relation_graph import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1: gSCAN Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p1/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p1_path_to_data}...\")\n",
    "p1_data_json = json.load(open(p1_path_to_data, \"r\"))\n",
    "\n",
    "p1_all_fake_train = p1_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p1_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p1_id_example_map = OrderedDict({})\n",
    "p1_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p1_data_json[\"examples\"][\"train\"]:\n",
    "    p1_id_example_map[index] = example\n",
    "    p1_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_splits_distribution = OrderedDict({})\n",
    "p1_splits_assignment = OrderedDict({})\n",
    "for index, splits in p1_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p1_splits_distribution.keys():\n",
    "            p1_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p1_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p1_splits_assignment:\n",
    "            p1_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p1_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p1_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p1_all_fake_train)*0.052)\n",
    "p1_all_example_id = p1_splits_assignment[\"train\"]\n",
    "random.shuffle(p1_all_example_id)\n",
    "p1_train_example_id = p1_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p1_dev_example_id = p1_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p1_test_example_id = p1_all_example_id[-gscan_dev_size:]\n",
    "p1_splits_assignment[\"train\"] = p1_train_example_id\n",
    "p1_splits_assignment[\"dev\"] = p1_dev_example_id\n",
    "p1_splits_assignment[\"test\"] = p1_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p1_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p1_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p1_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p1_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p1/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p1_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P2: Single Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p2/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p2_path_to_data}...\")\n",
    "p2_data_json = json.load(open(p2_path_to_data, \"r\"))\n",
    "\n",
    "p2_all_fake_train = p2_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p2_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p2_id_example_map = OrderedDict({})\n",
    "p2_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p2_data_json[\"examples\"][\"train\"]:\n",
    "    p2_id_example_map[index] = example\n",
    "    p2_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_splits_distribution = OrderedDict({})\n",
    "p2_splits_assignment = OrderedDict({})\n",
    "for index, splits in p2_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p2_splits_distribution.keys():\n",
    "            p2_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p2_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p2_splits_assignment:\n",
    "            p2_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p2_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p2_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p2_all_fake_train)*0.052)\n",
    "p2_all_example_id = p2_splits_assignment[\"train\"]\n",
    "random.shuffle(p2_all_example_id)\n",
    "p2_train_example_id = p2_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p2_dev_example_id = p2_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p2_test_example_id = p2_all_example_id[-gscan_dev_size:]\n",
    "p2_splits_assignment[\"train\"] = p2_train_example_id\n",
    "p2_splits_assignment[\"dev\"] = p2_dev_example_id\n",
    "p2_splits_assignment[\"test\"] = p2_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p2_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p2_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p2_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p2_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p2/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p2_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3: Double Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p3/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_path_to_data}...\")\n",
    "p3_data_json = json.load(open(p3_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_all_fake_train = p3_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p3_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p3_id_example_map = OrderedDict({})\n",
    "p3_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p3_data_json[\"examples\"][\"train\"]:\n",
    "    p3_id_example_map[index] = example\n",
    "    p3_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_splits_distribution = OrderedDict({})\n",
    "p3_splits_assignment = OrderedDict({})\n",
    "for index, splits in p3_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p3_splits_distribution.keys():\n",
    "            p3_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p3_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p3_splits_assignment:\n",
    "            p3_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p3_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p3_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p3_all_fake_train)*0.052)\n",
    "p3_all_example_id = p3_splits_assignment[\"train\"]\n",
    "random.shuffle(p3_all_example_id)\n",
    "p3_train_example_id = p3_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p3_dev_example_id = p3_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p3_test_example_id = p3_all_example_id[-gscan_dev_size:]\n",
    "p3_splits_assignment[\"train\"] = p3_train_example_id\n",
    "p3_splits_assignment[\"dev\"] = p3_dev_example_id\n",
    "p3_splits_assignment[\"test\"] = p3_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p3_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p3_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p3_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p3_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p3/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p3_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3: Double Clause (combing with P3 sharding, this section is shared across all patterns as well!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "pattern = \"p4\"\n",
    "special_condition = \"\"\n",
    "if special_condition != \"\":\n",
    "    prefix = f\"{pattern}-{special_condition}\"\n",
    "else:\n",
    "    prefix = pattern\n",
    "sharding_dir = f\"../../data-files-{prefix}/\"\n",
    "if pattern == \"p3\":\n",
    "    upper_limit = 3375\n",
    "elif pattern == \"p2\":\n",
    "    upper_limit = 2025\n",
    "elif pattern == \"p1\":\n",
    "    upper_limit = 675\n",
    "elif pattern == \"p4\":\n",
    "    upper_limit = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_command = set([])\n",
    "for subdir, dirs, files in os.walk(sharding_dir):\n",
    "    if \"jobid\" in subdir:\n",
    "        \n",
    "        # Completeness check!\n",
    "        logging_file = os.path.join(subdir, \"generator.log\")\n",
    "        with open(logging_file) as f:\n",
    "            content = f.readlines()\n",
    "        # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "        content = [x.strip() for x in content]\n",
    "        completed = False\n",
    "        for c in content:\n",
    "            if \"==FINISH==\" in c:\n",
    "                completed = True\n",
    "                break\n",
    "        jobid = logging_file.split(\"/\")[-2].split(\"-\")[-1]\n",
    "        print(f\"jobid={jobid}, status=complete={completed}\")\n",
    "        if not completed:\n",
    "            break\n",
    "        \n",
    "        # Uniqueness check!\n",
    "        data_file_path = os.path.join(subdir, \"data-train.txt\")\n",
    "        print(f\"scanning for file: {data_file_path}\")\n",
    "        data_file = json.load(open(data_file_path, \"r\"))\n",
    "        for example in data_file[\"examples\"][\"train\"]:\n",
    "            command_split = re.split(',a,|,the,', example['command'])\n",
    "            command_mono = \",\".join(command_split)\n",
    "            unique_command.add(command_mono)\n",
    "assert len(unique_command) > upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_example_combined = {}\n",
    "per_command_mono_count = {}\n",
    "for subdir, dirs, files in os.walk(sharding_dir):\n",
    "    if \"jobid\" in subdir:\n",
    "        data_file_path = os.path.join(subdir, \"data-train.txt\")\n",
    "        print(f\"Collecting for file: {data_file_path}\")\n",
    "        data_file = json.load(open(data_file_path, \"r\"))\n",
    "        for example in data_file[\"examples\"][\"train\"]:\n",
    "            command_split = re.split(',a,|,the,', example['command'])\n",
    "            command_mono = \",\".join(command_split)\n",
    "            if command_mono in per_command_mono_count.keys():\n",
    "                if per_command_mono_count[command_mono] == 180: # for p4 this may never hit!\n",
    "                    continue # we are not adding this example since redundant!\n",
    "                per_command_mono_count[command_mono] += 1\n",
    "            else:\n",
    "                per_command_mono_count[command_mono] = 1\n",
    "            if command_mono in shared_example_combined.keys():\n",
    "                shared_example_combined[command_mono].append(example)\n",
    "            else:\n",
    "                shared_example_combined[command_mono] = [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk!\n",
    "import random\n",
    "shared_examples = []\n",
    "commands_mono = list(shared_example_combined.keys())\n",
    "random.shuffle(commands_mono)\n",
    "for i in range(upper_limit):\n",
    "    examples_to_include = shared_example_combined[commands_mono[i]]\n",
    "    for example in examples_to_include:\n",
    "        shared_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file[\"examples\"][\"train\"] = shared_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(shared_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../data-files-{prefix}/ReaSCAN-compositional-{prefix}/data-train.txt\", \"w\") as fd:\n",
    "    json.dump(data_file, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3-RD: Double Clause with Only Random Distractors (and some contextual distractors, which are also random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_rd_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p3-rd/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_rd_path_to_data}...\")\n",
    "p3_rd_data_json = json.load(open(p3_rd_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_rd_all_fake_train = p3_rd_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p3_rd_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p3_rd_id_example_map = OrderedDict({})\n",
    "p3_rd_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p3_rd_data_json[\"examples\"][\"train\"]:\n",
    "    p3_rd_id_example_map[index] = example\n",
    "    p3_rd_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_rd_splits_distribution = OrderedDict({})\n",
    "p3_rd_splits_assignment = OrderedDict({})\n",
    "for index, splits in p3_rd_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p3_rd_splits_distribution.keys():\n",
    "            p3_rd_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p3_rd_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p3_rd_splits_assignment:\n",
    "            p3_rd_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p3_rd_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p3_rd_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p3_rd_all_fake_train)*0.052)\n",
    "p3_rd_all_example_id = p3_rd_splits_assignment[\"train\"]\n",
    "random.shuffle(p3_rd_all_example_id)\n",
    "p3_rd_train_example_id = p3_rd_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p3_rd_dev_example_id = p3_rd_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p3_rd_test_example_id = p3_rd_all_example_id[-gscan_dev_size:]\n",
    "p3_rd_splits_assignment[\"train\"] = p3_rd_train_example_id\n",
    "p3_rd_splits_assignment[\"dev\"] = p3_rd_dev_example_id\n",
    "p3_rd_splits_assignment[\"test\"] = p3_rd_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p3_rd_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p3_rd_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p3_rd_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p3_rd_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p3-rd/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p3_rd_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1+P2+P3: Compositional Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "p1_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p1/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p1_path_to_data}...\")\n",
    "p1_data_json = json.load(open(p1_path_to_data, \"r\"))\n",
    "\n",
    "p2_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p2/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p2_path_to_data}...\")\n",
    "p2_data_json = json.load(open(p2_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p3/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_path_to_data}...\")\n",
    "p3_data_json = json.load(open(p3_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them into a single big train!\n",
    "p1_examples = p1_data_json[\"examples\"][\"train\"]\n",
    "p2_examples = p2_data_json[\"examples\"][\"train\"]\n",
    "p3_data_json[\"examples\"][\"train\"].extend(p1_examples)\n",
    "p3_data_json[\"examples\"][\"train\"].extend(p2_examples)\n",
    "data_json = p3_data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us downsample it to ?K\n",
    "len(p3_data_json[\"examples\"][\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "id_example_map = OrderedDict({})\n",
    "id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in data_json[\"examples\"][\"train\"]:\n",
    "    id_example_map[index] = example\n",
    "    id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "\n",
    "    # a1_novel_color_attribute\n",
    "    if \"yellow,square\" in example['command']:\n",
    "        id_splits_map[index].add(\"a1_novel_color_attribute\")\n",
    "    \n",
    "    # a2_novel_color_attribute_visual\n",
    "    if example[\"derivation\"] == \"$OBJ_0\":\n",
    "        if \"red,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"red\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    elif example[\"derivation\"] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        if \"red,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"red\") or \\\n",
    "            (example['situation']['placed_objects']['1']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['1']['object']['color'] == \"red\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    elif example[\"derivation\"] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        if \"red,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"red\") or \\\n",
    "            (example['situation']['placed_objects']['1']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['1']['object']['color'] == \"red\") or \\\n",
    "            (example['situation']['placed_objects']['2']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['2']['object']['color'] == \"red\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # gscan_small_cylinder_command_target_only\n",
    "    if \"small,cylinder\" in example['command'] or \\\n",
    "        \"small,red,cylinder\" in example['command'] or \\\n",
    "        \"small,blue,cylinder\" in example['command'] or \\\n",
    "        \"small,yellow,cylinder\" in example['command'] or \\\n",
    "        \"small,green,cylinder\" in example['command']:\n",
    "        id_splits_map[index].add(\"a3_novel_size_attribute\")\n",
    "    \n",
    "    # novel_yellow_square_blue_circle_coexist_shape\n",
    "    if \"yellow,square\" in example['command'] and \"blue,circle\" in example['command']:\n",
    "        id_splits_map[index].add(\"b_novel_object_coexist\")\n",
    "\n",
    "    # novel_same_shape_is_inside_coexist_relation\n",
    "    if \"same,shape\" in example['command'] and \"is,inside\" in example['command']:\n",
    "        id_splits_map[index].add(\"c_novel_relation_coexist\")\n",
    "        \n",
    "    # novel_inside_of_as_yellow_box\n",
    "    if \"is,inside,of,a,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,a,small,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,small,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,a,big,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,big,yellow,box\" in example['command']:\n",
    "        id_splits_map[index].add(\"d_novel_object_relation_pair\")\n",
    "    \n",
    "    if example['grammer_pattern'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2 & $OBJ_3\":\n",
    "        id_splits_map[index].add(\"e_novel_clause_length\")\n",
    "    \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_distribution = OrderedDict({})\n",
    "splits_assignment = OrderedDict({})\n",
    "count = 0\n",
    "ccount = 0\n",
    "for index, splits in id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        count += 1\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in splits_distribution.keys():\n",
    "            splits_distribution[split] += 1\n",
    "        else:\n",
    "            splits_distribution[split] = 1\n",
    "        \n",
    "        if split in splits_assignment:\n",
    "            splits_assignment[split].append(index)\n",
    "        else:\n",
    "            splits_assignment[split] = [index]\n",
    "    else:\n",
    "        ccount += 1\n",
    "        for split in splits:\n",
    "            if split in splits_distribution.keys():\n",
    "                splits_distribution[split] += 1\n",
    "            else:\n",
    "                splits_distribution[split] = 1\n",
    "                \n",
    "            if split in splits_assignment:\n",
    "                splits_assignment[split].append(index)\n",
    "            else:\n",
    "                splits_assignment[split] = [index]\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "all_example_id = splits_assignment[\"train\"]\n",
    "gscan_dev_size = int(len(all_example_id)*0.01)\n",
    "gscan_test_size = int(len(all_example_id)*0.052)\n",
    "random.shuffle(all_example_id)\n",
    "train_example_id = all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "dev_example_id = all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "test_example_id = all_example_id[-gscan_dev_size:]\n",
    "splits_assignment[\"train\"] = train_example_id\n",
    "splits_assignment[\"dev\"] = dev_example_id\n",
    "splits_assignment[\"test\"] = test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(id_example_map[_id])\n",
    "# save it to the disk\n",
    "data_json[\"examples\"] = updated_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data-files-updated/ReaSCAN-compositional/data-compositional-splits-all.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in splits_assignment.items():\n",
    "    if split == \"train\" or split == \"dev\" or split == \"test\":\n",
    "        updated_examples[split] = []\n",
    "        for _id in all_ids:\n",
    "            updated_examples[split].append(id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional/data-compositional-splits-train.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1+P2+P3: Compositional Splits Continue\n",
    "We need to make sure novel attribute splits actually require the attribute to reason, otherwise, it becomes less meaningful, and may cause accuracy inflation afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "path_to_data = \"../../data-files/ReaSCAN-compositional/data-compositional-splits-all.txt\"\n",
    "logger.info(f\"Reading dataset from file: {path_to_data}...\")\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 \n",
    "a1_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a1_novel_color_attribute']:\n",
    "    if example['has_attribute_distractor']:\n",
    "        for k, v in example['object_expression'].items():\n",
    "            if \"yellow square\" in v:\n",
    "                if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$COLOR\":\n",
    "                        a1_attribute_example_filtered += [example]\n",
    "                        attribute_change += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual examples for a1 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a2\n",
    "a2_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a2_novel_color_attribute_visual']:\n",
    "    if \"red,square\" in example[\"command\"]:\n",
    "        if example['has_attribute_distractor']:\n",
    "            for k, v in example['object_expression'].items():\n",
    "                if \"red square\" in v:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                        if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$COLOR\":\n",
    "                            a2_attribute_example_filtered += [example]\n",
    "                            attribute_change += 1\n",
    "    else:\n",
    "        # this is for the visual part, we automatically added in.\n",
    "        a2_attribute_example_filtered += [example]\n",
    "        attribute_change += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual examples for a2 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a3\n",
    "a3_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a3_novel_size_attribute']:\n",
    "    if example['has_attribute_distractor']:\n",
    "        for k, v in example['object_expression'].items():\n",
    "            if \"small\" in v and \"cylinder\" in v:\n",
    "                if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$SIZE\":\n",
    "                        a3_attribute_example_filtered += [example]\n",
    "                        attribute_change += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual examples for a3 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['b_novel_object_coexist']:\n",
    "    b1_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b1_attribute_example_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['c_novel_relation_coexist']:\n",
    "    b2_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b2_attribute_example_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['d_novel_object_relation_pair']:\n",
    "    b3_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_test_example_filtered = []\n",
    "p2_test_example_filtered = []\n",
    "p3_test_example_filtered = []\n",
    "for example in data_json[\"examples\"][\"test\"]:\n",
    "    if example['derivation'] == \"$OBJ_0\":\n",
    "        p1_test_example_filtered += [example]\n",
    "    elif example['derivation'] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        p2_test_example_filtered += [example]\n",
    "    elif example['derivation'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        p3_test_example_filtered += [example]\n",
    "print(f\"p1 test example count={len(p1_test_example_filtered)}\")\n",
    "print(f\"p2 test example count={len(p2_test_example_filtered)}\")\n",
    "print(f\"p3 test example count={len(p3_test_example_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a1_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-a1/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a2_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-a2/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a3_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-a3/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b1_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-b1/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b2_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-b2/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b3_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-b3/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p1_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p1-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p2_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p2-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p3_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p3-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Novel Clause Length Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p4/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {path_to_data}...\")\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4_test_example_filtered = data_json[\"examples\"][\"train\"]\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p4_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p4-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_json[\"examples\"][\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore more possibilities of harder splits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command constant but more complex distractor sampling stratigies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator(object):\n",
    "    \"\"\"\n",
    "    This convert generated grammers into a world/situation.\n",
    "    \n",
    "    Sample Situation:\n",
    "    Situation(grid_size=15, agent_position=Position(row=7, column=2), agent_direction=INT_TO_DIR[0],\n",
    "              target_object=PositionedObject(object=Object(size=2, color='red', shape='circle'),\n",
    "                                             position=Position(row=10, column=4),\n",
    "                                             vector=np.array([1, 0, 1])),\n",
    "              placed_objects=[PositionedObject(object=Object(size=2, color='red', shape='circle'),\n",
    "                                               position=Position(row=10, column=4),\n",
    "                                               vector=np.array([1, 0, 1])),\n",
    "                              PositionedObject(object=Object(size=4, color='green', shape='circle'),\n",
    "                                               position=Position(row=3, column=12),\n",
    "                                               vector=np.array([0, 1, 0]))], carrying=None)\n",
    "                                               \n",
    "    Sample Placement in the World:\n",
    "    world.place_object(Object(size=2, color=\"green\", shape=\"box\"), position=Position(row=2, column=2))\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, object_vocabulary, vocabulary, grid_size=6, \n",
    "                 n_object_min=6,\n",
    "                 n_object_max=12,\n",
    "                 save_directory=\"./tmp/\"):\n",
    "        self.object_vocabulary = object_vocabulary\n",
    "        self.vocabulary = vocabulary\n",
    "        self.grid_size = grid_size\n",
    "        self.n_object_min = n_object_min\n",
    "        self.n_object_max = n_object_max\n",
    "\n",
    "        self._world = World(grid_size=grid_size, colors=vocabulary.get_semantic_colors(),\n",
    "                            object_vocabulary=object_vocabulary,\n",
    "                            shapes=vocabulary.get_semantic_shapes(),\n",
    "                            save_directory=save_directory)\n",
    "        self._world.clear_situation()\n",
    "    \n",
    "    def sample_object_shape(\n",
    "        self, obj_grammer, obj_pattern, obj_str, rel_map, \n",
    "        is_root, shape_map\n",
    "    ):\n",
    "        obj_pattern = obj_pattern.split(\" \")\n",
    "        obj_str = obj_str.split(\" \")\n",
    "        shape = None\n",
    "        if len(obj_str) == 3:\n",
    "            shape = obj_str[2]\n",
    "        elif len(obj_str) == 2:\n",
    "            shape = obj_str[1]\n",
    "        elif len(obj_str) == 1:\n",
    "            # it must be the object\n",
    "            shape = obj_str[0]\n",
    "        # Final handling for the shape.\n",
    "        if shape == \"object\":\n",
    "            shape = self.object_vocabulary.sample_shape()\n",
    "            \n",
    "        # Override size, color and shape based on relations.\n",
    "        if not is_root:\n",
    "            # Go through the rel.\n",
    "            for pair, rel in rel_map.items():\n",
    "                if obj_grammer == pair[-1]:\n",
    "                    if pair[0] in shape_map.keys():\n",
    "                        # if this obj is acting as a child node\n",
    "                        # then have to complain with parent node\n",
    "                        if rel == \"$SAME_SHAPE\":\n",
    "                            shape = shape_map[pair[0]]\n",
    "                        elif rel == \"$IS_INSIDE\":\n",
    "                            shape = \"box\"\n",
    "        return shape\n",
    "    \n",
    "    def sample_object_spec(\n",
    "        self, obj_grammer, obj_pattern, obj_str, rel_map, \n",
    "        is_root, obj_placed_map, \n",
    "        size_restriction_map=None,\n",
    "        mentioned_shapes=None\n",
    "    ):\n",
    "        obj_pattern = obj_pattern.split(\" \")\n",
    "        obj_str = obj_str.split(\" \")\n",
    "        color = None\n",
    "        size = None\n",
    "        shape = None\n",
    "        if len(obj_str) == 3:\n",
    "            size = self.object_vocabulary.sample_size_with_prior(prior=obj_str[0])\n",
    "            color = obj_str[1]\n",
    "            shape = obj_str[2]\n",
    "        elif len(obj_str) == 2:\n",
    "            if \"$COLOR\" in obj_pattern: # color + shape.\n",
    "                size = self.object_vocabulary.sample_size()\n",
    "                color = obj_str[0]\n",
    "                shape = obj_str[1]\n",
    "            elif \"$SIZE\" in obj_pattern: # size + shape.\n",
    "                size = self.object_vocabulary.sample_size_with_prior(prior=obj_str[0])\n",
    "                color = self.object_vocabulary.sample_color()\n",
    "                shape = obj_str[1]\n",
    "        elif len(obj_str) == 1:\n",
    "            # it must be the object\n",
    "            size = self.object_vocabulary.sample_size()\n",
    "            color = self.object_vocabulary.sample_color()\n",
    "            shape = obj_str[0]\n",
    "        # Final handling for the shape.\n",
    "        if shape == \"object\":\n",
    "            # WARNING: this is a corner case you will hit\n",
    "            # if your logic chain is long, you may need to\n",
    "            # consider remove object option!\n",
    "            if mentioned_shapes != None and len(mentioned_shapes) == self.object_vocabulary._shapes:\n",
    "                assert False\n",
    "            if is_root:\n",
    "                shape = self.object_vocabulary.sample_shape() # _exclude=mentioned_shapes\n",
    "            else:\n",
    "                shape = self.object_vocabulary.sample_shape()\n",
    "                \n",
    "        return Object(color=color,size=size,shape=shape)\n",
    "                    \n",
    "    def sample_object_position(\n",
    "        self, sampled_obj, root, obj_grammer, \n",
    "        rel_map, obj_placed_map, \n",
    "        obj_position_map\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "        -1: No position can be sampled.\n",
    "        \"\"\"\n",
    "        # If it is the first node, we directly return.\n",
    "        if sampled_obj.shape != \"box\":\n",
    "            obj_random_pos = self._world.sample_position_complex(\n",
    "                condition=\"normal\", sample_one=True\n",
    "            )\n",
    "        else:\n",
    "            obj_random_pos = self._world.sample_position_complex(\n",
    "                condition=\"box\", box_size=sampled_obj.size, sample_one=True\n",
    "            )\n",
    "        if obj_grammer == root:\n",
    "            return obj_random_pos # for this round, the root node can be placed anywhere!\n",
    "        \n",
    "        # For some relations, we might need to resample positions!\n",
    "        for pair, rel in rel_map.items():\n",
    "            if obj_grammer == pair[-1]:\n",
    "                if not pair[0] in obj_placed_map.keys():\n",
    "                    assert False # this should never be the case! the position sampling in from top to bottom!\n",
    "\n",
    "                # if this obj is acting as a child node\n",
    "                # then have to complain with parent node\n",
    "                if rel == \"$SAME_ROW\":\n",
    "                    row = obj_position_map[pair[0]].row\n",
    "                    for i in range(0, self.grid_size):\n",
    "                        proposed_position = Position(row=row, column=i)\n",
    "                        if not self._world.position_taken(proposed_position, condition=\"normal\"):\n",
    "                            return proposed_position\n",
    "                    return -1 # too many objs\n",
    "                if rel == \"$SAME_COLUMN\":\n",
    "                    col = obj_position_map[pair[0]].column\n",
    "                    for i in range(0, self.grid_size):\n",
    "                        proposed_position = Position(row=i, column=col)\n",
    "                        if not self._world.position_taken(proposed_position, condition=\"normal\"):\n",
    "                            return proposed_position\n",
    "                    return -1 # too many objs\n",
    "                elif rel == \"$IS_INSIDE\":\n",
    "                    # we need to make sure enclosure\n",
    "                    assert sampled_obj.shape == \"box\"\n",
    "                    size = sampled_obj.size\n",
    "                    potential_positions = []\n",
    "                    row = obj_position_map[pair[0]].row\n",
    "                    col = obj_position_map[pair[0]].column\n",
    "                    for i in range(0, self.grid_size-size+1):\n",
    "                        for j in range(0, self.grid_size-size+1):\n",
    "                            # we need to cover the is inside obj\n",
    "                            if row >= i and row < i + sampled_obj.size and \\\n",
    "                                col >= j and col < j + sampled_obj.size:\n",
    "                                proposed_position = Position(row=i, column=j)\n",
    "                                if not self._world.position_taken(proposed_position, condition=\"box\"):\n",
    "                                    potential_positions.append(Position(row=i, column=j))\n",
    "                    random.shuffle(potential_positions)\n",
    "                    if len(potential_positions) < 1:\n",
    "                        return -1\n",
    "                    return potential_positions[0]\n",
    "\n",
    "        return obj_random_pos\n",
    "    \n",
    "    def sample_random_object_spec(\n",
    "        self, \n",
    "        size_exclude=None, \n",
    "        color_exclude=None, shape_exclude=None, combo_exclude=None\n",
    "    ):\n",
    "        if combo_exclude:\n",
    "            while True:\n",
    "                d_size = self.object_vocabulary.sample_size(_exclude=size_exclude)\n",
    "                d_color = self.object_vocabulary.sample_color(_exclude=color_exclude)\n",
    "                if shape_exclude != None and len(shape_exclude) >= 3:\n",
    "                    shape_exclude = None # let us not excluding anything, and let graph matching reject this!\n",
    "                d_shape = self.object_vocabulary.sample_shape(_exclude=shape_exclude)\n",
    "                if d_color + \" \" + d_shape not in combo_exclude:\n",
    "                    return Object(color=d_color,size=d_size,shape=d_shape)\n",
    "        else:\n",
    "            d_size = self.object_vocabulary.sample_size(_exclude=size_exclude)\n",
    "            d_color = self.object_vocabulary.sample_color(_exclude=color_exclude)\n",
    "            if shape_exclude != None and len(shape_exclude) >= 3:\n",
    "                shape_exclude = None # let us not excluding anything, and let graph matching reject this!\n",
    "            d_shape = self.object_vocabulary.sample_shape(_exclude=shape_exclude)\n",
    "            return Object(color=d_color,size=d_size,shape=d_shape)\n",
    "    \n",
    "    def place_distractor_from_dict(\n",
    "        self, distractors_dict, \n",
    "        obj_placed_map, obj_position_map, \n",
    "        debug=False, \n",
    "        special_shape_size_bound=None,\n",
    "        mentioned_shapes=None,\n",
    "    ):\n",
    "        if debug:\n",
    "            import pprint\n",
    "            pp = pprint.PrettyPrinter(indent=4)\n",
    "            pp.pprint(distractors_dict)\n",
    "        distractor_root = f\"$OBJ_{len(obj_placed_map)}\"\n",
    "        success = True\n",
    "        distractors_obj_map = distractors_dict[\"obj_map\"]\n",
    "        distractors_rel_map = distractors_dict[\"rel_map\"]\n",
    "        distractors_obj_pattern_map = distractors_dict[\"obj_pattern_map\"]\n",
    "        distractors_size_map = distractors_dict[\"size_map\"]\n",
    "        \n",
    "        distractors_sampled_obj_map = {}\n",
    "        for dis_grammer, dis_str in distractors_obj_map.items():\n",
    "            # 1. Sample object.\n",
    "            sampled_dis = self.sample_object_spec(\n",
    "                dis_grammer,\n",
    "                distractors_obj_pattern_map[dis_grammer], \n",
    "                dis_str, distractors_rel_map, \n",
    "                is_root=dis_grammer==distractor_root, \n",
    "                obj_placed_map=obj_placed_map,\n",
    "                mentioned_shapes=mentioned_shapes,\n",
    "            )\n",
    "            # 1.1. Update the size of the object if needed.\n",
    "            if dis_grammer in distractors_size_map.keys():\n",
    "                sampled_dis = Object(\n",
    "                    color=sampled_dis.color,\n",
    "                    size=distractors_size_map[dis_grammer],\n",
    "                    shape=sampled_dis.shape\n",
    "                )\n",
    "            # 1.2. Another pass of override by using global constraints.\n",
    "            special_shape_super = sampled_dis.shape\n",
    "            special_shape_sub = sampled_dis.color + \" \" + sampled_dis.shape\n",
    "            # e.g., small circle exists in the command, then any colored circle needs to be constrain\n",
    "            if special_shape_super in special_shape_size_bound.keys():\n",
    "                if \"small\" in dis_str:\n",
    "                    updated_size = special_shape_size_bound[special_shape_super][0]\n",
    "                else:\n",
    "                    updated_size = special_shape_size_bound[special_shape_super][1]\n",
    "                sampled_dis = Object(\n",
    "                    color=sampled_dis.color,\n",
    "                    size=updated_size,\n",
    "                    shape=sampled_dis.shape\n",
    "                )\n",
    "            elif special_shape_sub in special_shape_size_bound.keys():\n",
    "                if \"small\" in dis_str:\n",
    "                    updated_size = special_shape_size_bound[special_shape_sub][0]\n",
    "                else:\n",
    "                    updated_size = special_shape_size_bound[special_shape_sub][1]\n",
    "                sampled_dis = Object(\n",
    "                    color=sampled_dis.color,\n",
    "                    size=updated_size,\n",
    "                    shape=sampled_dis.shape\n",
    "                )\n",
    "            else:\n",
    "                pass # Do nothing.\n",
    "            distractors_sampled_obj_map[dis_grammer] = sampled_dis\n",
    "        \n",
    "        # 2. Update it using relationships.\n",
    "        for pair, rel in distractors_rel_map.items():\n",
    "            if rel == \"$SAME_SHAPE\":\n",
    "                pass\n",
    "            elif rel == \"$SAME_COLOR\":\n",
    "                pass\n",
    "            elif rel == \"$SAME_SIZE\":\n",
    "                # Update the src node size information.\n",
    "                size = distractors_sampled_obj_map[pair[1]].size\n",
    "                distractors_sampled_obj_map[pair[0]] = Object(\n",
    "                    color=distractors_sampled_obj_map[pair[0]].color,\n",
    "                    size=size,\n",
    "                    shape=distractors_sampled_obj_map[pair[0]].shape\n",
    "                )\n",
    "            elif rel == \"$IS_INSIDE\":\n",
    "                pass # Do nothing!\n",
    "\n",
    "        placed_dis_grammer = []\n",
    "        for dis_grammer, sampled_dis in distractors_sampled_obj_map.items():\n",
    "            # 2. Place on the world map.\n",
    "            sampled_pos = self.sample_object_position(\n",
    "                sampled_dis, distractor_root, \n",
    "                dis_grammer, distractors_rel_map, \n",
    "                obj_placed_map, obj_position_map\n",
    "            )\n",
    "\n",
    "            if sampled_dis == -1 or sampled_pos == -1:\n",
    "                # We allow partial placement as they add difficulties!\n",
    "                return False\n",
    "\n",
    "            return_code = self._world.place_object(\n",
    "                sampled_dis, \n",
    "                position=sampled_pos, target=False # Distractor is never the target!\n",
    "            )\n",
    "            if return_code == -1:\n",
    "                assert False # Due to our design, this should never happen!\n",
    "            obj_placed_map[dis_grammer] = sampled_dis\n",
    "            obj_position_map[dis_grammer] = sampled_pos\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def sample_situations_from_grounded_grammer(\n",
    "        self, grammer_pattern, \n",
    "        obj_pattern_map, rel_map, obj_map, root=\"$OBJ_0\", \n",
    "        is_plot=False, \n",
    "        include_random_distractor=False, \n",
    "        include_relation_distractor=False, \n",
    "        include_attribute_distractor=False, \n",
    "        include_isomorphism_distractor=False, \n",
    "        full_relation_probability=0.5,\n",
    "        debug=False,\n",
    "    ):\n",
    "        # Clear current world.\n",
    "        self._world.clear_situation()\n",
    "        \n",
    "        # Start placing objects with specs.\n",
    "        obj_placed_map = OrderedDict({})\n",
    "        obj_position_map = OrderedDict({})\n",
    "        referred_obj = root\n",
    "        \n",
    "        # Preliminary size check!\n",
    "        \"\"\"\n",
    "        Here is a list of potential internal conflicts:\n",
    "        (1) ... to a small box ... to a yellow box ...\n",
    "        Explain: we need to adjust the size of two boxes\n",
    "        so that small box has 1 size, and all other boxes \n",
    "        have the same other size.\n",
    "        There will at max two different size of same type objects.\n",
    "        \n",
    "        So this is the rule:\n",
    "        For 1 type of shape, max two different sizes.\n",
    "        \"\"\"\n",
    "        # Ok, we need to determine shapes first!\n",
    "        # Even there is any abstract object, the\n",
    "        # shape is now determined.\n",
    "        object_map = {}\n",
    "        mentioned_shapes = set([]) # this is used to sample shapes for object.\n",
    "        for obj_grammer, obj_str in obj_map.items():\n",
    "            shape = self.extract_shape(obj_str)\n",
    "            if shape != \"\":\n",
    "                mentioned_shapes.add(shape) # For abstract mention, shape is determined later!\n",
    "        for obj_grammer, obj_str in obj_map.items():\n",
    "            # 1. Sample object.\n",
    "            sampled_obj = self.sample_object_spec(\n",
    "                obj_grammer,\n",
    "                obj_pattern_map[obj_grammer], obj_str, rel_map, \n",
    "                is_root=obj_grammer==root, \n",
    "                obj_placed_map=object_map,\n",
    "                mentioned_shapes=mentioned_shapes,\n",
    "            )\n",
    "            object_map[obj_grammer] = sampled_obj\n",
    "        \n",
    "        # Next, we update all of them based on relations.\n",
    "        # Final pass, we need to change attributes of objects based\n",
    "        # on relations.\n",
    "        # Here, we only change size!\n",
    "        for pair, rel in rel_map.items():\n",
    "            if rel == \"$SAME_SHAPE\":\n",
    "                # Update the src node shape information.\n",
    "                shape = object_map[pair[1]].shape\n",
    "                object_map[pair[0]] = Object(\n",
    "                    color=object_map[pair[0]].color,\n",
    "                    size=object_map[pair[0]].size,\n",
    "                    shape=shape\n",
    "                )\n",
    "                pass\n",
    "            elif rel == \"$SAME_COLOR\":\n",
    "                # Update the src node color information.\n",
    "                color = object_map[pair[1]].color\n",
    "                object_map[pair[0]] = Object(\n",
    "                    color=color,\n",
    "                    size=object_map[pair[0]].size,\n",
    "                    shape=object_map[pair[0]].shape\n",
    "                )\n",
    "                pass\n",
    "            elif rel == \"$SAME_SIZE\":\n",
    "                # Update the src node size information.\n",
    "                size = object_map[pair[1]].size\n",
    "                object_map[pair[0]] = Object(\n",
    "                    color=object_map[pair[0]].color,\n",
    "                    size=size,\n",
    "                    shape=object_map[pair[0]].shape\n",
    "                )\n",
    "            elif rel == \"$IS_INSIDE\":\n",
    "                pass\n",
    "            \n",
    "        # Then, we will determine size bounds.\n",
    "        special_shape_size_bound = {}\n",
    "        for obj_grammer, obj_pattern in obj_pattern_map.items():\n",
    "            \n",
    "            small_size = random.randint(\n",
    "                self.object_vocabulary._min_size, \n",
    "                self.object_vocabulary._max_size-1\n",
    "            )\n",
    "            big_size = random.randint(\n",
    "                small_size+1, \n",
    "                self.object_vocabulary._max_size\n",
    "            )\n",
    "            \n",
    "            if \"$SIZE\" in obj_pattern and \"$COLOR\" in obj_pattern:\n",
    "                special_shape = object_map[obj_grammer].color + \" \" + object_map[obj_grammer].shape\n",
    "                if object_map[obj_grammer].shape in special_shape_size_bound.keys():\n",
    "                    # e.g., small circle exists\n",
    "                    special_shape_size_bound[special_shape] = special_shape_size_bound[object_map[obj_grammer].shape]\n",
    "                else:\n",
    "                    # e.g., small yellow circle\n",
    "                    special_shape_size_bound[special_shape] = [small_size, big_size]\n",
    "            elif \"$SIZE\" in obj_pattern and not \"$COLOR\" in obj_pattern:\n",
    "                # e.g., small circle\n",
    "                # overwrite any existing bounds.\n",
    "                special_shape = object_map[obj_grammer].shape\n",
    "                for ss, bound in special_shape_size_bound.items():\n",
    "                    if special_shape in ss:\n",
    "                        special_shape_size_bound[ss] = [small_size, big_size]\n",
    "                # for shape, it adds.\n",
    "                special_shape_size_bound[special_shape] = [small_size, big_size]\n",
    "                # for non-sized shape, it also adds as long as shape is the same.\n",
    "                for obj_grammer, obj_pattern in obj_pattern_map.items():\n",
    "                    if special_shape in obj_map[obj_grammer]:\n",
    "                        if \"$COLOR\" in obj_pattern:\n",
    "                            special_shape = object_map[obj_grammer].color + \" \" + object_map[obj_grammer].shape\n",
    "                            special_shape_size_bound[special_shape] = [small_size, big_size]\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Update object size based on global scanning results.\n",
    "        updated_object_map = {}\n",
    "        for obj_grammer, obj_pattern in obj_pattern_map.items():\n",
    "\n",
    "            special_shape_super = object_map[obj_grammer].shape\n",
    "            special_shape_sub = object_map[obj_grammer].color + \" \" + object_map[obj_grammer].shape\n",
    "            \n",
    "            # e.g., small circle exists in the command, then any colored circle needs to be constrain\n",
    "            if special_shape_super in special_shape_size_bound.keys():\n",
    "                if \"small\" in obj_map[obj_grammer]:\n",
    "                    updated_size = special_shape_size_bound[special_shape_super][0]\n",
    "                else:\n",
    "                    updated_size = special_shape_size_bound[special_shape_super][1]\n",
    "                updated_object_map[obj_grammer] = Object(\n",
    "                    color=object_map[obj_grammer].color,\n",
    "                    size=updated_size,\n",
    "                    shape=object_map[obj_grammer].shape\n",
    "                )\n",
    "            elif special_shape_sub in special_shape_size_bound.keys():\n",
    "                if \"small\" in obj_map[obj_grammer]:\n",
    "                    updated_size = special_shape_size_bound[special_shape_sub][0]\n",
    "                else:\n",
    "                    updated_size = special_shape_size_bound[special_shape_sub][1]\n",
    "                updated_object_map[obj_grammer] = Object(\n",
    "                    color=object_map[obj_grammer].color,\n",
    "                    size=updated_size,\n",
    "                    shape=object_map[obj_grammer].shape\n",
    "                )\n",
    "            else:\n",
    "                # If nothing exists in the special size map, then we don't need\n",
    "                # to alter the size.\n",
    "                updated_object_map[obj_grammer] = object_map[obj_grammer]\n",
    "\n",
    "        # Final pass, we need to change attributes of objects based\n",
    "        # on relations.\n",
    "        # Here, we only change size!\n",
    "        for pair, rel in rel_map.items():\n",
    "            if rel == \"$SAME_SHAPE\":\n",
    "                pass\n",
    "            elif rel == \"$SAME_COLOR\":\n",
    "                pass\n",
    "            elif rel == \"$SAME_SIZE\":\n",
    "                # Update the src node size information.\n",
    "                size = updated_object_map[pair[1]].size\n",
    "                updated_object_map[pair[0]] = Object(\n",
    "                    color=object_map[pair[0]].color,\n",
    "                    size=size,\n",
    "                    shape=object_map[pair[0]].shape\n",
    "                )\n",
    "            elif rel == \"$IS_INSIDE\":\n",
    "                pass\n",
    "        \n",
    "        # Next, we sample positions of all objects and place them.\n",
    "        for obj_grammer, obj_str in obj_map.items():\n",
    "            # 1. Sample object (bu fetching the updated one).\n",
    "            sampled_obj = updated_object_map[obj_grammer]\n",
    "            \n",
    "            # 2. Place on the world map.\n",
    "            sampled_pos = self.sample_object_position(\n",
    "                sampled_obj, root, obj_grammer, rel_map, \n",
    "                obj_placed_map, obj_position_map\n",
    "            )\n",
    "            \n",
    "            if sampled_obj == -1 or sampled_pos == -1:\n",
    "                assert False # we can assert false as it is impossible!\n",
    "        \n",
    "            self._world.place_object(\n",
    "                sampled_obj, \n",
    "                position=sampled_pos, target=obj_grammer==root\n",
    "            )\n",
    "            obj_placed_map[obj_grammer] = sampled_obj\n",
    "            obj_position_map[obj_grammer] = sampled_pos\n",
    "        \n",
    "        # Update the shape mentions for later use.\n",
    "        for obj_grammer, obj in obj_placed_map.items():\n",
    "            mentioned_shapes.add(obj.shape)\n",
    "            # maybe let us add color + shape to be more specific\n",
    "            mentioned_shapes.add(obj.color + \" \" + obj.shape)\n",
    "            # we also need to be more strict.\n",
    "            # if we have something like blue object\n",
    "            # $COLOR + $ABS_SHAPE, then\n",
    "            # we cannot have that color for any shape i think!\n",
    "            if \"$COLOR\" in obj_pattern_map[obj_grammer] and \\\n",
    "                \"$ABS_SHAPE\" in obj_pattern_map[obj_grammer]:\n",
    "                for shape in self.object_vocabulary._shapes:\n",
    "                    mentioned_shapes.add(obj.color + \" \" + shape)\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        Distractor Sampling Strategies and Design\n",
    "        \n",
    "        Giving a complex command as:\n",
    "        \"the small red circle(1) that is in the same row(a) \n",
    "        as a big green square(2) and that is in the same column(b) \n",
    "        as a small yellow cylinder(3).\"\n",
    "        \n",
    "        We have 4 types of distractors (objects):\n",
    "        - Attribute-based Distractors\n",
    "        - Relation-based Distractors\n",
    "        - Sytax-based Distractors\n",
    "        - Random Distractors\n",
    "        \n",
    "        For each type of distractors, we will modify the command\n",
    "        to generate a new command for distractors. Then, we will\n",
    "        ensure such every command-world pair needs to reason about\n",
    "        attribute, relation and syntax. \n",
    "        \n",
    "        There are some caveats around this design. Due to the \n",
    "        complexity of the command, to make sure\n",
    "        every attribute/relation is necessary becomes unfeasible. For\n",
    "        example, if we want to make \"small\" in \"the small red circle (1)\"\n",
    "        necessary, then, we need to put another non-\"small\" \"red circle\".\n",
    "        This is easy. However, if we want to make \"big\" in \"the big\n",
    "        green square\" necessary, we essentially need to sample another\n",
    "        set of objects (at max 3) that complies with a modified command\n",
    "        \"the small red circle(1) that is in the same row(a) \n",
    "        as a small green square(2*) and that is in the same column(b) \n",
    "        as a small yellow cylinder(3).\"\n",
    "        \n",
    "        Following this logic, if we want to make sure *every descriptor*\\\n",
    "        (i.e., every adjective) is necessary to identity the referent\n",
    "        target, we could flood the system easily with way too many\n",
    "        distractors that cannot fit in our grid world.\n",
    "        \n",
    "        On the other hand, the goal of having the distractors is to\n",
    "        have the system learn the importantce of relations, attributes\n",
    "        and linguistic syntax. So, are these distractors necessary?\n",
    "        Do we need to actually have an exhaustive list of distractors\n",
    "        for each command-world pair in order to have the model to learn\n",
    "        this? We propose the answer is No, but Yes in the dataset level. In\n",
    "        the command level, we will not make sure *every descriptor* is\n",
    "        necessary, but in the command level, we will make sure \n",
    "        *every descriptor* matters for at least some of the command.\n",
    "        Otherwise, the model may just completely ignores one part of\n",
    "        the command and relies on the rest.\n",
    "        \n",
    "        In our design, we ensure for each command-world pair, some attribute\n",
    "        and some relation and some syntax are needed. In the dataset\n",
    "        level, we ensure different attribute, relation and syntax are \n",
    "        weighted equally.\n",
    "        \n",
    "        We propose to sample distractors following the design below:\n",
    "        \n",
    "        For a command such as\n",
    "        \"A that is X B and that is Y C\"\n",
    "        (1) We generate two distractor commands: \"A that X B\"; and \"A that Y C\"\n",
    "        without guarantee all relations in the original command. This samples\n",
    "        4 distractors. This ensures X and Y are necessary!\n",
    "        \n",
    "        (2) Next, we need to ensure that if we change some descriptors for\n",
    "        A, B or C, referent target cannot be identified. For example, if\n",
    "        we change B from \"yellow square\" to \"blue square\" the referent target\n",
    "        should change. In this case, we need to sample a new set of {A,B,C}.\n",
    "        And if we do this for each object, this results in 9 new distractors.\n",
    "        If size is not selected, we potentially need 3 more distractors to\n",
    "        ground the size aspects.\n",
    "        \n",
    "        (3) Next, to ensure model learns linguistic syntax, instead of simple\n",
    "        BoW approach to represent the command, we would perform swap attributes\n",
    "        between objects. We pick a pair of objects, and swap attributes \n",
    "        randomly. This results in 3 more distractors.\n",
    "        \n",
    "        (1) + (2) + (3) results in at max 19 distractors for each command-world pair.\n",
    "        Plus the original 3 objects, we have in total 21 distractors.\n",
    "        This is still a lot higher than gSCAN which is at max about 12.\n",
    "        \n",
    "        Then, we design another way to sample distractors:\n",
    "        (1) We pick 1 relations from {X, Y}, and generate distracotrs: 3 distractors.\n",
    "        \n",
    "        (2) We pick 1 object from {A, B, C} and modify its attribute, sample 3 distractors.\n",
    "        if size is not selected for any object, we need to randomly sample non-relational\n",
    "        counterparts, at max 3.\n",
    "        \n",
    "        (3) Same, so 3.\n",
    "        \n",
    "        (1) + (2) + (3) results in 3 + 3 + 3 + 3 = 12, 12 + 3 -> at max 15. Is this doable?\n",
    "        \n",
    "        Test set. global v.s. local compositional generalization. In the test set, we \n",
    "        can pick different/more aspect of differeent/more obj that matter for the\n",
    "        correctly reasonings, and generate test cases with  more distractors.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Calling in this way to create distractors:\n",
    "\n",
    "        simulator.sample_distractor_grammer_by_relation(\n",
    "            grammer_pattern, \n",
    "            obj_pattern_map, \n",
    "            rel_map, \n",
    "            obj_map, \n",
    "            sampled_world\n",
    "        )\n",
    "        \"\"\"\n",
    "        temp_sampled_world = {\n",
    "            \"obj_map\" : obj_placed_map,\n",
    "            \"pos_map\" : obj_position_map,\n",
    "            \"referred_obj\" : referred_obj,\n",
    "            \"situation\" : copy.deepcopy(self._world.get_current_situation())\n",
    "        }\n",
    "        \n",
    "        # Three types of distractor sampling for different purposes:\n",
    "        # sample_distractor_grammer_by_relation()\n",
    "        # - We will edit one leaf node, so that it makes sure\n",
    "        #   the command is necessary!\n",
    "        # sample_distractor_grammer_by_size()\n",
    "        # - Size relatives need to be meaningful. We will add relational\n",
    "        #   objects to make sure.\n",
    "        # sample_distractor_grammer_by_isomorphism()\n",
    "        # - This is to ensure syntax learning.\n",
    "        \n",
    "        distractor_switch_map = OrderedDict({\n",
    "            \"relation\" : [],\n",
    "            \"attribute\" : False,\n",
    "            \"isomorphism\" : False, \n",
    "            \"random\" : False,\n",
    "        })\n",
    "        relation_distractors_dicts = [{\n",
    "            \"distractor_metadata\": {}\n",
    "        }]\n",
    "        attribute_distractors_dicts = [{\n",
    "            \"distractor_metadata\": {}\n",
    "        }]\n",
    "        isomorphism_distractors_dicts = [{\n",
    "            \"distractor_metadata\": {}\n",
    "        }]\n",
    "        \n",
    "        if random.random() < full_relation_probability:\n",
    "            full_relation_set=True\n",
    "        else:\n",
    "            full_relation_set=False\n",
    "            \n",
    "        obj_drafted_count = len(obj_placed_map)\n",
    "        if include_relation_distractor:\n",
    "            \"\"\"\n",
    "            Relation Distractors: Count=3*n, at max 6.\n",
    "            Relation Distractors (fast): Count=2*n, at max 4.\n",
    "            \n",
    "            sample_distractor_grammer_by_relation_fast\n",
    "            \n",
    "            sample_distractor_grammer_by_relation\n",
    "            \"\"\"\n",
    "            relation_distractors_dicts = self.sample_distractor_grammer_by_relation_fast(\n",
    "                grammer_pattern, \n",
    "                obj_pattern_map, \n",
    "                rel_map, \n",
    "                obj_map, \n",
    "                temp_sampled_world,\n",
    "                obj_base_count=len(obj_placed_map),\n",
    "                full_set=full_relation_set,\n",
    "            )\n",
    "            if len(relation_distractors_dicts) == 0:\n",
    "                pass # Size distractor is not applicable \n",
    "            else:\n",
    "                distractor_switch = []\n",
    "                is_full_relation = True\n",
    "                for distractors_dict in relation_distractors_dicts:\n",
    "                    obj_drafted_count += len(distractors_dict[\"obj_map\"])\n",
    "                    succeed = self.place_distractor_from_dict(\n",
    "                        distractors_dict, \n",
    "                        obj_placed_map, \n",
    "                        obj_position_map,\n",
    "                        debug=debug,\n",
    "                        special_shape_size_bound=special_shape_size_bound,\n",
    "                        mentioned_shapes=mentioned_shapes,\n",
    "                        # This is needed as maybe distractors also \n",
    "                        # need to be bounded by global constraints.\n",
    "                    )\n",
    "                    if succeed:\n",
    "                        distractor_switch_map[\"relation\"].append(True)\n",
    "                    else:\n",
    "                        distractor_switch_map[\"relation\"].append(False)\n",
    "\n",
    "        if include_attribute_distractor:\n",
    "            \"\"\"\n",
    "            Attribution Distractors: Count=3-6.\n",
    "            \"\"\"\n",
    "            # If the command is small, we can overwrite this\n",
    "            full_set = not full_relation_set\n",
    "            attribute_distractors_dicts = self.sample_distractor_grammer_by_attribute(\n",
    "                grammer_pattern, \n",
    "                obj_pattern_map, \n",
    "                rel_map, \n",
    "                obj_map, \n",
    "                temp_sampled_world,\n",
    "                special_shape_size_bound,\n",
    "                obj_base_count=obj_drafted_count, # This is important, as previous draft may success but placement can fail!\n",
    "                full_set=True, # always include some attribute based distractors.\n",
    "            )\n",
    "            if len(attribute_distractors_dicts) == 0:\n",
    "                pass # Size distractor is not applicable \n",
    "            else:\n",
    "                obj_drafted_count += len(attribute_distractors_dicts[0][\"obj_map\"])\n",
    "                succeed = self.place_distractor_from_dict(\n",
    "                    attribute_distractors_dicts[0], \n",
    "                    obj_placed_map, \n",
    "                    obj_position_map,\n",
    "                    debug=debug,\n",
    "                    special_shape_size_bound=special_shape_size_bound,\n",
    "                    mentioned_shapes=mentioned_shapes,\n",
    "                    # This is needed as maybe distractors also \n",
    "                    # need to be bounded by global constraints.\n",
    "                )\n",
    "                if succeed:\n",
    "                    if attribute_distractors_dicts[0][\"grammer_pattern\"] != \"DUMMY\":\n",
    "                        distractor_switch_map[\"attribute\"] = True # If one time it is true, it is true.\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            # we need to ground size.\n",
    "            attribute_distractors_dicts = self.sample_distractor_grammer_by_attribute(\n",
    "                grammer_pattern, \n",
    "                obj_pattern_map, \n",
    "                rel_map, \n",
    "                obj_map, \n",
    "                temp_sampled_world,\n",
    "                special_shape_size_bound,\n",
    "                obj_base_count=obj_drafted_count, # This is important, as previous draft may success but placement can fail!\n",
    "                full_set=False,\n",
    "            )\n",
    "            if len(attribute_distractors_dicts) == 0:\n",
    "                pass # Size distractor is not applicable \n",
    "            else:\n",
    "                obj_drafted_count += len(attribute_distractors_dicts[0][\"obj_map\"])\n",
    "                succeed = self.place_distractor_from_dict(\n",
    "                    attribute_distractors_dicts[0], \n",
    "                    obj_placed_map, \n",
    "                    obj_position_map,\n",
    "                    debug=debug,\n",
    "                    special_shape_size_bound=special_shape_size_bound,\n",
    "                    mentioned_shapes=mentioned_shapes,\n",
    "                    # This is needed as maybe distractors also \n",
    "                    # need to be bounded by global constraints.\n",
    "                )\n",
    "                if succeed:\n",
    "                    if attribute_distractors_dicts[0][\"grammer_pattern\"] != \"DUMMY\":\n",
    "                        distractor_switch_map[\"attribute\"] = True # If one time it is true, it is true.\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        if include_isomorphism_distractor:\n",
    "            \"\"\"\n",
    "            Syntax Distractors: Count=3.\n",
    "            \"\"\"\n",
    "            isomorphism_distractors_dicts = self.sample_distractor_grammer_by_isomorphism(\n",
    "                grammer_pattern, \n",
    "                obj_pattern_map, \n",
    "                rel_map, \n",
    "                obj_map, \n",
    "                temp_sampled_world,\n",
    "                obj_base_count=obj_drafted_count # This is important, as previous draft may success but placement can fail!\n",
    "            )\n",
    "            if len(isomorphism_distractors_dicts) == 0:\n",
    "                pass # Size distractor is not applicable \n",
    "            else:\n",
    "                obj_drafted_count += len(isomorphism_distractors_dicts[0][\"obj_map\"])\n",
    "                succeed = self.place_distractor_from_dict(\n",
    "                    isomorphism_distractors_dicts[0], \n",
    "                    obj_placed_map, \n",
    "                    obj_position_map,\n",
    "                    debug=debug,\n",
    "                    special_shape_size_bound=special_shape_size_bound,\n",
    "                    mentioned_shapes=mentioned_shapes,\n",
    "                    # This is needed as maybe distractors also \n",
    "                    # need to be bounded by global constraints.\n",
    "                )\n",
    "                if succeed:\n",
    "                    distractor_switch_map[\"isomorphism\"] = True\n",
    "        \n",
    "        # Probably never need this!\n",
    "        \"\"\"\n",
    "        Random Distractors.\n",
    "        \"\"\"\n",
    "        # Place random distractors. These are gSCAN like distractors\n",
    "        # which are often not very meaningful for testing agents language\n",
    "        # knowledge. We recommand always turn this off and use other\n",
    "        # relation-based distractor sampling strategies.\n",
    "        \n",
    "        random_distractor_metadata = {}\n",
    "        n_random_distractor = -1\n",
    "        if include_random_distractor:\n",
    "            if len(obj_placed_map) >= self.n_object_max:\n",
    "                pass # Do nothing!\n",
    "            else:\n",
    "                n_distractor = self.n_object_max-len(obj_placed_map) # for simplier command, maybe all of them will be random?\n",
    "                n_random_distractor = n_distractor\n",
    "                core_obj_count = obj_drafted_count\n",
    "                for i in range(0, n_distractor):\n",
    "                    distractor_idx = core_obj_count+i\n",
    "                    distractor_name = f\"$OBJ_{distractor_idx}\"\n",
    "\n",
    "                    # Let us only sample shapes that are not exist\n",
    "                    sampled_distractor = self.sample_random_object_spec(\n",
    "                        shape_exclude=list(mentioned_shapes),\n",
    "                        combo_exclude=list(mentioned_shapes)\n",
    "                    )\n",
    "                    \n",
    "                    # Ok, we need to consider global size constraint!\n",
    "                    special_shape_super = sampled_distractor.shape\n",
    "                    special_shape_sub = sampled_distractor.color + \" \" +sampled_distractor.shape\n",
    "\n",
    "                    # e.g., small circle exists in the command, then any colored circle needs to be constrain\n",
    "                    size_idx = -1\n",
    "                    if special_shape_super in special_shape_size_bound.keys():\n",
    "                        size_idx = random.randint(0,1)\n",
    "                        updated_size = special_shape_size_bound[special_shape_super][size_idx]\n",
    "                        sampled_distractor = Object(\n",
    "                            color=sampled_distractor.color,\n",
    "                            size=updated_size,\n",
    "                            shape=sampled_distractor.shape\n",
    "                        )\n",
    "                    elif special_shape_sub in special_shape_size_bound.keys():\n",
    "                        size_idx = random.randint(0,1)\n",
    "                        updated_size = special_shape_size_bound[special_shape_sub][size_idx]\n",
    "                        sampled_distractor = Object(\n",
    "                            color=sampled_distractor.color,\n",
    "                            size=updated_size,\n",
    "                            shape=sampled_distractor.shape\n",
    "                        )\n",
    "                    \n",
    "                    if sampled_distractor.shape == \"box\":\n",
    "                        sampled_dis_pos = self._world.sample_position_complex(\n",
    "                            condition=\"box\", box_size=sampled_distractor.size, sample_one=True\n",
    "                        )\n",
    "                    else:\n",
    "                        sampled_dis_pos = self._world.sample_position_complex(\n",
    "                            condition=\"normal\", sample_one=True\n",
    "                        )\n",
    "                    \n",
    "                    self._world.place_object(\n",
    "                        sampled_distractor, \n",
    "                        position=sampled_dis_pos, target=False\n",
    "                    )\n",
    "                    obj_placed_map[distractor_name] = sampled_distractor\n",
    "                    obj_position_map[distractor_name] = sampled_dis_pos\n",
    "                    size_str = \"\"\n",
    "                    if size_idx != -1:\n",
    "                        size_str = \"big\" if size_idx == 1 else \"small\"\n",
    "                    random_distractor_metadata[distractor_name] = \" \".join([\n",
    "                        size_str,\n",
    "                        sampled_distractor.color,\n",
    "                        sampled_distractor.shape\n",
    "                    ])\n",
    "                distractor_switch_map[\"random\"] = True\n",
    "\n",
    "        agent_position = self._world.sample_position_complex(\n",
    "                            condition=\"normal\", sample_one=True\n",
    "                        )\n",
    "        self._world.place_agent_at(agent_position)\n",
    "        if is_plot:\n",
    "            _ = self._world.render_simple()\n",
    "        \n",
    "        situation_snapshot = copy.deepcopy(self._world.get_current_situation())\n",
    "        \n",
    "        return {\n",
    "            \"obj_map\" : obj_placed_map,\n",
    "            \"pos_map\" : obj_position_map,\n",
    "            \"obj_pattern_map\" : obj_pattern_map,\n",
    "            \"referred_obj\" : referred_obj,\n",
    "            \"situation\" : situation_snapshot, \n",
    "            \"distractor_switch_map\" : distractor_switch_map,\n",
    "            \"relation_distractor_metadata\" : [\n",
    "                {\n",
    "                    \"distractor_metadata\": md[\"distractor_metadata\"],\n",
    "                    # \"obj_map\": md[\"obj_map\"],\n",
    "                    # \"rel_map\": md[\"rel_map\"],\n",
    "                } for md in relation_distractors_dicts\n",
    "            ],\n",
    "            \"attribute_distractor_metadata\" : [\n",
    "                {\n",
    "                    \"distractor_metadata\": md[\"distractor_metadata\"],\n",
    "                    # \"obj_map\": md[\"obj_map\"],\n",
    "                    # \"rel_map\": md[\"rel_map\"],\n",
    "                } for md in attribute_distractors_dicts\n",
    "            ],\n",
    "            \"isomorphism_distractor_metadata\" : [\n",
    "                {\n",
    "                    \"distractor_metadata\": md[\"distractor_metadata\"],\n",
    "                    # \"obj_map\": md[\"obj_map\"],\n",
    "                    # \"rel_map\": md[\"rel_map\"],\n",
    "                } for md in isomorphism_distractors_dicts\n",
    "            ],\n",
    "            \"random_distractor_metadata\" : [random_distractor_metadata],\n",
    "            \"n_random_distractor\" : n_random_distractor\n",
    "        }\n",
    "    \n",
    "    def get_action_list(\n",
    "        self,\n",
    "        verb=None,\n",
    "        adverb=None,\n",
    "    ):\n",
    "        pass\n",
    "    \n",
    "    def extract_size(self, obj_str):\n",
    "        obj_descriptors = obj_str.split(\" \")\n",
    "        for descriptor in obj_descriptors:\n",
    "            if descriptor in [\"small\", \"big\"]:\n",
    "                return descriptor\n",
    "        return \"\"\n",
    "\n",
    "    def extract_color(self, obj_str):\n",
    "        obj_descriptors = obj_str.split(\" \")\n",
    "        for descriptor in obj_descriptors:\n",
    "            if descriptor in self.object_vocabulary.object_colors:\n",
    "                return descriptor\n",
    "        return \"\"\n",
    "    \n",
    "    def extract_shape(self, obj_str):\n",
    "        obj_descriptors = obj_str.split(\" \")\n",
    "        for descriptor in obj_descriptors:\n",
    "            if descriptor in self.object_vocabulary.object_shapes:\n",
    "                return descriptor\n",
    "        return \"\"\n",
    "\n",
    "    def convert_object_str_to_grammer(self, obj_str):\n",
    "        size_g = False\n",
    "        color_g = False\n",
    "        abs_shape_g = False\n",
    "\n",
    "        obj_descriptors = obj_str.split(\" \")\n",
    "        for descriptor in obj_descriptors:\n",
    "            if descriptor in [\"small\", \"big\"]:\n",
    "                size_g = True\n",
    "            elif descriptor in self.object_vocabulary.object_colors:\n",
    "                color_g = True\n",
    "            elif descriptor in self.object_vocabulary.object_shapes:\n",
    "                pass\n",
    "            elif descriptor == \"object\":\n",
    "                abs_shape_g = True\n",
    "\n",
    "        grammer = []\n",
    "        if size_g:\n",
    "            grammer.append(\"$SIZE\")\n",
    "        if color_g:\n",
    "            grammer.append(\"$COLOR\")\n",
    "        if abs_shape_g:\n",
    "            grammer.append(\"$ABS_SHAPE\") # Mark as deprecated!\n",
    "        else:\n",
    "            grammer.append(\"$SHAPE\")\n",
    "        \n",
    "        return \" \".join(grammer)\n",
    "\n",
    "    def snap_pattern_to_referent_map(self, distractor_grammer_pattern, base_count):\n",
    "        distractor_grammer_pattern_snapped = []\n",
    "        for item in distractor_grammer_pattern.split(\" \"):\n",
    "            if item.startswith(\"$\"):\n",
    "                new_id = int(item.split(\"_\")[1])+base_count\n",
    "                distractor_grammer_pattern_snapped.append(f\"$OBJ_{new_id}\")\n",
    "            else:\n",
    "                distractor_grammer_pattern_snapped.append(item)\n",
    "        return \" \".join(distractor_grammer_pattern_snapped)\n",
    "\n",
    "    def snap_object_map_to_referent_map(self, distractor_map, base_count):\n",
    "        distractor_map_snapped = OrderedDict({})\n",
    "        for obj_name, item in distractor_map.items():\n",
    "            new_id = int(obj_name.split(\"_\")[1])+base_count\n",
    "            new_obj_name = f\"$OBJ_{new_id}\"\n",
    "            distractor_map_snapped[new_obj_name] = item\n",
    "        return distractor_map_snapped\n",
    "\n",
    "    def snap_relation_map_to_referent_map(self, distractor_rel_map, base_count):\n",
    "        distractor_rel_map_snapped = OrderedDict({})\n",
    "        for edge, item in distractor_rel_map.items():\n",
    "            if edge[0].startswith(\"$\"):\n",
    "                new_id_left = int(edge[0].split(\"_\")[1])+base_count\n",
    "                new_obj_name_left = f\"$OBJ_{new_id_left}\"\n",
    "            else:\n",
    "                new_obj_name_left = edge[0]\n",
    "            \n",
    "            if edge[1].startswith(\"$\"):\n",
    "                new_id_right = int(edge[1].split(\"_\")[1])+base_count\n",
    "                new_obj_name_right = f\"$OBJ_{new_id_right}\"\n",
    "            else:\n",
    "                new_obj_name_right = edge[1]\n",
    "            distractor_rel_map_snapped[(new_obj_name_left, new_obj_name_right)] = item\n",
    "        return distractor_rel_map_snapped\n",
    "\n",
    "    def sample_distractor_grammer_by_relation_fast(\n",
    "        self, \n",
    "        referent_grammer_pattern, \n",
    "        referent_obj_pattern_map,\n",
    "        referent_rel_map,\n",
    "        referent_obj_map, \n",
    "        sampled_world,\n",
    "        obj_base_count=0,\n",
    "        full_set=True,\n",
    "        stress_test=False, # Open this for our own purpose only.\n",
    "    ):\n",
    "        \"\"\"\n",
    "        You can choose between two versions.\n",
    "        In this fast version, we only sample objects for the \n",
    "        selected edge.\n",
    "        \"\"\"\n",
    "        distractors_dicts = []\n",
    "        # We first collect all the relations\n",
    "        relation_edges = []\n",
    "        for edge, relation in referent_rel_map.items():\n",
    "            # I don't want to sample box again\n",
    "            # this will add too many box at the end!\n",
    "            relation_edges.append(edge)\n",
    "        # if there is only 1 relation, do we really need it?\n",
    "        # probably not?\n",
    "        if stress_test:\n",
    "            if len(relation_edges) == 1:\n",
    "                for i in range(10):\n",
    "                    # for this, we simple sample another obj_0?\n",
    "                    distractor_size_map = {}\n",
    "                    distractor_obj_pattern_map = {}\n",
    "                    distractor_obj_map = {}\n",
    "                    distractor_rel_map = OrderedDict({})\n",
    "\n",
    "                    distractor_size_map[\"$OBJ_0\"] = int(sampled_world[\"obj_map\"][\"$OBJ_0\"].size)\n",
    "                    distractor_obj_pattern_map[\"$OBJ_0\"] = referent_obj_pattern_map[\"$OBJ_0\"]\n",
    "                    distractor_obj_map[\"$OBJ_0\"] = referent_obj_map[\"$OBJ_0\"]\n",
    "                    distractor_grammer_pattern = referent_grammer_pattern.split(\" \")[0]\n",
    "\n",
    "                    distractor_metadata = {\n",
    "                        \"edge\" : \"null\",\n",
    "                        \"relation_old_type\" : \"null\", # omit relations\n",
    "                        \"full_set\" : True,\n",
    "                    }\n",
    "\n",
    "                    distractors_dicts += [{\n",
    "                                            \"grammer_pattern\" : self.snap_pattern_to_referent_map(\n",
    "                                                distractor_grammer_pattern,\n",
    "                                                obj_base_count\n",
    "                                            ),\n",
    "                                            \"obj_pattern_map\" : self.snap_object_map_to_referent_map(\n",
    "                                                distractor_obj_pattern_map,\n",
    "                                                obj_base_count\n",
    "                                            ),\n",
    "                                            \"rel_map\" : self.snap_relation_map_to_referent_map(\n",
    "                                                distractor_rel_map,\n",
    "                                                obj_base_count\n",
    "                                            ),\n",
    "                                            \"obj_map\" : self.snap_object_map_to_referent_map(\n",
    "                                                distractor_obj_map,\n",
    "                                                obj_base_count\n",
    "                                            ),\n",
    "                                            \"size_map\" : self.snap_object_map_to_referent_map(\n",
    "                                                distractor_size_map,\n",
    "                                                obj_base_count\n",
    "                                            ),\n",
    "                                            \"distractor_metadata\" : distractor_metadata\n",
    "                                        }]\n",
    "                    obj_base_count += len(distractor_obj_pattern_map)\n",
    "            elif len(relation_edges) == 2:\n",
    "                for i in range(0, 2):\n",
    "                    existing_relations = set([v for k, v in referent_rel_map.items()])\n",
    "                    for selected_leaf_edge in relation_edges:\n",
    "\n",
    "                        distractor_metadata = {\n",
    "                            \"edge\" : selected_leaf_edge,\n",
    "                            \"relation_old_type\" : referent_rel_map[selected_leaf_edge], # omit relations\n",
    "                            \"full_set\" : full_set,\n",
    "                        }\n",
    "\n",
    "                        # now for the selected edge, we omit them!\n",
    "                        distractor_grammer_pattern = referent_grammer_pattern.split(\" \")[:-2]\n",
    "                        distractor_grammer_pattern = \" \".join(distractor_grammer_pattern) # omiting the last.\n",
    "\n",
    "                        distractor_size_map = {}\n",
    "\n",
    "                        # First, let us make copies.\n",
    "                        distractor_obj_pattern_map = {}\n",
    "                        keeping_edges = []\n",
    "                        keeping_objects = []\n",
    "                        for keeping_edge in relation_edges:\n",
    "                            if keeping_edge != selected_leaf_edge:\n",
    "                                keeping_edges.append(keeping_edge)\n",
    "                                if keeping_edge[0] not in keeping_objects:\n",
    "                                    keeping_objects.append(keeping_edge[0])\n",
    "                                if keeping_edge[1] not in keeping_objects:\n",
    "                                    keeping_objects.append(keeping_edge[1])\n",
    "                        reverse_mapping = {}\n",
    "                        distractor_obj_map = {}\n",
    "                        for i in range(0, len(keeping_objects)):\n",
    "                            # The following line is way too strict I think!\n",
    "                            # distractor_size_map[f\"$OBJ_{i}\"] = int(sampled_world[\"obj_map\"][keeping_objects[i]].size)\n",
    "                            distractor_obj_pattern_map[f\"$OBJ_{i}\"] = referent_obj_pattern_map[keeping_objects[i]]\n",
    "                            distractor_obj_map[f\"$OBJ_{i}\"] = referent_obj_map[keeping_objects[i]]\n",
    "                            reverse_mapping[keeping_objects[i]] = f\"$OBJ_{i}\"\n",
    "\n",
    "                        distractor_rel_map = OrderedDict({})\n",
    "                        for keeping_edge in keeping_edges:\n",
    "                            distractor_rel_map[reverse_mapping[keeping_edge[0]], reverse_mapping[keeping_edge[1]]] = referent_rel_map[keeping_edge]\n",
    "\n",
    "                        # We need to increment the object counters.\n",
    "                        distractors_dicts += [{\n",
    "                                                \"grammer_pattern\" : self.snap_pattern_to_referent_map(\n",
    "                                                    distractor_grammer_pattern,\n",
    "                                                    obj_base_count\n",
    "                                                ),\n",
    "                                                \"obj_pattern_map\" : self.snap_object_map_to_referent_map(\n",
    "                                                    distractor_obj_pattern_map,\n",
    "                                                    obj_base_count\n",
    "                                                ),\n",
    "                                                \"rel_map\" : self.snap_relation_map_to_referent_map(\n",
    "                                                    distractor_rel_map,\n",
    "                                                    obj_base_count\n",
    "                                                ),\n",
    "                                                \"obj_map\" : self.snap_object_map_to_referent_map(\n",
    "                                                    distractor_obj_map,\n",
    "                                                    obj_base_count\n",
    "                                                ),\n",
    "                                                \"size_map\" : self.snap_object_map_to_referent_map(\n",
    "                                                    distractor_size_map,\n",
    "                                                    obj_base_count\n",
    "                                                ),\n",
    "                                                \"distractor_metadata\" : distractor_metadata\n",
    "                                            }]\n",
    "\n",
    "                        obj_base_count += len(distractor_obj_pattern_map)\n",
    "                \n",
    "        elif len(relation_edges) == 0:\n",
    "            # If there is only a single relation,\n",
    "            # we will use the replacement method\n",
    "            # to replace this relation with a new one!\n",
    "            return []\n",
    "        elif len(relation_edges) == 1:\n",
    "            # for this, we simple sample another obj_0?\n",
    "            distractor_size_map = {}\n",
    "            distractor_obj_pattern_map = {}\n",
    "            distractor_obj_map = {}\n",
    "            distractor_rel_map = OrderedDict({})\n",
    "            \n",
    "            distractor_size_map[\"$OBJ_0\"] = int(sampled_world[\"obj_map\"][\"$OBJ_0\"].size)\n",
    "            distractor_obj_pattern_map[\"$OBJ_0\"] = referent_obj_pattern_map[\"$OBJ_0\"]\n",
    "            distractor_obj_map[\"$OBJ_0\"] = referent_obj_map[\"$OBJ_0\"]\n",
    "            distractor_grammer_pattern = referent_grammer_pattern.split(\" \")[0]\n",
    "\n",
    "            distractor_metadata = {\n",
    "                \"edge\" : \"null\",\n",
    "                \"relation_old_type\" : \"null\", # omit relations\n",
    "                \"full_set\" : True,\n",
    "            }\n",
    "            \n",
    "            distractors_dicts += [{\n",
    "                                    \"grammer_pattern\" : self.snap_pattern_to_referent_map(\n",
    "                                        distractor_grammer_pattern,\n",
    "                                        obj_base_count\n",
    "                                    ),\n",
    "                                    \"obj_pattern_map\" : self.snap_object_map_to_referent_map(\n",
    "                                        distractor_obj_pattern_map,\n",
    "                                        obj_base_count\n",
    "                                    ),\n",
    "                                    \"rel_map\" : self.snap_relation_map_to_referent_map(\n",
    "                                        distractor_rel_map,\n",
    "                                        obj_base_count\n",
    "                                    ),\n",
    "                                    \"obj_map\" : self.snap_object_map_to_referent_map(\n",
    "                                        distractor_obj_map,\n",
    "                                        obj_base_count\n",
    "                                    ),\n",
    "                                    \"size_map\" : self.snap_object_map_to_referent_map(\n",
    "                                        distractor_size_map,\n",
    "                                        obj_base_count\n",
    "                                    ),\n",
    "                                    \"distractor_metadata\" : distractor_metadata\n",
    "                                }]\n",
    "                \n",
    "        else:\n",
    "            if referent_grammer_pattern == \"$OBJ_0 ^ $OBJ_1 ^ $OBJ_2\":\n",
    "                # for this, we simple sample another obj_0?\n",
    "                distractor_size_map = {}\n",
    "                distractor_obj_pattern_map = {}\n",
    "                distractor_obj_map = {}\n",
    "                distractor_rel_map = OrderedDict({})\n",
    "\n",
    "                # distractor_size_map[\"$OBJ_0\"] = int(sampled_world[\"obj_map\"][\"$OBJ_0\"].size)\n",
    "                distractor_obj_pattern_map[\"$OBJ_0\"] = referent_obj_pattern_map[\"$OBJ_0\"]\n",
    "                distractor_obj_pattern_map[\"$OBJ_1\"] = referent_obj_pattern_map[\"$OBJ_1\"]\n",
    "                distractor_obj_map[\"$OBJ_0\"] = referent_obj_map[\"$OBJ_0\"]\n",
    "                distractor_obj_map[\"$OBJ_1\"] = referent_obj_map[\"$OBJ_1\"]\n",
    "                distractor_grammer_pattern = \"$OBJ_0 ^ $OBJ_1\"\n",
    "                distractor_rel_map[(\"$OBJ_0\", \"$OBJ_1\")] = referent_rel_map[(\"$OBJ_0\", \"$OBJ_1\")]\n",
    "                distractor_metadata = {\n",
    "                    \"edge\" : (\"$OBJ_0\", \"$OBJ_1\"),\n",
    "                    \"relation_old_type\" : referent_rel_map[(\"$OBJ_0\", \"$OBJ_1\")], # omit relations\n",
    "                    \"full_set\" : True,\n",
    "                }\n",
    "\n",
    "                distractors_dicts += [{\n",
    "                                        \"grammer_pattern\" : self.snap_pattern_to_referent_map(\n",
    "                                            distractor_grammer_pattern,\n",
    "                                            obj_base_count\n",
    "                                        ),\n",
    "                                        \"obj_pattern_map\" : self.snap_object_map_to_referent_map(\n",
    "                                            distractor_obj_pattern_map,\n",
    "                                            obj_base_count\n",
    "                                        ),\n",
    "                                        \"rel_map\" : self.snap_relation_map_to_referent_map(\n",
    "                                            distractor_rel_map,\n",
    "                                            obj_base_count\n",
    "                                        ),\n",
    "                                        \"obj_map\" : self.snap_object_map_to_referent_map(\n",
    "                                            distractor_obj_map,\n",
    "                                            obj_base_count\n",
    "                                        ),\n",
    "                                        \"size_map\" : self.snap_object_map_to_referent_map(\n",
    "                                            distractor_size_map,\n",
    "                                            obj_base_count\n",
    "                                        ),\n",
    "                                        \"distractor_metadata\" : distractor_metadata\n",
    "                                    }]\n",
    "\n",
    "            else:\n",
    "            \n",
    "                # PR: always fullset.\n",
    "                # random.shuffle(relation_edges)\n",
    "                # if full_set:\n",
    "                #     pass\n",
    "                # else:\n",
    "                #    relation_edges = relation_edges[:1] # select only the first element.\n",
    "                existing_relations = set([v for k, v in referent_rel_map.items()])\n",
    "                for selected_leaf_edge in relation_edges:\n",
    "\n",
    "                    distractor_metadata = {\n",
    "                        \"edge\" : selected_leaf_edge,\n",
    "                        \"relation_old_type\" : referent_rel_map[selected_leaf_edge], # omit relations\n",
    "                        \"full_set\" : full_set,\n",
    "                    }\n",
    "\n",
    "                    # now for the selected edge, we omit them!\n",
    "                    distractor_grammer_pattern = referent_grammer_pattern.split(\" \")[:-2]\n",
    "                    distractor_grammer_pattern = \" \".join(distractor_grammer_pattern) # omiting the last.\n",
    "\n",
    "                    distractor_size_map = {}\n",
    "\n",
    "                    # First, let us make copies.\n",
    "                    distractor_obj_pattern_map = {}\n",
    "                    keeping_edges = []\n",
    "                    keeping_objects = []\n",
    "                    for keeping_edge in relation_edges:\n",
    "                        if keeping_edge != selected_leaf_edge:\n",
    "                            keeping_edges.append(keeping_edge)\n",
    "                            if keeping_edge[0] not in keeping_objects:\n",
    "                                keeping_objects.append(keeping_edge[0])\n",
    "                            if keeping_edge[1] not in keeping_objects:\n",
    "                                keeping_objects.append(keeping_edge[1])\n",
    "                    reverse_mapping = {}\n",
    "                    distractor_obj_map = {}\n",
    "                    for i in range(0, len(keeping_objects)):\n",
    "                        # The following line is way too strict I think!\n",
    "                        # distractor_size_map[f\"$OBJ_{i}\"] = int(sampled_world[\"obj_map\"][keeping_objects[i]].size)\n",
    "                        distractor_obj_pattern_map[f\"$OBJ_{i}\"] = referent_obj_pattern_map[keeping_objects[i]]\n",
    "                        distractor_obj_map[f\"$OBJ_{i}\"] = referent_obj_map[keeping_objects[i]]\n",
    "                        reverse_mapping[keeping_objects[i]] = f\"$OBJ_{i}\"\n",
    "\n",
    "                    distractor_rel_map = OrderedDict({})\n",
    "                    for keeping_edge in keeping_edges:\n",
    "                        distractor_rel_map[reverse_mapping[keeping_edge[0]], reverse_mapping[keeping_edge[1]]] = referent_rel_map[keeping_edge]\n",
    "\n",
    "                    # We need to increment the object counters.\n",
    "                    distractors_dicts += [{\n",
    "                                            \"grammer_pattern\" : self.snap_pattern_to_referent_map(\n",
    "                                                distractor_grammer_pattern,\n",
    "                                                obj_base_count\n",
    "                                            ),\n",
    "                                            \"obj_pattern_map\" : self.snap_object_map_to_referent_map(\n",
    "                                                distractor_obj_pattern_map,\n",
    "                                                obj_base_count\n",
    "                                            ),\n",
    "                                            \"rel_map\" : self.snap_relation_map_to_referent_map(\n",
    "                                                distractor_rel_map,\n",
    "                                                obj_base_count\n",
    "                                            ),\n",
    "                                            \"obj_map\" : self.snap_object_map_to_referent_map(\n",
    "                                                distractor_obj_map,\n",
    "                                                obj_base_count\n",
    "                                            ),\n",
    "                                            \"size_map\" : self.snap_object_map_to_referent_map(\n",
    "                                                distractor_size_map,\n",
    "                                                obj_base_count\n",
    "                                            ),\n",
    "                                            \"distractor_metadata\" : distractor_metadata\n",
    "                                        }]\n",
    "\n",
    "                    obj_base_count += len(distractor_obj_pattern_map)\n",
    "\n",
    "        return distractors_dicts\n",
    "            \n",
    "            \n",
    "    def sample_distractor_grammer_by_relation(\n",
    "        self, \n",
    "        referent_grammer_pattern, \n",
    "        referent_obj_pattern_map,\n",
    "        referent_rel_map,\n",
    "        referent_obj_map, \n",
    "        sampled_world,\n",
    "        obj_base_count=0,\n",
    "        full_set=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This will select 1 relation mentioned in the command\n",
    "        and modify it to a new one. Then, sample distractors\n",
    "        based on that command (sampling step is outside of \n",
    "        this function). This function only construct the semantics\n",
    "        of distractors.\n",
    "        \"\"\"\n",
    "\n",
    "        distractors_dicts = []\n",
    "        # We first collect all the relations\n",
    "        relation_edges = []\n",
    "        for edge, relation in referent_rel_map.items():\n",
    "            relation_edges.append(edge)\n",
    "        random.shuffle(relation_edges)\n",
    "        if full_set:\n",
    "            pass\n",
    "        else:\n",
    "            relation_edges = relation_edges[:1] # select only the first element.\n",
    "        \n",
    "        existing_relations = set([v for k, v in referent_rel_map.items()])\n",
    "        # print(referent_rel_map)\n",
    "        for selected_leaf_edge in relation_edges:\n",
    "\n",
    "            # First, let us make copies.\n",
    "            distractor_grammer_pattern = copy.deepcopy(referent_grammer_pattern)\n",
    "            distractor_obj_pattern_map = copy.deepcopy(referent_obj_pattern_map)\n",
    "            distractor_rel_map = copy.deepcopy(referent_rel_map)\n",
    "            distractor_obj_map = copy.deepcopy(referent_obj_map)\n",
    "\n",
    "            # We may need to enforce the size of the distractor due to size descriptors!\n",
    "            distractor_size_map = {}\n",
    "        \n",
    "            selected_surgery = \"REL_ADJUST\" # Dummy\n",
    "\n",
    "            distractor_metadata = {\n",
    "                \"edge\" : selected_leaf_edge,\n",
    "                \"relation_old_type\" : distractor_rel_map[selected_leaf_edge]\n",
    "            }\n",
    "\n",
    "            if selected_surgery == \"REL_ADJUST\":\n",
    "                # Determine the new relation as not the same one as the current one.\n",
    "                new_rels = [\"$SAME_ROW\", \"$SAME_COLUMN\", \"$SAME_SHAPE\", \"$SAME_COLOR\", \"$SAME_SIZE\", \"$IS_INSIDE\"]\n",
    "                new_rels = set(new_rels) - existing_relations # make this very strict!\n",
    "                # There are something else do not make sense to sample!\n",
    "                # if \"$SIZE\" in distractor_obj_pattern_map[selected_leaf_edge[0]]:\n",
    "                #     new_rels -= set([\"$SAME_SIZE\"])\n",
    "                # if \"$COLOR\" in distractor_obj_pattern_map[selected_leaf_edge[0]]:\n",
    "                #     new_rels -= set([\"$SAME_COLOR\"])\n",
    "                # if \"$SHAPE\" in distractor_obj_pattern_map[selected_leaf_edge[0]]:\n",
    "                #    new_rels -= set([\"$SAME_SHAPE\"])\n",
    "                new_rel = random.choice(list(new_rels))\n",
    "                existing_relations.add(new_rel)\n",
    "                distractor_metadata[\"relation_new_type\"] = new_rel\n",
    "                distractor_rel_map[selected_leaf_edge] = new_rel\n",
    "                if new_rel == \"$IS_INSIDE\":\n",
    "                    # We can still try to keep the color and size the same.\n",
    "                    distractor_size_map[selected_leaf_edge[1]] = sampled_world[\"obj_map\"][selected_leaf_edge[1]].size\n",
    "                    distractor_obj_map[selected_leaf_edge[1]] = sampled_world[\"obj_map\"][selected_leaf_edge[1]].color + \" box\"\n",
    "                    distractor_obj_pattern_map[selected_leaf_edge[1]] = '$COLOR $SHAPE'\n",
    "                else:\n",
    "                    distractor_size_map[selected_leaf_edge[1]] = sampled_world[\"obj_map\"][selected_leaf_edge[1]].size\n",
    "                    if \"box\" in distractor_obj_map[selected_leaf_edge[1]]:\n",
    "                        # it used to box type object.\n",
    "                        distractor_size_map[selected_leaf_edge[1]] = sampled_world[\"obj_map\"][selected_leaf_edge[1]].size\n",
    "                        distractor_obj_map[selected_leaf_edge[1]] = sampled_world[\"obj_map\"][selected_leaf_edge[1]].color + \" object\"\n",
    "                        distractor_obj_pattern_map[selected_leaf_edge[1]] = '$COLOR $ABS_SHAPE'\n",
    "                    else:\n",
    "                        distractor_size_map[selected_leaf_edge[1]] = sampled_world[\"obj_map\"][selected_leaf_edge[1]].size\n",
    "                        distractor_obj_map[selected_leaf_edge[1]] = \\\n",
    "                            sampled_world[\"obj_map\"][selected_leaf_edge[1]].color + \" \" + \\\n",
    "                            sampled_world[\"obj_map\"][selected_leaf_edge[1]].shape\n",
    "                        distractor_obj_pattern_map[selected_leaf_edge[1]] = '$COLOR $SHAPE'\n",
    "            else:\n",
    "                assert False\n",
    "        \n",
    "            # We need to increment the object counters.\n",
    "            distractors_dicts += [{\n",
    "                                    \"grammer_pattern\" : self.snap_pattern_to_referent_map(\n",
    "                                        distractor_grammer_pattern,\n",
    "                                        obj_base_count\n",
    "                                    ),\n",
    "                                    \"obj_pattern_map\" : self.snap_object_map_to_referent_map(\n",
    "                                        distractor_obj_pattern_map,\n",
    "                                        obj_base_count\n",
    "                                    ),\n",
    "                                    \"rel_map\" : self.snap_relation_map_to_referent_map(\n",
    "                                        distractor_rel_map,\n",
    "                                        obj_base_count\n",
    "                                    ),\n",
    "                                    \"obj_map\" : self.snap_object_map_to_referent_map(\n",
    "                                        distractor_obj_map,\n",
    "                                        obj_base_count\n",
    "                                    ),\n",
    "                                    \"size_map\" : self.snap_object_map_to_referent_map(\n",
    "                                        distractor_size_map,\n",
    "                                        obj_base_count\n",
    "                                    ),\n",
    "                                    \"distractor_metadata\" : distractor_metadata\n",
    "                                }]\n",
    "            obj_base_count += len(distractor_obj_pattern_map)\n",
    "\n",
    "        return distractors_dicts\n",
    "\n",
    "    def sample_distractor_grammer_by_isomorphism(\n",
    "        self,\n",
    "        referent_grammer_pattern, \n",
    "        referent_obj_pattern_map,\n",
    "        referent_rel_map,\n",
    "        referent_obj_map, \n",
    "        sampled_world,\n",
    "        obj_base_count=0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This set of distractors are for learning syntax and grammers.\n",
    "        If you simply use BoW approach, it will not work because we \n",
    "        always instill confusing targets for you with isomorphism of the\n",
    "        referent graph.\n",
    "\n",
    "        For example, if the original grounded command is:\n",
    "        Go to the red square that is inside of the yellow box.\n",
    "\n",
    "        We can do a isomorphism which is\n",
    "        Go to the yellow square that is inside of the red box.\n",
    "\n",
    "        If the model is not understanding the language correctly,\n",
    "        it will not able to find the referent target correctly.\n",
    "        \"\"\"\n",
    "        # First, let us make copies.\n",
    "        distractor_grammer_pattern = copy.deepcopy(referent_grammer_pattern)\n",
    "        distractor_obj_pattern_map = copy.deepcopy(referent_obj_pattern_map)\n",
    "        distractor_rel_map = copy.deepcopy(referent_rel_map)\n",
    "        distractor_obj_map = copy.deepcopy(referent_obj_map)\n",
    "        # We may need to enforce the size of the distractor due to size descriptors!\n",
    "        distractor_size_map = {}\n",
    "\n",
    "        shufflable_objects = []\n",
    "        for obj_name, obj_str in distractor_obj_map.items():\n",
    "            if obj_name == \"$OBJ_0\":\n",
    "                continue # We need to sample distractors of object 0, thus, we keep it intact!\n",
    "            obj_descriptors = obj_str.split(\" \")\n",
    "            # if this is a box, we don't swap it\n",
    "            if \"box\" in obj_descriptors:\n",
    "                continue\n",
    "            if \"object\" in obj_descriptors:\n",
    "                # \"object\" itself is not shufflable!\n",
    "                if len(obj_descriptors) > 1:\n",
    "                    shufflable_objects.append((obj_name, obj_str))\n",
    "            else:\n",
    "                shufflable_objects.append((obj_name, obj_str))\n",
    "        if len(shufflable_objects) > 2:\n",
    "            random.shuffle(shufflable_objects)\n",
    "        shufflable_objects = shufflable_objects[:2]\n",
    "        \n",
    "        if len(shufflable_objects) == 1 or len(shufflable_objects) == 0:\n",
    "            return [] # We simply don't have enough objects to do this.\n",
    "\n",
    "        # We will shuffle attributes between two objects.\n",
    "        # We actually shuffle by looking at their relations.\n",
    "        obj_name_left = shufflable_objects[0][0]\n",
    "        obj_name_right = shufflable_objects[1][0]\n",
    "        swap_color = True\n",
    "        swap_size = False # Let us stop swapping size for now.\n",
    "        swap_shape = True\n",
    "        if (obj_name_left, obj_name_right) in distractor_rel_map.keys() or \\\n",
    "            (obj_name_right, obj_name_left) in distractor_rel_map.keys():\n",
    "            if ((obj_name_left, obj_name_right) in distractor_rel_map.keys() and \\\n",
    "                    distractor_rel_map[(obj_name_left, obj_name_right)] == \"SameColor\") or \\\n",
    "                ((obj_name_right, obj_name_left) in distractor_rel_map.keys() and \\\n",
    "                     distractor_rel_map[(obj_name_right, obj_name_left)] == \"SameColor\"):\n",
    "                swap_color = False\n",
    "            elif ((obj_name_left, obj_name_right) in distractor_rel_map.keys() and \\\n",
    "                    distractor_rel_map[(obj_name_left, obj_name_right)] == \"SameSize\") or \\\n",
    "                ((obj_name_right, obj_name_left) in distractor_rel_map.keys() and \\\n",
    "                     distractor_rel_map[(obj_name_right, obj_name_left)] == \"SameSize\"):\n",
    "                swap_size = False\n",
    "            elif ((obj_name_left, obj_name_right) in distractor_rel_map.keys() and \\\n",
    "                    distractor_rel_map[(obj_name_left, obj_name_right)] == \"SameShape\") or \\\n",
    "                ((obj_name_right, obj_name_left) in distractor_rel_map.keys() and \\\n",
    "                     distractor_rel_map[(obj_name_right, obj_name_left)] == \"SameShape\"):\n",
    "                swap_shape = False\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        size_left = self.extract_size(shufflable_objects[0][1])\n",
    "        size_right = self.extract_size(shufflable_objects[1][1])\n",
    "        color_left = self.extract_color(shufflable_objects[0][1])\n",
    "        color_right = self.extract_color(shufflable_objects[1][1])\n",
    "        shape_left = self.extract_shape(shufflable_objects[0][1])\n",
    "        shape_right = self.extract_shape(shufflable_objects[1][1])\n",
    "        \n",
    "        if size_left == \"\" and size_right == \"\":\n",
    "            swap_size = False\n",
    "        if color_left == \"\" and color_right == \"\":\n",
    "            swap_color = False\n",
    "        if shape_left == \"\" and shape_right == \"\":\n",
    "            swap_shape = False\n",
    "        if shape_left == \"box\" or shape_right == \"box\":\n",
    "            swap_shape = False\n",
    "        \n",
    "        if not swap_color and not swap_size and not swap_shape:\n",
    "            return []\n",
    "        \n",
    "        swapping_attribute = []\n",
    "        if swap_color:\n",
    "            swapping_attribute += [\"color\"]\n",
    "        if swap_size and swap_shape:\n",
    "            swapping_attribute += [\"size+shape\"]\n",
    "        if not swap_size and swap_shape:\n",
    "            swapping_attribute += [\"size+shape\"]\n",
    "        swapping_attribute = random.choice(swapping_attribute)\n",
    "\n",
    "        left_rebuild = []\n",
    "        right_rebuild = []\n",
    "        \n",
    "        size_shuffled = False\n",
    "        color_shuffled = False\n",
    "        shape_shuffled = False\n",
    "        if swapping_attribute == \"color\":\n",
    "            tmp = color_left\n",
    "            color_left = color_right\n",
    "            color_right = tmp\n",
    "            color_shuffled = True\n",
    "        elif swapping_attribute == \"shape\":\n",
    "            tmp = shape_left\n",
    "            shape_left = shape_right\n",
    "            shape_right = tmp\n",
    "            shape_shuffled = True\n",
    "        elif swapping_attribute == \"size+shape\":\n",
    "            tmp = shape_left\n",
    "            shape_left = shape_right\n",
    "            shape_right = tmp\n",
    "            shape_shuffled = True\n",
    "            \n",
    "            tmp = size_left\n",
    "            size_left = size_right\n",
    "            size_right = tmp\n",
    "            size_shuffled = True\n",
    "            \n",
    "        # We don't swap size!\n",
    "        if size_left != \"\":\n",
    "            left_rebuild.append(size_left)\n",
    "        if size_right != \"\":\n",
    "            right_rebuild.append(size_right)\n",
    "            \n",
    "        if color_left != \"\":\n",
    "            left_rebuild.append(color_left)\n",
    "        if color_right != \"\":\n",
    "            right_rebuild.append(color_right)\n",
    "\n",
    "        if shape_left != \"\":\n",
    "            left_rebuild.append(shape_left)\n",
    "        else:\n",
    "            left_rebuild.append(\"object\")\n",
    "        if shape_right != \"\":\n",
    "            right_rebuild.append(shape_right)\n",
    "        else:\n",
    "            right_rebuild.append(\"object\")\n",
    "        \n",
    "        if not color_shuffled and not shape_shuffled:\n",
    "            return []\n",
    "                \n",
    "        left_rebuild = \" \".join(left_rebuild)\n",
    "        right_rebuild = \" \".join(right_rebuild)\n",
    "        left_grammer_rebuild = self.convert_object_str_to_grammer(left_rebuild)\n",
    "        right_grammer_rebuild = self.convert_object_str_to_grammer(right_rebuild)\n",
    "        \n",
    "        # It seems like it is possible with our case\n",
    "        # You need extra cautious of you want to extend for longer logics\n",
    "        # if left_rebuild == shufflable_objects[1][1] or right_rebuild == shufflable_objects[0][1]:\n",
    "        #     return [] # we don't allow complete swap!\n",
    "        \n",
    "        distractor_obj_pattern_map[obj_name_left] = left_grammer_rebuild \n",
    "        distractor_obj_pattern_map[obj_name_right] = right_grammer_rebuild \n",
    "        distractor_obj_map[obj_name_left] = left_rebuild\n",
    "        distractor_obj_map[obj_name_right] = right_rebuild\n",
    "        \n",
    "        distractor_metadata = {\n",
    "            \"swapped_pair\" : (obj_name_left, obj_name_right),\n",
    "            \"before_pair_obj_str\" : (shufflable_objects[0][1], shufflable_objects[1][1]),\n",
    "            \"after_pair_obj_str\" : (left_rebuild, right_rebuild),\n",
    "            \"size_shuffled\" : size_shuffled,\n",
    "            \"color_shuffled\" : color_shuffled,\n",
    "            \"shape_shuffled\" : shape_shuffled\n",
    "        }\n",
    "        \n",
    "        return [{\n",
    "                    \"grammer_pattern\" : self.snap_pattern_to_referent_map(\n",
    "                        distractor_grammer_pattern,\n",
    "                        obj_base_count\n",
    "                    ),\n",
    "                    \"obj_pattern_map\" : self.snap_object_map_to_referent_map(\n",
    "                        distractor_obj_pattern_map,\n",
    "                        obj_base_count\n",
    "                    ),\n",
    "                    \"rel_map\" : self.snap_relation_map_to_referent_map(\n",
    "                        distractor_rel_map,\n",
    "                        obj_base_count\n",
    "                    ),\n",
    "                    \"obj_map\" : self.snap_object_map_to_referent_map(\n",
    "                        distractor_obj_map,\n",
    "                        obj_base_count\n",
    "                    ),\n",
    "                    \"size_map\" : self.snap_object_map_to_referent_map(\n",
    "                        distractor_size_map,\n",
    "                        obj_base_count\n",
    "                    ),\n",
    "                    \"distractor_metadata\" : [distractor_metadata]\n",
    "                }]\n",
    "\n",
    "    def sample_distractor_grammer_by_attribute(\n",
    "        self,\n",
    "        referent_grammer_pattern, \n",
    "        referent_obj_pattern_map,\n",
    "        referent_rel_map,\n",
    "        referent_obj_map, \n",
    "        sampled_world,\n",
    "        special_shape_size_bound,\n",
    "        obj_base_count=0,\n",
    "        full_set=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        We randomly select 1 object and 1 attribute\n",
    "        that exists in the command to do the attack.\n",
    "        \n",
    "        Then, for all objects if size attribute exists\n",
    "        this function is also responsible for sampling\n",
    "        dummy size distractors!\n",
    "        \"\"\"\n",
    "        # First, let us make copies.\n",
    "        distractor_grammer_pattern = copy.deepcopy(referent_grammer_pattern)\n",
    "        distractor_obj_pattern_map = copy.deepcopy(referent_obj_pattern_map)\n",
    "        distractor_rel_map = copy.deepcopy(referent_rel_map)\n",
    "        distractor_obj_map = copy.deepcopy(referent_obj_map)\n",
    "        \n",
    "        # Let us do some pre-checks\n",
    "        # If there are two boxes already\n",
    "        # Let us stop sampling boxes!\n",
    "        box_count = 0\n",
    "        for obj_name, obj in sampled_world['obj_map'].items():\n",
    "            if obj.shape == \"box\":\n",
    "                box_count += 1\n",
    "        if box_count >= 2:\n",
    "            full_set = False\n",
    "        \n",
    "        # We may need to enforce the size of the distractor due to size descriptors!\n",
    "        distractor_size_map = OrderedDict({})\n",
    "        sizing_covered = []\n",
    "        if full_set:\n",
    "            obj_pool = []\n",
    "            for obj_name, obj_grammer in referent_obj_pattern_map.items():\n",
    "                if \"$ABS_SHAPE\" in obj_grammer or \"box\" in distractor_obj_map[obj_name]:\n",
    "                    continue\n",
    "                obj_pool += [obj_name]\n",
    "            # there is a case where, we cannot do anything with this!\n",
    "            if len(obj_pool) < 1:\n",
    "                return [] # We simply don't have enough objects to do this.\n",
    "            obj_selected = random.choice(obj_pool)\n",
    "            attribute_pool = referent_obj_pattern_map[obj_selected].split(\" \")\n",
    "            attribute_pool = list(set(attribute_pool)-set([\"$ABS_SHAPE\"]))\n",
    "            attribute_selected = random.choice(attribute_pool)\n",
    "\n",
    "            distractor_metadata = {\n",
    "                \"modified_obj\" : obj_selected,\n",
    "                \"modified_attribute\" : attribute_selected,\n",
    "            }\n",
    "\n",
    "            if attribute_selected == \"$SIZE\":\n",
    "                sizing_covered.append(obj_selected)\n",
    "                obj_name = obj_selected\n",
    "                original_object_str = distractor_obj_map[obj_name]\n",
    "                obj_grammer = distractor_obj_pattern_map[obj_name]\n",
    "                original_object = sampled_world['obj_map'][obj_name]\n",
    "                original_object_size = original_object.size\n",
    "                if \"$COLOR\" in obj_grammer:\n",
    "                    special_shape = \\\n",
    "                        sampled_world['obj_map'][obj_name].color + \\\n",
    "                        \" \" + sampled_world['obj_map'][obj_name].shape\n",
    "                else:\n",
    "                    special_shape = sampled_world['obj_map'][obj_name].shape\n",
    "                \n",
    "                if \"small\" in original_object_str:\n",
    "                    distractor_size = special_shape_size_bound[special_shape][1]\n",
    "                elif \"big\" in original_object_str:\n",
    "                    distractor_size = special_shape_size_bound[special_shape][0]\n",
    "                distractor_size_map[obj_name] = distractor_size\n",
    "                distractor_shape = original_object.shape\n",
    "                tmp_name = \"\"\n",
    "                if \"$COLOR\" in obj_grammer:\n",
    "                    distractor_color = original_object.color\n",
    "                    new_object_grammer = \"$SIZE $COLOR $SHAPE\" # $SIZE is a must right?\n",
    "                    tmp_name = distractor_color + \" \" + distractor_shape\n",
    "                else:\n",
    "                    distractor_color = self.object_vocabulary.sample_color()\n",
    "                    new_object_grammer = \"$SIZE $SHAPE\"\n",
    "                    tmp_name = distractor_shape\n",
    "                if \"small\" in original_object_str:\n",
    "                    tmp_name = \"big\" + \" \" + tmp_name\n",
    "                elif \"big\" in original_object_str:\n",
    "                    tmp_name = \"small\" + \" \" + tmp_name\n",
    "                else:\n",
    "                    pass # Not Implemented\n",
    "                distractor_obj_map[obj_name] = tmp_name\n",
    "                distractor_obj_pattern_map[obj_name] = new_object_grammer\n",
    "\n",
    "                # Then, we will also consider other object sizes. Basically,\n",
    "                # we keep them the same, unless they form SameShape relation\n",
    "                # with our core object.\n",
    "                for _obj_name, _obj in sampled_world['obj_map'].items():\n",
    "                    if _obj_name != obj_name:\n",
    "                        if (_obj_name, obj_name) in referent_rel_map and \\\n",
    "                            referent_rel_map[(_obj_name, obj_name)] == \"SameSize\":\n",
    "                            distractor_size_map[_obj_name] = distractor_size\n",
    "                        elif (obj_name, _obj_name) in referent_rel_map and \\\n",
    "                            referent_rel_map[(obj_name, _obj_name)] == \"SameSize\":\n",
    "                            distractor_size_map[_obj_name] = distractor_size\n",
    "                        else:\n",
    "                            distractor_size_map[_obj_name] = _obj.size\n",
    "            elif attribute_selected == \"$COLOR\":\n",
    "                original_object_name = obj_selected\n",
    "                original_object_str = distractor_obj_map[original_object_name]\n",
    "                original_object = sampled_world['obj_map'][original_object_name]\n",
    "                new_color = self.object_vocabulary.sample_color(_exclude=[original_object.color])\n",
    "                new_object_str = new_color + \" \" + original_object.shape\n",
    "                new_object_grammer = \"$COLOR $SHAPE\"\n",
    "                distractor_obj_map[original_object_name] = new_object_str\n",
    "                distractor_obj_pattern_map[original_object_name] = new_object_grammer\n",
    "            elif attribute_selected == \"$SHAPE\":\n",
    "\n",
    "                original_object_name = obj_selected\n",
    "                original_object_str = distractor_obj_map[original_object_name]\n",
    "                original_object = sampled_world['obj_map'][original_object_name]\n",
    "                new_shape = self.object_vocabulary.sample_shape(_exclude=[original_object.shape])\n",
    "                new_object_str = original_object.color + \" \" + new_shape\n",
    "                new_object_grammer = \"$COLOR $SHAPE\"\n",
    "                distractor_obj_map[original_object_name] = new_object_str\n",
    "                distractor_obj_pattern_map[original_object_name] = new_object_grammer\n",
    "                \n",
    "            # Now for all other objects with size attribute, we need\n",
    "            # to ground them even if it is not relational.\n",
    "            base_distractor_count = len(list(distractor_obj_map.keys()))\n",
    "            for obj_name, obj_grammer in referent_obj_pattern_map.items():\n",
    "                if \"$SIZE\" in obj_grammer:\n",
    "                    if obj_name not in sizing_covered:\n",
    "                        # Just sample a single one.\n",
    "                        new_obj_name = f\"OBJ_{base_distractor_count}\"\n",
    "                        original_object_str = referent_obj_map[obj_name]\n",
    "\n",
    "                        tmp_name = \" \".join(original_object_str.split(\" \")[1:])\n",
    "                        if \"small\" in original_object_str:\n",
    "                            tmp_name = \"big\" + \" \" + tmp_name\n",
    "                        elif \"big\" in original_object_str:\n",
    "                            tmp_name = \"small\" + \" \" + tmp_name\n",
    "                        else:\n",
    "                            pass # Not Implemented\n",
    "\n",
    "                        if \"$COLOR\" in obj_grammer:\n",
    "                            special_shape = \\\n",
    "                                sampled_world['obj_map'][obj_name].color + \\\n",
    "                                \" \" + sampled_world['obj_map'][obj_name].shape\n",
    "                        else:\n",
    "                            special_shape = sampled_world['obj_map'][obj_name].shape\n",
    "                        if \"small\" in original_object_str:\n",
    "                            distractor_size = special_shape_size_bound[special_shape][1]\n",
    "                        elif \"big\" in original_object_str:\n",
    "                            distractor_size = special_shape_size_bound[special_shape][0]\n",
    "                        # the above size if the proposed size!\n",
    "\n",
    "                        # Let us iterate through the map, if there is\n",
    "                        # already a shape working as the size counterparts\n",
    "                        # we don't need it!\n",
    "                        color = self.extract_color(original_object_str)\n",
    "                        shape = self.extract_shape(original_object_str)\n",
    "                        if color != \"\" and shape != \"object\":\n",
    "                            found = False\n",
    "                            for obj_str, obj in sampled_world['obj_map'].items():\n",
    "                                if obj.color == color and obj.shape == shape:\n",
    "                                    if obj.size == distractor_size:\n",
    "                                        found = True\n",
    "                                        break\n",
    "                            if found:\n",
    "                                continue\n",
    "                        elif color != \"\" and shape == \"object\":\n",
    "                            found = False\n",
    "                            for obj_str, obj in sampled_world['obj_map'].items():\n",
    "                                if obj.color == color:\n",
    "                                    if obj.size == distractor_size:\n",
    "                                        found = True\n",
    "                                        break\n",
    "                            if found:\n",
    "                                continue\n",
    "                        elif color == \"\" and shape == \"object\":\n",
    "                            found = False\n",
    "                            for obj_str, obj in sampled_world['obj_map'].items():\n",
    "                                if obj.size == distractor_size:\n",
    "                                    found = True\n",
    "                                    break\n",
    "                            if found:\n",
    "                                continue\n",
    "                        elif color == \"\" and shape != \"object\":\n",
    "                            found = False\n",
    "                            for obj_str, obj in sampled_world['obj_map'].items():\n",
    "                                if obj.shape == shape:\n",
    "                                    if obj.size == distractor_size:\n",
    "                                        found = True\n",
    "                                        break\n",
    "                            if found:\n",
    "                                continue\n",
    "\n",
    "                        distractor_obj_map[new_obj_name] = tmp_name\n",
    "                        distractor_obj_pattern_map[new_obj_name] = obj_grammer\n",
    "                        distractor_size_map[new_obj_name] = distractor_size\n",
    "                        base_distractor_count += 1\n",
    "                \n",
    "        else:\n",
    "            # We cleanup, and simply place random objects.\n",
    "            distractor_grammer_pattern = \"DUMMY\"\n",
    "            distractor_obj_pattern_map.clear()\n",
    "            distractor_rel_map.clear()\n",
    "            distractor_obj_map.clear()\n",
    "            \n",
    "            distractor_metadata = {\n",
    "                \"modified_obj\" : None,\n",
    "                \"modified_attribute\" : None,\n",
    "            }\n",
    "            # Now for all other objects with size attribute, we need\n",
    "            # to ground them even if it is not relational.\n",
    "            base_distractor_count = len(list(distractor_obj_map.keys()))\n",
    "            for obj_name, obj_grammer in referent_obj_pattern_map.items():\n",
    "                if \"$SIZE\" in obj_grammer:\n",
    "                    if obj_name not in sizing_covered:\n",
    "                        # Just sample a single one.\n",
    "                        new_obj_name = f\"OBJ_{base_distractor_count}\"\n",
    "                        original_object_str = referent_obj_map[obj_name]\n",
    "                        \n",
    "                        tmp_name = \" \".join(original_object_str.split(\" \")[1:])\n",
    "                        if \"small\" in original_object_str:\n",
    "                            tmp_name = \"big\" + \" \" + tmp_name\n",
    "                        elif \"big\" in original_object_str:\n",
    "                            tmp_name = \"small\" + \" \" + tmp_name\n",
    "                        else:\n",
    "                            pass # Not Implemented\n",
    "\n",
    "                        if \"$COLOR\" in obj_grammer:\n",
    "                            special_shape = \\\n",
    "                                sampled_world['obj_map'][obj_name].color + \\\n",
    "                                \" \" + sampled_world['obj_map'][obj_name].shape\n",
    "                        else:\n",
    "                            special_shape = sampled_world['obj_map'][obj_name].shape\n",
    "                        \n",
    "                        # We need to be a little careful when\n",
    "                        # dealing with abstract shape object\n",
    "                        # for example, big object -> small object.\n",
    "                            \n",
    "                        if \"small\" in original_object_str:\n",
    "                            distractor_size = special_shape_size_bound[special_shape][1]\n",
    "                        elif \"big\" in original_object_str:\n",
    "                            distractor_size = special_shape_size_bound[special_shape][0]\n",
    "                        # the above size if the proposed size!\n",
    "                        \n",
    "                        # Let us iterate through the map, if there is\n",
    "                        # already a shape working as the size counterparts\n",
    "                        # we don't need it!\n",
    "                        color = self.extract_color(original_object_str)\n",
    "                        shape = self.extract_shape(original_object_str)\n",
    "                        if color != \"\" and shape != \"object\":\n",
    "                            found = False\n",
    "                            for obj_str, obj in sampled_world['obj_map'].items():\n",
    "                                if obj.color == color and obj.shape == shape:\n",
    "                                    if obj.size == distractor_size:\n",
    "                                        found = True\n",
    "                                        break\n",
    "                            if found:\n",
    "                                continue\n",
    "                        elif color != \"\" and shape == \"object\":\n",
    "                            found = False\n",
    "                            for obj_str, obj in sampled_world['obj_map'].items():\n",
    "                                if obj.color == color:\n",
    "                                    if obj.size == distractor_size:\n",
    "                                        found = True\n",
    "                                        break\n",
    "                            if found:\n",
    "                                continue\n",
    "                        elif color == \"\" and shape == \"object\":\n",
    "                            found = False\n",
    "                            for obj_str, obj in sampled_world['obj_map'].items():\n",
    "                                if obj.size == distractor_size:\n",
    "                                    found = True\n",
    "                                    break\n",
    "                            if found:\n",
    "                                continue\n",
    "                        elif color == \"\" and shape != \"object\":\n",
    "                            found = False\n",
    "                            for obj_str, obj in sampled_world['obj_map'].items():\n",
    "                                if obj.shape == shape:\n",
    "                                    if obj.size == distractor_size:\n",
    "                                        found = True\n",
    "                                        break\n",
    "                            if found:\n",
    "                                continue\n",
    "                            \n",
    "                        distractor_obj_map[new_obj_name] = tmp_name\n",
    "                        distractor_obj_pattern_map[new_obj_name] = obj_grammer\n",
    "                        distractor_size_map[new_obj_name] = distractor_size\n",
    "                        base_distractor_count += 1\n",
    "        return [{\n",
    "            \"grammer_pattern\" : self.snap_pattern_to_referent_map(\n",
    "                distractor_grammer_pattern,\n",
    "                obj_base_count\n",
    "            ),\n",
    "            \"obj_pattern_map\" : self.snap_object_map_to_referent_map(\n",
    "                distractor_obj_pattern_map,\n",
    "                obj_base_count\n",
    "            ),\n",
    "            \"rel_map\" : self.snap_relation_map_to_referent_map(\n",
    "                distractor_rel_map,\n",
    "                obj_base_count\n",
    "            ),\n",
    "            \"obj_map\" : self.snap_object_map_to_referent_map(\n",
    "                distractor_obj_map,\n",
    "                obj_base_count\n",
    "            ),\n",
    "            \"size_map\" : self.snap_object_map_to_referent_map(\n",
    "                distractor_size_map,\n",
    "                obj_base_count\n",
    "            ),\n",
    "            \"distractor_metadata\" : [distractor_metadata]\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out the vocabulary\n",
    "intransitive_verbs = [\"walk\"]\n",
    "transitive_verbs = [\"push\", \"pull\"]\n",
    "adverbs = [\"while zigzagging\", \"while spinning\", \"cautiously\", \"hesitantly\"]\n",
    "nouns = [\"circle\", \"cylinder\", \"square\", \"box\"]\n",
    "color_adjectives = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "size_adjectives = [\"big\", \"small\"]\n",
    "relative_pronouns = [\"that is\"]\n",
    "relation_clauses = [\"in the same row as\", \n",
    "                    \"in the same column as\", \n",
    "                    \"in the same color as\", \n",
    "                    \"in the same shape as\", \n",
    "                    \"in the same size as\",\n",
    "                    \"inside of\"]\n",
    "vocabulary = Vocabulary.initialize(intransitive_verbs=intransitive_verbs,\n",
    "                                   transitive_verbs=transitive_verbs, adverbs=adverbs, nouns=nouns,\n",
    "                                   color_adjectives=color_adjectives,\n",
    "                                   size_adjectives=size_adjectives, \n",
    "                                   relative_pronouns=relative_pronouns, \n",
    "                                   relation_clauses=relation_clauses)\n",
    "\n",
    "min_object_size = 1\n",
    "max_object_size = 4\n",
    "object_vocabulary = ObjectVocabulary(shapes=vocabulary.get_semantic_shapes(),\n",
    "                                     colors=vocabulary.get_semantic_colors(),\n",
    "                                     min_size=min_object_size, max_size=max_object_size)\n",
    "\n",
    "grammer = Grammer(vocabulary)\n",
    "\n",
    "simulator = Simulator(\n",
    "    object_vocabulary, vocabulary, \n",
    "    grid_size=6, \n",
    "    n_object_max=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us explore different distractor sampling!\n",
    "pattern = \"\"\n",
    "if pattern == \"p4\":\n",
    "    ReaSCAN_data_file = f\"ReaSCAN-compositional{pattern}/data-train.txt\"\n",
    "else:\n",
    "    ReaSCAN_data_file = f\"ReaSCAN-compositional{pattern}/data-compositional-splits.txt\"\n",
    "ReaSCAN_data_json = json.load(open(os.path.join(\"../../data-files-updated/\", ReaSCAN_data_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count=27\n",
      "progress=10\n",
      "count=57\n",
      "progress=20\n",
      "count=87\n",
      "progress=30\n",
      "count=117\n",
      "progress=40\n",
      "count=147\n",
      "progress=50\n",
      "count=177\n",
      "progress=60\n",
      "count=207\n",
      "progress=70\n",
      "count=237\n",
      "progress=80\n",
      "count=267\n",
      "progress=90\n",
      "count=297\n",
      "progress=100\n",
      "count=327\n",
      "progress=110\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3411b5612545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0minclude_random_distractor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mfull_relation_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 0.5 seems to work as well!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                     \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b2e752bd6155>\u001b[0m in \u001b[0;36msample_situations_from_grounded_grammer\u001b[0;34m(self, grammer_pattern, obj_pattern_map, rel_map, obj_map, root, is_plot, include_random_distractor, include_relation_distractor, include_attribute_distractor, include_isomorphism_distractor, full_relation_probability, debug)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         agent_position = self._world.sample_position_complex(\n\u001b[0;32m--> 873\u001b[0;31m                             \u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_one\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m                         )\n\u001b[1;32m    875\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_world\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_agent_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/Reason-SCAN/code/dataset/world.py\u001b[0m in \u001b[0;36msample_position_complex\u001b[0;34m(self, condition, box_size, sample_one)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_positions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mproposed_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPosition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_taken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposed_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m                 \u001b[0mfiltered_positions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposed_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msample_one\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/Reason-SCAN/code/dataset/world.py\u001b[0m in \u001b[0;36mposition_taken\u001b[0;34m(self, position, condition)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mposition_taken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPosition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mexist_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexist_cell\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/Reason-SCAN/code/dataset/gym_minigrid/minigrid.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, i, j)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "synthetic_tasks = []\n",
    "\n",
    "per_command_world_target_count = 3 # Need to increase this for some tasks!\n",
    "per_command_world_retry_max = 200\n",
    "sampled_command_count = 5000\n",
    "\n",
    "examples = ReaSCAN_data_json[\"examples\"][\"train\"]\n",
    "random.shuffle(examples)\n",
    "sampled_examples = random.sample(examples, k=sampled_command_count)\n",
    "progress = 0\n",
    "\n",
    "for example_selected in sampled_examples:\n",
    "    \n",
    "    if example_selected['grammer_pattern'] != '$OBJ_0 ^ $OBJ_1 & $OBJ_2':\n",
    "        continue\n",
    "    \n",
    "    progress += 1\n",
    "    if progress % 10 == 0:\n",
    "        print(f\"count={len(synthetic_tasks)}\")\n",
    "        print(f\"progress={progress}\")\n",
    "    if len(synthetic_tasks) > 10000:\n",
    "        break # we get enough!\n",
    "        \n",
    "    rel_map = OrderedDict({})\n",
    "    for ele in example_selected[\"relation_map\"]:\n",
    "        rel_map[tuple(ele[0])] = ele[1]\n",
    "    example_struct = {'obj_pattern_map': example_selected[\"object_pattern_map\"],\n",
    "     'rel_map': rel_map,\n",
    "     'obj_map': example_selected[\"object_expression\"],\n",
    "     'grammer_pattern': example_selected['grammer_pattern'],\n",
    "     'adverb': example_selected['adverb_in_command'],\n",
    "     'verb': example_selected['verb_in_command']}\n",
    "    \n",
    "    obj_pattern_map = example_struct[\"obj_pattern_map\"]\n",
    "    rel_map = example_struct[\"rel_map\"]\n",
    "    obj_map = example_struct[\"obj_map\"]\n",
    "    grammer_pattern = example_struct[\"grammer_pattern\"]\n",
    "    verb = example_struct[\"verb\"]\n",
    "    adverb = example_struct[\"adverb\"]\n",
    "\n",
    "    for _ in range(per_command_world_target_count):\n",
    "        for i in range(per_command_world_retry_max): # retry essentially!\n",
    "            sampled_world = simulator.sample_situations_from_grounded_grammer(\n",
    "                    copy.deepcopy(grammer_pattern), \n",
    "                    copy.deepcopy(obj_pattern_map), \n",
    "                    copy.deepcopy(rel_map), \n",
    "                    copy.deepcopy(obj_map),\n",
    "                    is_plot=False,\n",
    "                    include_relation_distractor=True, \n",
    "                    include_attribute_distractor=False, \n",
    "                    include_isomorphism_distractor=False, \n",
    "                    include_random_distractor=True,\n",
    "                    full_relation_probability=1.0, # 0.5 seems to work as well!\n",
    "                    debug=False\n",
    "                )\n",
    "            \n",
    "            # print(sampled_world)\n",
    "            # _ = simulator._world.render_simple()\n",
    "            \n",
    "            assert len(sampled_world['obj_map']) == len(simulator._world.get_current_situation().to_representation()[\"placed_objects\"])\n",
    "\n",
    "            graph = ReaSCANGraph(\n",
    "                objects=sampled_world[\"obj_map\"], \n",
    "                object_patterns=sampled_world[\"obj_pattern_map\"], \n",
    "                vocabulary=vocabulary,\n",
    "                positions=sampled_world[\"pos_map\"], \n",
    "                referred_object=sampled_world[\"referred_obj\"],\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            pattern_graph = ReaSCANGraph(\n",
    "                objects=obj_map, \n",
    "                object_patterns=None,\n",
    "                vocabulary=vocabulary,\n",
    "                relations=rel_map, \n",
    "                referred_object='$OBJ_0', \n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            potential_referent_target = graph.find_referred_object_super_fast(\n",
    "                pattern_graph, referred_object='$OBJ_0', \n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            if len(potential_referent_target) == 1 and '$OBJ_0' in potential_referent_target:\n",
    "                # test_unique_find += 1\n",
    "                # print(f\"{test_unique_find} / {i+1} unique solution find!\")\n",
    "\n",
    "                # Form the command with grounded determiners!\n",
    "                obj_determiner_map = graph.find_determiners(\n",
    "                    pattern_graph, \n",
    "                    referred_object='$OBJ_0', \n",
    "                    debug=False,\n",
    "                )\n",
    "                command_str = grammer.repre_str_command(\n",
    "                    grammer_pattern, rel_map, obj_map, \n",
    "                    obj_determiner_map, \n",
    "                    verb,\n",
    "                    adverb,\n",
    "                )\n",
    "\n",
    "                # Form the golden label for the action list!\n",
    "                is_transitive = False\n",
    "                if verb in simulator.vocabulary.get_transitive_verbs():\n",
    "                    is_transitive = True\n",
    "                # Direct walk.\n",
    "                action = \"walk\" # this is definit!\n",
    "                primitive_command = simulator.vocabulary.translate_word(action)\n",
    "                target_position = sampled_world[\"situation\"].target_object.position\n",
    "                simulator._world.go_to_position(\n",
    "                    position=target_position, manner=adverb, \n",
    "                    primitive_command=primitive_command\n",
    "                )\n",
    "                # Object actions.\n",
    "                if is_transitive:\n",
    "                    semantic_action = simulator.vocabulary.translate_word(verb)\n",
    "                    simulator._world.move_object_to_wall(action=semantic_action, manner=adverb)\n",
    "                target_commands, _ = simulator._world.get_current_observations()\n",
    "\n",
    "                has_relation_distractor = False\n",
    "                full_relation_distractor = True\n",
    "                for rel_bool in sampled_world[\"distractor_switch_map\"][\"relation\"]:\n",
    "                    if rel_bool:\n",
    "                        has_relation_distractor = True\n",
    "                    else:\n",
    "                        full_relation_distractor = False\n",
    "\n",
    "                # Save all relevant information for a task.\n",
    "                task_struct = OrderedDict({\n",
    "                    \"command\": \",\".join(command_str.split(\" \")),\n",
    "                    \"grammer_pattern\": grammer_pattern,\n",
    "                    \"meaning\": \",\".join(command_str.split(\" \")),\n",
    "                    \"derivation\": grammer_pattern,\n",
    "                    \"situation\": sampled_world[\"situation\"].to_representation(),\n",
    "                    \"target_commands\": \",\".join(target_commands),\n",
    "                    \"verb_in_command\": verb,\n",
    "                    \"adverb_in_command\": adverb,\n",
    "                    \"referred_target\": obj_map[\"$OBJ_0\"],\n",
    "                    \"object_pattern_map\": obj_pattern_map,\n",
    "                    \"relation_map\": [(k, v) for k, v in rel_map.items()],\n",
    "                    \"object_expression\": obj_map,\n",
    "                    \"n_object\": len(sampled_world[\"obj_map\"]),\n",
    "                    \"n_distractor\": len(sampled_world[\"obj_map\"])-len(obj_map),\n",
    "                    \"full_relation_distractor\": full_relation_distractor,\n",
    "                    \"has_relation_distractor\": has_relation_distractor,\n",
    "                    \"has_attribute_distractor\": sampled_world[\"distractor_switch_map\"][\"attribute\"],\n",
    "                    \"has_isomorphism_distractor\": sampled_world[\"distractor_switch_map\"][\"isomorphism\"],\n",
    "                    \"has_random_distractor\": True if sampled_world[\"n_random_distractor\"] != -1 else False,\n",
    "                    \"n_random_distractor\": sampled_world[\"n_random_distractor\"] if sampled_world[\"n_random_distractor\"] != -1 else 0,\n",
    "                    \"relation_distractor_metadata\": sampled_world[\"relation_distractor_metadata\"],\n",
    "                    \"attribute_distractor_metadata\": sampled_world[\"attribute_distractor_metadata\"],\n",
    "                    \"isomorphism_distractor_metadata\": sampled_world[\"isomorphism_distractor_metadata\"],\n",
    "                    \"random_distractor_metadata\": sampled_world[\"random_distractor_metadata\"],\n",
    "                })\n",
    "                synthetic_tasks += [task_struct]\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_tasks = {\n",
    "    \"test\" : synthetic_tasks\n",
    "}\n",
    "dataset_representation = {\n",
    "    \"grid_size\": 6,\n",
    "    \"type_grammar\": \"ReaSCAN-Grammer\",\n",
    "    \"min_object_size\": 1,\n",
    "    \"max_object_size\": 4,\n",
    "    \"percentage_train\": 0.0,\n",
    "    \"examples\": synthetic_tasks,\n",
    "    \"intransitive_verbs\": intransitive_verbs,\n",
    "    \"transitive_verbs\": transitive_verbs,\n",
    "    \"adverbs\": adverbs,\n",
    "    \"nouns\": nouns,\n",
    "    \"color_adjectives\": color_adjectives,\n",
    "    \"size_adjectives\": size_adjectives,\n",
    "    \"relative_pronouns\": relative_pronouns,\n",
    "    \"relation_clauses\": relation_clauses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name = \"-f1\"\n",
    "with open(f\"../../data-files-updated/ReaSCAN-compositional{split_name}/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(dataset_representation, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unseen co-occurence of relations and objects but with seen atomic concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-14 12:20 Including pattern:= $OBJ_0 ^ $OBJ_1 & $OBJ_2...\n"
     ]
    }
   ],
   "source": [
    "# Currently, we hard-code the pattern!\n",
    "grammer_pattern = '$OBJ_0 ^ $OBJ_1 & $OBJ_2'\n",
    "logger.info(f\"Including pattern:= {grammer_pattern}...\")\n",
    "# Sampling relations\n",
    "relations = grammer.sample_object_relation_grammer(\n",
    "    '$OBJ_0', \n",
    "    grammer.build_dependency_graph(grammer_pattern))\n",
    "command_structs = {}\n",
    "for relation in relations:\n",
    "    obj_pattern_map = relation[0]\n",
    "    rel_map = relation[1]\n",
    "    grammer_bindings = grammer.grounding_grammer_with_vocabulary(grammer_pattern, obj_pattern_map, rel_map)\n",
    "    for obj_map in grammer_bindings:\n",
    "        # here, we also sample the verb and adverb bindings!\n",
    "        adverb_enhance_list = vocabulary.get_adverbs()\n",
    "        adverb_enhance_list += [\"\"]\n",
    "        command_struct = OrderedDict({\n",
    "            \"obj_pattern_map\" : obj_pattern_map,\n",
    "            \"rel_map\" : rel_map,\n",
    "            \"obj_map\" : obj_map,\n",
    "            \"grammer_pattern\" : grammer_pattern,\n",
    "            \"adverb\" : random.choice(adverb_enhance_list),\n",
    "            \"verb\" : random.choice(vocabulary.get_transitive_verbs() + vocabulary.get_intransitive_verbs()),\n",
    "        })\n",
    "        command_str = grammer.repre_str_command(\n",
    "            grammer_pattern, rel_map, obj_map, \n",
    "            {\"$OBJ_0\" : \"the\", \"$OBJ_1\" : \"a\", \"$OBJ_2\" : \"a\"}, \n",
    "            command_struct[\"verb\"],\n",
    "            command_struct[\"adverb\"],\n",
    "        )\n",
    "        command_structs[command_str] = command_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_command_structs = {}\n",
    "seen_concepts = {} # add in seen concepts, so we can select concepts that are seen, but new composites!\n",
    "seen_object_co = set([])\n",
    "seen_rel_co = set([])\n",
    "seen_rel_obj_co = set([])\n",
    "\n",
    "for example_selected in ReaSCAN_data_json[\"examples\"][\"train\"]:\n",
    "    rel_map = OrderedDict({})\n",
    "    for ele in example_selected[\"relation_map\"]:\n",
    "        rel_map[tuple(ele[0])] = ele[1]\n",
    "    example_struct = OrderedDict({\n",
    "        'obj_pattern_map': example_selected[\"object_pattern_map\"],\n",
    "        'rel_map': rel_map,\n",
    "        'obj_map': example_selected[\"object_expression\"],\n",
    "        'grammer_pattern': example_selected['grammer_pattern'],\n",
    "        'adverb': example_selected['adverb_in_command'],\n",
    "        'verb': example_selected['verb_in_command']\n",
    "    })\n",
    "    obj_co = []\n",
    "    for k, v in example_selected[\"object_expression\"].items():\n",
    "        if v not in seen_concepts:\n",
    "            seen_concepts[v] = 1\n",
    "        else:\n",
    "            seen_concepts[v] += 1\n",
    "        obj_co += [v]\n",
    "    obj_co.sort()\n",
    "    seen_object_co.add(tuple(obj_co))\n",
    "    \n",
    "    rel_co = []\n",
    "    for k, v in rel_map.items():\n",
    "        if v not in seen_concepts:\n",
    "            seen_concepts[v] = 1\n",
    "        else:\n",
    "            seen_concepts[v] += 1\n",
    "        rel_co += [v]\n",
    "    rel_co.sort()\n",
    "    seen_rel_co.add(tuple(rel_co))\n",
    "    \n",
    "    if example_selected['grammer_pattern'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        pair = (rel_map[(\"$OBJ_0\", \"$OBJ_1\")], example_selected[\"object_expression\"][\"$OBJ_1\"])\n",
    "        seen_rel_obj_co.add(pair)\n",
    "        pair = (rel_map[(\"$OBJ_0\", \"$OBJ_2\")], example_selected[\"object_expression\"][\"$OBJ_2\"])\n",
    "        seen_rel_obj_co.add(pair)\n",
    "    elif example_selected['grammer_pattern'] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        pair = (rel_map[(\"$OBJ_0\", \"$OBJ_1\")], example_selected[\"object_expression\"][\"$OBJ_1\"])\n",
    "        seen_rel_obj_co.add(pair)\n",
    "\n",
    "    # if example_selected['grammer_pattern'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "    command_str = grammer.repre_str_command(\n",
    "        example_selected['grammer_pattern'], rel_map, example_selected[\"object_expression\"], \n",
    "        {\"$OBJ_0\" : \"the\", \"$OBJ_1\" : \"a\", \"$OBJ_2\" : \"a\"}, \n",
    "        example_selected['verb_in_command'],\n",
    "        example_selected['adverb_in_command'],\n",
    "    )\n",
    "    seen_command_structs[command_str] = example_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_obj_co_command_structs = []\n",
    "unseen_obj_rel_co_command_structs = []\n",
    "unseen_rel_co_command_structs = []\n",
    "unseen_obj_rel_pair_command_structs = []\n",
    "for k, v in command_structs.items():\n",
    "\n",
    "    if k not in seen_command_structs:\n",
    "\n",
    "        # we need to ensure concepts are seen before though!\n",
    "        concept_seen = True\n",
    "        obj_co = []\n",
    "        for kk, vv in v['obj_map'].items():\n",
    "            if vv not in seen_concepts.keys():\n",
    "                concept_seen = False\n",
    "                break\n",
    "            obj_co += [vv]\n",
    "        \n",
    "        rel_co = []\n",
    "        for kk, vv in v['rel_map'].items():\n",
    "            if vv not in seen_concepts.keys():\n",
    "                concept_seen = False\n",
    "                break\n",
    "            rel_co += [vv]\n",
    "\n",
    "        pair = (v['rel_map'][(\"$OBJ_0\", \"$OBJ_1\")], v[\"obj_map\"][\"$OBJ_1\"])\n",
    "        rel_obj_co_1 = pair\n",
    "        pair = (v['rel_map'][(\"$OBJ_0\", \"$OBJ_2\")], v[\"obj_map\"][\"$OBJ_2\"])\n",
    "        rel_obj_co_2 = pair\n",
    "\n",
    "        if concept_seen:\n",
    "            obj_co.sort()\n",
    "            obj_co = tuple(obj_co)\n",
    "            rel_co.sort()\n",
    "            rel_co = tuple(rel_co)\n",
    "            if rel_co not in seen_rel_co and obj_co in seen_object_co:\n",
    "                unseen_rel_co_command_structs += [v]\n",
    "            if obj_co not in seen_object_co and rel_co in seen_rel_co:\n",
    "                unseen_obj_co_command_structs += [v]\n",
    "            if obj_co not in seen_object_co and rel_co not in seen_rel_co:\n",
    "                unseen_obj_rel_co_command_structs += [v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count=147\n",
      "progress=50\n",
      "count=297\n",
      "progress=100\n",
      "count=447\n",
      "progress=150\n",
      "count=597\n",
      "progress=200\n",
      "count=747\n",
      "progress=250\n",
      "count=897\n",
      "progress=300\n",
      "count=1047\n",
      "progress=350\n",
      "count=1197\n",
      "progress=400\n",
      "count=1347\n",
      "progress=450\n",
      "count=1497\n",
      "progress=500\n",
      "count=1647\n",
      "progress=550\n",
      "count=1797\n",
      "progress=600\n",
      "count=1947\n",
      "progress=650\n",
      "count=2097\n",
      "progress=700\n",
      "count=2247\n",
      "progress=750\n",
      "count=2397\n",
      "progress=800\n",
      "count=2547\n",
      "progress=850\n",
      "count=2697\n",
      "progress=900\n",
      "count=2847\n",
      "progress=950\n",
      "count=2997\n",
      "progress=1000\n",
      "count=3147\n",
      "progress=1050\n",
      "count=3297\n",
      "progress=1100\n",
      "count=3447\n",
      "progress=1150\n",
      "count=3597\n",
      "progress=1200\n",
      "count=3747\n",
      "progress=1250\n",
      "count=3897\n",
      "progress=1300\n",
      "count=4047\n",
      "progress=1350\n",
      "count=4197\n",
      "progress=1400\n",
      "count=4308\n",
      "progress=1450\n",
      "count=4407\n",
      "progress=1500\n",
      "count=4533\n",
      "progress=1550\n",
      "count=4677\n",
      "progress=1600\n",
      "count=4811\n",
      "progress=1650\n",
      "count=4950\n",
      "progress=1700\n",
      "count=5088\n",
      "progress=1750\n",
      "count=5229\n",
      "progress=1800\n",
      "count=5379\n",
      "progress=1850\n",
      "count=5529\n",
      "progress=1900\n",
      "count=5676\n",
      "progress=1950\n",
      "count=5813\n",
      "progress=2000\n",
      "count=5952\n",
      "progress=2050\n",
      "count=6090\n",
      "progress=2100\n",
      "count=6228\n",
      "progress=2150\n",
      "count=6378\n",
      "progress=2200\n",
      "count=6528\n",
      "progress=2250\n",
      "count=6678\n",
      "progress=2300\n",
      "count=6828\n",
      "progress=2350\n",
      "count=6978\n",
      "progress=2400\n",
      "count=7128\n",
      "progress=2450\n",
      "count=7278\n",
      "progress=2500\n",
      "count=7425\n",
      "progress=2550\n",
      "count=7557\n",
      "progress=2600\n",
      "count=7695\n",
      "progress=2650\n",
      "count=7833\n",
      "progress=2700\n",
      "count=7968\n",
      "progress=2750\n",
      "count=8097\n",
      "progress=2800\n",
      "count=8235\n",
      "progress=2850\n",
      "count=8373\n",
      "progress=2900\n",
      "count=8511\n",
      "progress=2950\n",
      "count=8658\n",
      "progress=3000\n",
      "count=8808\n",
      "progress=3050\n",
      "count=8904\n",
      "progress=3100\n",
      "count=9051\n",
      "progress=3150\n",
      "count=9201\n",
      "progress=3200\n",
      "count=9306\n",
      "progress=3250\n",
      "count=9444\n",
      "progress=3300\n",
      "count=9594\n",
      "progress=3350\n",
      "count=9744\n",
      "progress=3400\n",
      "count=9894\n",
      "progress=3450\n"
     ]
    }
   ],
   "source": [
    "synthetic_tasks = []\n",
    "progress = 0\n",
    "\n",
    "per_command_world_target_count = 3 # Need to increase this for some tasks!\n",
    "per_command_world_retry_max = 200\n",
    "sampled_command_count = 5000\n",
    "\n",
    "for example_selected in unseen_obj_co_command_structs:\n",
    "\n",
    "    progress += 1\n",
    "    if progress % 50 == 0:\n",
    "        print(f\"count={len(synthetic_tasks)}\")\n",
    "        print(f\"progress={progress}\")\n",
    "    if len(synthetic_tasks) > 10000:\n",
    "        break # we get enough!\n",
    "\n",
    "    obj_pattern_map = example_selected[\"obj_pattern_map\"]\n",
    "    rel_map = example_selected[\"rel_map\"]\n",
    "    obj_map = example_selected[\"obj_map\"]\n",
    "    grammer_pattern = example_selected[\"grammer_pattern\"]\n",
    "    verb = example_selected[\"verb\"]\n",
    "    adverb = example_selected[\"adverb\"]\n",
    "\n",
    "    for _ in range(per_command_world_target_count):\n",
    "        for i in range(per_command_world_retry_max): # retry essentially!\n",
    "            sampled_world = simulator.sample_situations_from_grounded_grammer(\n",
    "                    copy.deepcopy(grammer_pattern), \n",
    "                    copy.deepcopy(obj_pattern_map), \n",
    "                    copy.deepcopy(rel_map), \n",
    "                    copy.deepcopy(obj_map),\n",
    "                    is_plot=False,\n",
    "                    include_relation_distractor=True, \n",
    "                    include_attribute_distractor=True, \n",
    "                    include_isomorphism_distractor=True, \n",
    "                    include_random_distractor=True,\n",
    "                    full_relation_probability=1.0, # 0.5 seems to work as well!\n",
    "                    debug=False\n",
    "                )\n",
    "\n",
    "            assert len(sampled_world['obj_map']) == len(simulator._world.get_current_situation().to_representation()[\"placed_objects\"])\n",
    "\n",
    "            graph = ReaSCANGraph(\n",
    "                objects=sampled_world[\"obj_map\"], \n",
    "                object_patterns=sampled_world[\"obj_pattern_map\"], \n",
    "                vocabulary=vocabulary,\n",
    "                positions=sampled_world[\"pos_map\"], \n",
    "                referred_object=sampled_world[\"referred_obj\"],\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            pattern_graph = ReaSCANGraph(\n",
    "                objects=obj_map, \n",
    "                object_patterns=None,\n",
    "                vocabulary=vocabulary,\n",
    "                relations=rel_map, \n",
    "                referred_object='$OBJ_0', \n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            potential_referent_target = graph.find_referred_object_super_fast(\n",
    "                pattern_graph, referred_object='$OBJ_0', \n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            if len(potential_referent_target) == 1 and '$OBJ_0' in potential_referent_target:\n",
    "                # print(f\"{test_unique_find} / {i+1} unique solution find!\")\n",
    "                # Form the command with grounded determiners!\n",
    "                obj_determiner_map = graph.find_determiners(\n",
    "                    pattern_graph, \n",
    "                    referred_object='$OBJ_0', \n",
    "                    debug=False,\n",
    "                )\n",
    "                command_str = grammer.repre_str_command(\n",
    "                    grammer_pattern, rel_map, obj_map, \n",
    "                    obj_determiner_map, \n",
    "                    verb,\n",
    "                    adverb,\n",
    "                )\n",
    "\n",
    "                # Form the golden label for the action list!\n",
    "                is_transitive = False\n",
    "                if verb in simulator.vocabulary.get_transitive_verbs():\n",
    "                    is_transitive = True\n",
    "                # Direct walk.\n",
    "                action = \"walk\" # this is definit!\n",
    "                primitive_command = simulator.vocabulary.translate_word(action)\n",
    "                target_position = sampled_world[\"situation\"].target_object.position\n",
    "                simulator._world.go_to_position(\n",
    "                    position=target_position, manner=adverb, \n",
    "                    primitive_command=primitive_command\n",
    "                )\n",
    "                # Object actions.\n",
    "                if is_transitive:\n",
    "                    semantic_action = simulator.vocabulary.translate_word(verb)\n",
    "                    simulator._world.move_object_to_wall(action=semantic_action, manner=adverb)\n",
    "                target_commands, _ = simulator._world.get_current_observations()\n",
    "\n",
    "                has_relation_distractor = False\n",
    "                full_relation_distractor = True\n",
    "                for rel_bool in sampled_world[\"distractor_switch_map\"][\"relation\"]:\n",
    "                    if rel_bool:\n",
    "                        has_relation_distractor = True\n",
    "                    else:\n",
    "                        full_relation_distractor = False\n",
    "\n",
    "                # Save all relevant information for a task.\n",
    "                task_struct = OrderedDict({\n",
    "                    \"command\": \",\".join(command_str.split(\" \")),\n",
    "                    \"grammer_pattern\": grammer_pattern,\n",
    "                    \"meaning\": \",\".join(command_str.split(\" \")),\n",
    "                    \"derivation\": grammer_pattern,\n",
    "                    \"situation\": sampled_world[\"situation\"].to_representation(),\n",
    "                    \"target_commands\": \",\".join(target_commands),\n",
    "                    \"verb_in_command\": verb,\n",
    "                    \"adverb_in_command\": adverb,\n",
    "                    \"referred_target\": obj_map[\"$OBJ_0\"],\n",
    "                    \"object_pattern_map\": obj_pattern_map,\n",
    "                    \"relation_map\": [(k, v) for k, v in rel_map.items()],\n",
    "                    \"object_expression\": obj_map,\n",
    "                    \"n_object\": len(sampled_world[\"obj_map\"]),\n",
    "                    \"n_distractor\": len(sampled_world[\"obj_map\"])-len(obj_map),\n",
    "                    \"full_relation_distractor\": full_relation_distractor,\n",
    "                    \"has_relation_distractor\": has_relation_distractor,\n",
    "                    \"has_attribute_distractor\": sampled_world[\"distractor_switch_map\"][\"attribute\"],\n",
    "                    \"has_isomorphism_distractor\": sampled_world[\"distractor_switch_map\"][\"isomorphism\"],\n",
    "                    \"has_random_distractor\": True if sampled_world[\"n_random_distractor\"] != -1 else False,\n",
    "                    \"n_random_distractor\": sampled_world[\"n_random_distractor\"] if sampled_world[\"n_random_distractor\"] != -1 else 0,\n",
    "                    \"relation_distractor_metadata\": sampled_world[\"relation_distractor_metadata\"],\n",
    "                    \"attribute_distractor_metadata\": sampled_world[\"attribute_distractor_metadata\"],\n",
    "                    \"isomorphism_distractor_metadata\": sampled_world[\"isomorphism_distractor_metadata\"],\n",
    "                    \"random_distractor_metadata\": sampled_world[\"random_distractor_metadata\"],\n",
    "                })\n",
    "                synthetic_tasks += [task_struct]\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_tasks = {\n",
    "    \"test\" : synthetic_tasks\n",
    "}\n",
    "dataset_representation = {\n",
    "    \"grid_size\": 6,\n",
    "    \"type_grammar\": \"ReaSCAN-Grammer\",\n",
    "    \"min_object_size\": 1,\n",
    "    \"max_object_size\": 4,\n",
    "    \"percentage_train\": 0.0,\n",
    "    \"examples\": synthetic_tasks,\n",
    "    \"intransitive_verbs\": intransitive_verbs,\n",
    "    \"transitive_verbs\": transitive_verbs,\n",
    "    \"adverbs\": adverbs,\n",
    "    \"nouns\": nouns,\n",
    "    \"color_adjectives\": color_adjectives,\n",
    "    \"size_adjectives\": size_adjectives,\n",
    "    \"relative_pronouns\": relative_pronouns,\n",
    "    \"relation_clauses\": relation_clauses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name = \"-d2\"\n",
    "with open(f\"../../data-files-updated/ReaSCAN-compositional{split_name}/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(dataset_representation, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two \"that is\" clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_command_structs = []\n",
    "for example_selected in ReaSCAN_data_json[\"examples\"][\"train\"]:\n",
    "    rel_map = OrderedDict({})\n",
    "    for ele in example_selected[\"relation_map\"]:\n",
    "        rel_map[tuple(ele[0])] = ele[1]\n",
    "    example_struct = OrderedDict({\n",
    "        'obj_pattern_map': example_selected[\"object_pattern_map\"],\n",
    "        'rel_map': rel_map,\n",
    "        'obj_map': example_selected[\"object_expression\"],\n",
    "        'grammer_pattern': example_selected['grammer_pattern'], # force it to be double recursive\n",
    "        'adverb': example_selected['adverb_in_command'],\n",
    "        'verb': example_selected['verb_in_command']\n",
    "    })\n",
    "    \n",
    "    # the second object cannot be box!\n",
    "    if example_struct['grammer_pattern'] == '$OBJ_0 ^ $OBJ_1 & $OBJ_2':\n",
    "        if \"box\" not in example_struct[\"obj_map\"]['$OBJ_1']:\n",
    "            \n",
    "            # other filters as well!\n",
    "            if rel_map[(\"$OBJ_0\", \"$OBJ_1\")] in [\"$SAME_ROW\", \"$SAME_COLUMN\"] and \\\n",
    "                rel_map[(\"$OBJ_0\", \"$OBJ_2\")] in [\"$SAME_ROW\", \"$SAME_COLUMN\"]:\n",
    "                example_struct['grammer_pattern'] = '$OBJ_0 ^ $OBJ_1 ^ $OBJ_2'\n",
    "                rel_map_new = OrderedDict({})\n",
    "                rel_map_new[(\"$OBJ_0\", \"$OBJ_1\")] = example_struct['rel_map'][(\"$OBJ_0\", \"$OBJ_1\")]\n",
    "                rel_map_new[(\"$OBJ_1\", \"$OBJ_2\")] = example_struct['rel_map'][(\"$OBJ_0\", \"$OBJ_2\")]\n",
    "                example_struct['rel_map'] = rel_map_new\n",
    "                possible_command_structs += [example_struct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count=27\n",
      "progress=10\n",
      "count=57\n",
      "progress=20\n",
      "count=87\n",
      "progress=30\n",
      "count=116\n",
      "progress=40\n",
      "count=146\n",
      "progress=50\n",
      "count=172\n",
      "progress=60\n",
      "count=200\n",
      "progress=70\n",
      "count=230\n",
      "progress=80\n",
      "count=259\n",
      "progress=90\n",
      "count=287\n",
      "progress=100\n",
      "count=317\n",
      "progress=110\n",
      "count=347\n",
      "progress=120\n",
      "count=377\n",
      "progress=130\n",
      "count=407\n",
      "progress=140\n",
      "count=435\n",
      "progress=150\n",
      "count=465\n",
      "progress=160\n",
      "count=490\n",
      "progress=170\n",
      "count=520\n",
      "progress=180\n",
      "count=549\n",
      "progress=190\n",
      "count=579\n",
      "progress=200\n",
      "count=607\n",
      "progress=210\n",
      "count=637\n",
      "progress=220\n",
      "count=667\n",
      "progress=230\n",
      "count=696\n",
      "progress=240\n",
      "count=726\n",
      "progress=250\n",
      "count=756\n",
      "progress=260\n",
      "count=785\n",
      "progress=270\n",
      "count=814\n",
      "progress=280\n",
      "count=842\n",
      "progress=290\n",
      "count=872\n",
      "progress=300\n",
      "count=902\n",
      "progress=310\n",
      "count=932\n",
      "progress=320\n",
      "count=960\n",
      "progress=330\n",
      "count=990\n",
      "progress=340\n",
      "count=1020\n",
      "progress=350\n",
      "count=1050\n",
      "progress=360\n",
      "count=1076\n",
      "progress=370\n",
      "count=1106\n",
      "progress=380\n",
      "count=1136\n",
      "progress=390\n",
      "count=1166\n",
      "progress=400\n",
      "count=1196\n",
      "progress=410\n",
      "count=1226\n",
      "progress=420\n",
      "count=1253\n",
      "progress=430\n",
      "count=1283\n",
      "progress=440\n",
      "count=1313\n",
      "progress=450\n",
      "count=1343\n",
      "progress=460\n",
      "count=1373\n",
      "progress=470\n",
      "count=1403\n",
      "progress=480\n",
      "count=1430\n",
      "progress=490\n",
      "count=1460\n",
      "progress=500\n",
      "count=1490\n",
      "progress=510\n",
      "count=1520\n",
      "progress=520\n",
      "count=1549\n",
      "progress=530\n",
      "count=1579\n",
      "progress=540\n",
      "count=1609\n",
      "progress=550\n",
      "count=1639\n",
      "progress=560\n",
      "count=1667\n",
      "progress=570\n",
      "count=1696\n",
      "progress=580\n",
      "count=1725\n",
      "progress=590\n",
      "count=1755\n",
      "progress=600\n",
      "count=1785\n",
      "progress=610\n",
      "count=1815\n",
      "progress=620\n",
      "count=1845\n",
      "progress=630\n",
      "count=1875\n",
      "progress=640\n",
      "count=1905\n",
      "progress=650\n",
      "count=1935\n",
      "progress=660\n",
      "count=1965\n",
      "progress=670\n",
      "count=1995\n",
      "progress=680\n",
      "count=2025\n",
      "progress=690\n",
      "count=2055\n",
      "progress=700\n",
      "count=2085\n",
      "progress=710\n",
      "count=2115\n",
      "progress=720\n",
      "count=2144\n",
      "progress=730\n",
      "count=2173\n",
      "progress=740\n",
      "count=2203\n",
      "progress=750\n",
      "count=2230\n",
      "progress=760\n",
      "count=2260\n",
      "progress=770\n",
      "count=2290\n",
      "progress=780\n",
      "count=2320\n",
      "progress=790\n",
      "count=2348\n",
      "progress=800\n",
      "count=2378\n",
      "progress=810\n",
      "count=2408\n",
      "progress=820\n",
      "count=2438\n",
      "progress=830\n",
      "count=2467\n",
      "progress=840\n",
      "count=2497\n",
      "progress=850\n",
      "count=2524\n",
      "progress=860\n",
      "count=2552\n",
      "progress=870\n",
      "count=2580\n",
      "progress=880\n",
      "count=2607\n",
      "progress=890\n",
      "count=2636\n",
      "progress=900\n",
      "count=2666\n",
      "progress=910\n",
      "count=2695\n",
      "progress=920\n",
      "count=2724\n",
      "progress=930\n",
      "count=2754\n",
      "progress=940\n",
      "count=2784\n",
      "progress=950\n",
      "count=2814\n",
      "progress=960\n",
      "count=2844\n",
      "progress=970\n",
      "count=2874\n",
      "progress=980\n",
      "count=2903\n",
      "progress=990\n",
      "count=2930\n",
      "progress=1000\n",
      "count=2960\n",
      "progress=1010\n",
      "count=2990\n",
      "progress=1020\n",
      "count=3020\n",
      "progress=1030\n",
      "count=3046\n",
      "progress=1040\n",
      "count=3076\n",
      "progress=1050\n",
      "count=3106\n",
      "progress=1060\n",
      "count=3135\n",
      "progress=1070\n",
      "count=3165\n",
      "progress=1080\n",
      "count=3195\n",
      "progress=1090\n",
      "count=3225\n",
      "progress=1100\n",
      "count=3255\n",
      "progress=1110\n",
      "count=3283\n",
      "progress=1120\n",
      "count=3313\n",
      "progress=1130\n",
      "count=3343\n",
      "progress=1140\n",
      "count=3372\n",
      "progress=1150\n",
      "count=3401\n",
      "progress=1160\n",
      "count=3429\n",
      "progress=1170\n",
      "count=3459\n",
      "progress=1180\n",
      "count=3487\n",
      "progress=1190\n",
      "count=3517\n",
      "progress=1200\n",
      "count=3547\n",
      "progress=1210\n",
      "count=3577\n",
      "progress=1220\n",
      "count=3607\n",
      "progress=1230\n",
      "count=3635\n",
      "progress=1240\n",
      "count=3664\n",
      "progress=1250\n",
      "count=3693\n",
      "progress=1260\n",
      "count=3721\n",
      "progress=1270\n",
      "count=3749\n",
      "progress=1280\n",
      "count=3779\n",
      "progress=1290\n",
      "count=3809\n",
      "progress=1300\n",
      "count=3839\n",
      "progress=1310\n",
      "count=3868\n",
      "progress=1320\n",
      "count=3896\n",
      "progress=1330\n",
      "count=3926\n",
      "progress=1340\n",
      "count=3956\n",
      "progress=1350\n",
      "count=3986\n",
      "progress=1360\n",
      "count=4014\n",
      "progress=1370\n",
      "count=4044\n",
      "progress=1380\n",
      "count=4074\n",
      "progress=1390\n",
      "count=4102\n",
      "progress=1400\n",
      "count=4132\n",
      "progress=1410\n",
      "count=4162\n",
      "progress=1420\n",
      "count=4191\n",
      "progress=1430\n",
      "count=4221\n",
      "progress=1440\n",
      "count=4251\n",
      "progress=1450\n",
      "count=4278\n",
      "progress=1460\n",
      "count=4308\n",
      "progress=1470\n",
      "count=4337\n",
      "progress=1480\n",
      "count=4367\n",
      "progress=1490\n",
      "count=4397\n",
      "progress=1500\n",
      "count=4427\n",
      "progress=1510\n",
      "count=4457\n",
      "progress=1520\n",
      "count=4487\n",
      "progress=1530\n",
      "count=4517\n",
      "progress=1540\n",
      "count=4547\n",
      "progress=1550\n",
      "count=4577\n",
      "progress=1560\n",
      "count=4607\n",
      "progress=1570\n",
      "count=4637\n",
      "progress=1580\n",
      "count=4667\n",
      "progress=1590\n",
      "count=4697\n",
      "progress=1600\n",
      "count=4727\n",
      "progress=1610\n",
      "count=4756\n",
      "progress=1620\n",
      "count=4786\n",
      "progress=1630\n",
      "count=4816\n",
      "progress=1640\n",
      "count=4846\n",
      "progress=1650\n",
      "count=4874\n",
      "progress=1660\n",
      "count=4902\n",
      "progress=1670\n",
      "count=4932\n",
      "progress=1680\n",
      "count=4962\n",
      "progress=1690\n",
      "count=4992\n",
      "progress=1700\n",
      "count=5022\n",
      "progress=1710\n",
      "count=5052\n",
      "progress=1720\n",
      "count=5082\n",
      "progress=1730\n",
      "count=5111\n",
      "progress=1740\n",
      "count=5141\n",
      "progress=1750\n",
      "count=5171\n",
      "progress=1760\n",
      "count=5201\n",
      "progress=1770\n",
      "count=5231\n",
      "progress=1780\n",
      "count=5261\n",
      "progress=1790\n",
      "count=5291\n",
      "progress=1800\n",
      "count=5321\n",
      "progress=1810\n",
      "count=5350\n",
      "progress=1820\n",
      "count=5377\n",
      "progress=1830\n",
      "count=5407\n",
      "progress=1840\n",
      "count=5437\n",
      "progress=1850\n",
      "count=5465\n",
      "progress=1860\n",
      "count=5495\n",
      "progress=1870\n",
      "count=5525\n",
      "progress=1880\n",
      "count=5553\n",
      "progress=1890\n",
      "count=5583\n",
      "progress=1900\n",
      "count=5613\n",
      "progress=1910\n",
      "count=5643\n",
      "progress=1920\n",
      "count=5673\n",
      "progress=1930\n",
      "count=5701\n",
      "progress=1940\n",
      "count=5730\n",
      "progress=1950\n",
      "count=5760\n",
      "progress=1960\n",
      "count=5790\n",
      "progress=1970\n",
      "count=5820\n",
      "progress=1980\n",
      "count=5850\n",
      "progress=1990\n",
      "count=5880\n",
      "progress=2000\n",
      "count=5910\n",
      "progress=2010\n",
      "count=5940\n",
      "progress=2020\n",
      "count=5970\n",
      "progress=2030\n",
      "count=6000\n",
      "progress=2040\n",
      "count=6030\n",
      "progress=2050\n",
      "count=6060\n",
      "progress=2060\n",
      "count=6090\n",
      "progress=2070\n",
      "count=6120\n",
      "progress=2080\n",
      "count=6150\n",
      "progress=2090\n",
      "count=6180\n",
      "progress=2100\n",
      "count=6210\n",
      "progress=2110\n",
      "count=6240\n",
      "progress=2120\n",
      "count=6270\n",
      "progress=2130\n",
      "count=6299\n",
      "progress=2140\n",
      "count=6325\n",
      "progress=2150\n",
      "count=6355\n",
      "progress=2160\n",
      "count=6384\n",
      "progress=2170\n",
      "count=6414\n",
      "progress=2180\n",
      "count=6443\n",
      "progress=2190\n",
      "count=6473\n",
      "progress=2200\n",
      "count=6503\n",
      "progress=2210\n",
      "count=6531\n",
      "progress=2220\n",
      "count=6560\n",
      "progress=2230\n",
      "count=6590\n",
      "progress=2240\n",
      "count=6620\n",
      "progress=2250\n",
      "count=6645\n",
      "progress=2260\n",
      "count=6675\n",
      "progress=2270\n",
      "count=6705\n",
      "progress=2280\n",
      "count=6735\n",
      "progress=2290\n",
      "count=6765\n",
      "progress=2300\n",
      "count=6794\n",
      "progress=2310\n",
      "count=6823\n",
      "progress=2320\n",
      "count=6853\n",
      "progress=2330\n",
      "count=6883\n",
      "progress=2340\n",
      "count=6913\n",
      "progress=2350\n",
      "count=6943\n",
      "progress=2360\n",
      "count=6973\n",
      "progress=2370\n",
      "count=7003\n",
      "progress=2380\n",
      "count=7030\n",
      "progress=2390\n",
      "count=7059\n",
      "progress=2400\n",
      "count=7088\n",
      "progress=2410\n",
      "count=7118\n",
      "progress=2420\n",
      "count=7147\n",
      "progress=2430\n",
      "count=7175\n",
      "progress=2440\n",
      "count=7204\n",
      "progress=2450\n",
      "count=7232\n",
      "progress=2460\n",
      "count=7262\n",
      "progress=2470\n",
      "count=7292\n",
      "progress=2480\n",
      "count=7322\n",
      "progress=2490\n",
      "count=7351\n",
      "progress=2500\n",
      "count=7381\n",
      "progress=2510\n",
      "count=7411\n",
      "progress=2520\n",
      "count=7439\n",
      "progress=2530\n",
      "count=7469\n",
      "progress=2540\n",
      "count=7499\n",
      "progress=2550\n",
      "count=7529\n",
      "progress=2560\n",
      "count=7559\n",
      "progress=2570\n",
      "count=7589\n",
      "progress=2580\n",
      "count=7619\n",
      "progress=2590\n",
      "count=7649\n",
      "progress=2600\n",
      "count=7677\n",
      "progress=2610\n",
      "count=7707\n",
      "progress=2620\n",
      "count=7737\n",
      "progress=2630\n",
      "count=7767\n",
      "progress=2640\n",
      "count=7797\n",
      "progress=2650\n",
      "count=7827\n",
      "progress=2660\n",
      "count=7857\n",
      "progress=2670\n",
      "count=7886\n",
      "progress=2680\n",
      "count=7914\n",
      "progress=2690\n",
      "count=7943\n",
      "progress=2700\n",
      "count=7973\n",
      "progress=2710\n",
      "count=8000\n",
      "progress=2720\n"
     ]
    }
   ],
   "source": [
    "synthetic_tasks = []\n",
    "progress = 0\n",
    "for example_selected in possible_command_structs:\n",
    "\n",
    "    progress += 1\n",
    "    if progress % 10 == 0:\n",
    "        print(f\"count={len(synthetic_tasks)}\")\n",
    "        print(f\"progress={progress}\")\n",
    "    if len(synthetic_tasks) > 8000:\n",
    "        break # we get enough!\n",
    "        \n",
    "    obj_pattern_map = example_selected[\"obj_pattern_map\"]\n",
    "    rel_map = example_selected[\"rel_map\"]\n",
    "    obj_map = example_selected[\"obj_map\"]\n",
    "    grammer_pattern = example_selected[\"grammer_pattern\"]\n",
    "    verb = example_selected[\"verb\"]\n",
    "    adverb = example_selected[\"adverb\"]\n",
    "\n",
    "    for _ in range(per_command_world_target_count):\n",
    "        for i in range(per_command_world_retry_max): # retry essentially!\n",
    "            sampled_world = simulator.sample_situations_from_grounded_grammer(\n",
    "                    copy.deepcopy(grammer_pattern), \n",
    "                    copy.deepcopy(obj_pattern_map), \n",
    "                    copy.deepcopy(rel_map), \n",
    "                    copy.deepcopy(obj_map),\n",
    "                    is_plot=False,\n",
    "                    include_relation_distractor=True, \n",
    "                    include_attribute_distractor=True, \n",
    "                    include_isomorphism_distractor=True, \n",
    "                    include_random_distractor=True,\n",
    "                    full_relation_probability=1.0, # 0.5 seems to work as well!\n",
    "                    debug=False\n",
    "                )\n",
    "\n",
    "            assert len(sampled_world['obj_map']) == len(simulator._world.get_current_situation().to_representation()[\"placed_objects\"])\n",
    "\n",
    "            graph = ReaSCANGraph(\n",
    "                objects=sampled_world[\"obj_map\"], \n",
    "                object_patterns=sampled_world[\"obj_pattern_map\"], \n",
    "                vocabulary=vocabulary,\n",
    "                positions=sampled_world[\"pos_map\"], \n",
    "                referred_object=sampled_world[\"referred_obj\"],\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            pattern_graph = ReaSCANGraph(\n",
    "                objects=obj_map, \n",
    "                object_patterns=None,\n",
    "                vocabulary=vocabulary,\n",
    "                relations=rel_map, \n",
    "                referred_object='$OBJ_0', \n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            potential_referent_target = graph.find_referred_object_super_fast(\n",
    "                pattern_graph, referred_object='$OBJ_0', \n",
    "                pattern=grammer_pattern,\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            if len(potential_referent_target) == 1 and '$OBJ_0' in potential_referent_target:\n",
    "                # print(f\"{test_unique_find} / {i+1} unique solution find!\")\n",
    "                # Form the command with grounded determiners!\n",
    "                obj_determiner_map = graph.find_determiners(\n",
    "                    pattern_graph, \n",
    "                    referred_object='$OBJ_0', \n",
    "                    debug=False,\n",
    "                )\n",
    "                command_str = grammer.repre_str_command(\n",
    "                    grammer_pattern, rel_map, obj_map, \n",
    "                    obj_determiner_map, \n",
    "                    verb,\n",
    "                    adverb,\n",
    "                )\n",
    "\n",
    "                # Form the golden label for the action list!\n",
    "                is_transitive = False\n",
    "                if verb in simulator.vocabulary.get_transitive_verbs():\n",
    "                    is_transitive = True\n",
    "                # Direct walk.\n",
    "                action = \"walk\" # this is definit!\n",
    "                primitive_command = simulator.vocabulary.translate_word(action)\n",
    "                target_position = sampled_world[\"situation\"].target_object.position\n",
    "                simulator._world.go_to_position(\n",
    "                    position=target_position, manner=adverb, \n",
    "                    primitive_command=primitive_command\n",
    "                )\n",
    "                # Object actions.\n",
    "                if is_transitive:\n",
    "                    semantic_action = simulator.vocabulary.translate_word(verb)\n",
    "                    simulator._world.move_object_to_wall(action=semantic_action, manner=adverb)\n",
    "                target_commands, _ = simulator._world.get_current_observations()\n",
    "\n",
    "                has_relation_distractor = False\n",
    "                full_relation_distractor = True\n",
    "                for rel_bool in sampled_world[\"distractor_switch_map\"][\"relation\"]:\n",
    "                    if rel_bool:\n",
    "                        has_relation_distractor = True\n",
    "                    else:\n",
    "                        full_relation_distractor = False\n",
    "\n",
    "                # Save all relevant information for a task.\n",
    "                task_struct = OrderedDict({\n",
    "                    \"command\": \",\".join(command_str.split(\" \")),\n",
    "                    \"grammer_pattern\": grammer_pattern,\n",
    "                    \"meaning\": \",\".join(command_str.split(\" \")),\n",
    "                    \"derivation\": grammer_pattern,\n",
    "                    \"situation\": sampled_world[\"situation\"].to_representation(),\n",
    "                    \"target_commands\": \",\".join(target_commands),\n",
    "                    \"verb_in_command\": verb,\n",
    "                    \"adverb_in_command\": adverb,\n",
    "                    \"referred_target\": obj_map[\"$OBJ_0\"],\n",
    "                    \"object_pattern_map\": obj_pattern_map,\n",
    "                    \"relation_map\": [(k, v) for k, v in rel_map.items()],\n",
    "                    \"object_expression\": obj_map,\n",
    "                    \"n_object\": len(sampled_world[\"obj_map\"]),\n",
    "                    \"n_distractor\": len(sampled_world[\"obj_map\"])-len(obj_map),\n",
    "                    \"full_relation_distractor\": full_relation_distractor,\n",
    "                    \"has_relation_distractor\": has_relation_distractor,\n",
    "                    \"has_attribute_distractor\": sampled_world[\"distractor_switch_map\"][\"attribute\"],\n",
    "                    \"has_isomorphism_distractor\": sampled_world[\"distractor_switch_map\"][\"isomorphism\"],\n",
    "                    \"has_random_distractor\": True if sampled_world[\"n_random_distractor\"] != -1 else False,\n",
    "                    \"n_random_distractor\": sampled_world[\"n_random_distractor\"] if sampled_world[\"n_random_distractor\"] != -1 else 0,\n",
    "                    \"relation_distractor_metadata\": sampled_world[\"relation_distractor_metadata\"],\n",
    "                    \"attribute_distractor_metadata\": sampled_world[\"attribute_distractor_metadata\"],\n",
    "                    \"isomorphism_distractor_metadata\": sampled_world[\"isomorphism_distractor_metadata\"],\n",
    "                    \"random_distractor_metadata\": sampled_world[\"random_distractor_metadata\"],\n",
    "                })\n",
    "                synthetic_tasks += [task_struct]\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_tasks = {\n",
    "    \"test\" : synthetic_tasks\n",
    "}\n",
    "dataset_representation = {\n",
    "    \"grid_size\": 6,\n",
    "    \"type_grammar\": \"ReaSCAN-Grammer\",\n",
    "    \"min_object_size\": 1,\n",
    "    \"max_object_size\": 4,\n",
    "    \"percentage_train\": 0.0,\n",
    "    \"examples\": synthetic_tasks,\n",
    "    \"intransitive_verbs\": intransitive_verbs,\n",
    "    \"transitive_verbs\": transitive_verbs,\n",
    "    \"adverbs\": adverbs,\n",
    "    \"nouns\": nouns,\n",
    "    \"color_adjectives\": color_adjectives,\n",
    "    \"size_adjectives\": size_adjectives,\n",
    "    \"relative_pronouns\": relative_pronouns,\n",
    "    \"relation_clauses\": relation_clauses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name = \"-e\"\n",
    "with open(f\"../../data-files-updated/ReaSCAN-compositional{split_name}/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(dataset_representation, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

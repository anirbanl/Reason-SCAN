{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrips for generating splits\n",
    "This script assums you have the main ReaSCAN generated by the generate_ReaSCAN.py script. After that, you can use this file to generate/extrapolate different splits. In the future, we may consolidate two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, OrderedDict\n",
    "import os\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "if isnotebook():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO,\n",
    "                    datefmt=\"%Y-%m-%d %H:%M\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from world import *\n",
    "from vocabulary import Vocabulary as ReaSCANVocabulary\n",
    "from object_vocabulary import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1: gSCAN Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-23 22:13 Reading dataset from file: ../../data-files/ReaSCAN-compositional-p1/data-train.txt...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "121500"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1_path_to_data = \"../../data-files/ReaSCAN-compositional-p1/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p1_path_to_data}...\")\n",
    "p1_data_json = json.load(open(p1_path_to_data, \"r\"))\n",
    "\n",
    "p1_all_fake_train = p1_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p1_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p1_id_example_map = OrderedDict({})\n",
    "p1_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p1_data_json[\"examples\"][\"train\"]:\n",
    "    p1_id_example_map[index] = example\n",
    "    p1_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_splits_distribution = OrderedDict({})\n",
    "p1_splits_assignment = OrderedDict({})\n",
    "for index, splits in p1_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p1_splits_distribution.keys():\n",
    "            p1_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p1_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p1_splits_assignment:\n",
    "            p1_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p1_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p1_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p1_all_fake_train)*0.052)\n",
    "p1_all_example_id = p1_splits_assignment[\"train\"]\n",
    "random.shuffle(p1_all_example_id)\n",
    "p1_train_example_id = p1_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p1_dev_example_id = p1_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p1_test_example_id = p1_all_example_id[-gscan_dev_size:]\n",
    "p1_splits_assignment[\"train\"] = p1_train_example_id\n",
    "p1_splits_assignment[\"dev\"] = p1_dev_example_id\n",
    "p1_splits_assignment[\"test\"] = p1_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train split, we have 113967 examples.\n",
      "for dev split, we have 6318 examples.\n",
      "for test split, we have 1215 examples.\n"
     ]
    }
   ],
   "source": [
    "for split, all_ids in p1_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p1_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p1_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p1_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p1/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p1_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P2: Single Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-23 22:13 Reading dataset from file: ../../data-files/ReaSCAN-compositional-p2/data-train.txt...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "362233"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2_path_to_data = \"../../data-files/ReaSCAN-compositional-p2/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p2_path_to_data}...\")\n",
    "p2_data_json = json.load(open(p2_path_to_data, \"r\"))\n",
    "\n",
    "p2_all_fake_train = p2_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p2_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p2_id_example_map = OrderedDict({})\n",
    "p2_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p2_data_json[\"examples\"][\"train\"]:\n",
    "    p2_id_example_map[index] = example\n",
    "    p2_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_splits_distribution = OrderedDict({})\n",
    "p2_splits_assignment = OrderedDict({})\n",
    "for index, splits in p2_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p2_splits_distribution.keys():\n",
    "            p2_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p2_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p2_splits_assignment:\n",
    "            p2_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p2_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p2_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p2_all_fake_train)*0.052)\n",
    "p2_all_example_id = p2_splits_assignment[\"train\"]\n",
    "random.shuffle(p2_all_example_id)\n",
    "p2_train_example_id = p2_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p2_dev_example_id = p2_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p2_test_example_id = p2_all_example_id[-gscan_dev_size:]\n",
    "p2_splits_assignment[\"train\"] = p2_train_example_id\n",
    "p2_splits_assignment[\"dev\"] = p2_dev_example_id\n",
    "p2_splits_assignment[\"test\"] = p2_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train split, we have 339775 examples.\n",
      "for dev split, we have 18836 examples.\n",
      "for test split, we have 3622 examples.\n"
     ]
    }
   ],
   "source": [
    "for split, all_ids in p2_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p2_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p2_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p2_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p2/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p2_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3: Double Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-23 22:15 Reading dataset from file: ../../data-files/ReaSCAN-compositional-p3/data-train.txt...\n"
     ]
    }
   ],
   "source": [
    "p3_path_to_data = \"../../data-files/ReaSCAN-compositional-p3/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_path_to_data}...\")\n",
    "p3_data_json = json.load(open(p3_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531406"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3_all_fake_train = data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p3_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p3_id_example_map = OrderedDict({})\n",
    "p3_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p3_data_json[\"examples\"][\"train\"]:\n",
    "    p3_id_example_map[index] = example\n",
    "    p3_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_splits_distribution = OrderedDict({})\n",
    "p3_splits_assignment = OrderedDict({})\n",
    "for index, splits in p3_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p3_splits_distribution.keys():\n",
    "            p3_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p3_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p3_splits_assignment:\n",
    "            p3_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p3_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p3_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p3_all_fake_train)*0.052)\n",
    "p3_all_example_id = p3_splits_assignment[\"train\"]\n",
    "random.shuffle(p3_all_example_id)\n",
    "p3_train_example_id = p3_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p3_dev_example_id = p3_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p3_test_example_id = p3_all_example_id[-gscan_dev_size:]\n",
    "p3_splits_assignment[\"train\"] = p3_train_example_id\n",
    "p3_splits_assignment[\"dev\"] = p3_dev_example_id\n",
    "p3_splits_assignment[\"test\"] = p3_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train split, we have 565253 examples.\n",
      "for dev split, we have 27633 examples.\n",
      "for test split, we have 5314 examples.\n"
     ]
    }
   ],
   "source": [
    "for split, all_ids in p3_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p3_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p3_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p3_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p3/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p3_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3-RD: Double Clause with Only Random Distractors (and some contextual distractors, which are also random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-20 13:12 Reading dataset from file: ../../data-files/ReaSCAN-compositional-p3-rd/data-train.txt...\n"
     ]
    }
   ],
   "source": [
    "p3_rd_path_to_data = \"../../data-files/ReaSCAN-compositional-p3-rd/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_rd_path_to_data}...\")\n",
    "p3_rd_data_json = json.load(open(p3_rd_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607500"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3_rd_all_fake_train = p3_rd_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p3_rd_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p3_rd_id_example_map = OrderedDict({})\n",
    "p3_rd_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p3_rd_data_json[\"examples\"][\"train\"]:\n",
    "    p3_rd_id_example_map[index] = example\n",
    "    p3_rd_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_rd_splits_distribution = OrderedDict({})\n",
    "p3_rd_splits_assignment = OrderedDict({})\n",
    "for index, splits in p3_rd_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p3_rd_splits_distribution.keys():\n",
    "            p3_rd_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p3_rd_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p3_rd_splits_assignment:\n",
    "            p3_rd_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p3_rd_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p3_rd_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p3_rd_all_fake_train)*0.052)\n",
    "p3_rd_all_example_id = p3_rd_splits_assignment[\"train\"]\n",
    "random.shuffle(p3_rd_all_example_id)\n",
    "p3_rd_train_example_id = p3_rd_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p3_rd_dev_example_id = p3_rd_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p3_rd_test_example_id = p3_rd_all_example_id[-gscan_dev_size:]\n",
    "p3_rd_splits_assignment[\"train\"] = p3_rd_train_example_id\n",
    "p3_rd_splits_assignment[\"dev\"] = p3_rd_dev_example_id\n",
    "p3_rd_splits_assignment[\"test\"] = p3_rd_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train split, we have 569835 examples.\n",
      "for dev split, we have 31590 examples.\n",
      "for test split, we have 6075 examples.\n"
     ]
    }
   ],
   "source": [
    "for split, all_ids in p3_rd_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p3_rd_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p3_rd_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p3_rd_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p3-rd/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p3_rd_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1+P2+P3: Compositional Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-24 02:06 Reading dataset from file: ../../data-files/ReaSCAN-compositional-p1/data-train.txt...\n",
      "2021-05-24 02:06 Reading dataset from file: ../../data-files/ReaSCAN-compositional-p2/data-train.txt...\n",
      "2021-05-24 02:07 Reading dataset from file: ../../data-files/ReaSCAN-compositional-p3/data-train.txt...\n"
     ]
    }
   ],
   "source": [
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "p1_path_to_data = \"../../data-files/ReaSCAN-compositional-p1/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p1_path_to_data}...\")\n",
    "p1_data_json = json.load(open(p1_path_to_data, \"r\"))\n",
    "\n",
    "p2_path_to_data = \"../../data-files/ReaSCAN-compositional-p2/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p2_path_to_data}...\")\n",
    "p2_data_json = json.load(open(p2_path_to_data, \"r\"))\n",
    "\n",
    "p3_path_to_data = \"../../data-files/ReaSCAN-compositional-p3/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_path_to_data}...\")\n",
    "p3_data_json = json.load(open(p3_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them into a single big train!\n",
    "p1_examples = p1_data_json[\"examples\"][\"train\"]\n",
    "p2_examples = p2_data_json[\"examples\"][\"train\"]\n",
    "p3_data_json[\"examples\"][\"train\"].extend(p1_examples)\n",
    "p3_data_json[\"examples\"][\"train\"].extend(p2_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us downsample it to ?K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "id_example_map = OrderedDict({})\n",
    "id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in data_json[\"examples\"][\"train\"]:\n",
    "    id_example_map[index] = example\n",
    "    id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "\n",
    "    # a1_novel_color_attribute\n",
    "    if \"yellow,square\" in example['command']:\n",
    "        id_splits_map[index].add(\"a1_novel_color_attribute\")\n",
    "    \n",
    "    # a2_novel_color_attribute_visual\n",
    "    if example[\"derivation\"] == \"$OBJ_0\":\n",
    "        if \"yellow,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"yellow\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    elif example[\"derivation\"] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        if \"yellow,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"yellow\") or \\\n",
    "            (example['situation']['placed_objects']['1']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['1']['object']['color'] == \"yellow\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    elif example[\"derivation\"] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        if \"yellow,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"yellow\") or \\\n",
    "            (example['situation']['placed_objects']['1']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['1']['object']['color'] == \"yellow\") or \\\n",
    "            (example['situation']['placed_objects']['2']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['2']['object']['color'] == \"yellow\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # gscan_small_cylinder_command_target_only\n",
    "    if \"small,cylinder\" in example['command'] or \\\n",
    "        \"small,red,cylinder\" in example['command'] or \\\n",
    "        \"small,blue,cylinder\" in example['command'] or \\\n",
    "        \"small,yellow,cylinder\" in example['command'] or \\\n",
    "        \"small,green,cylinder\" in example['command']:\n",
    "        id_splits_map[index].add(\"a3_novel_size_attribute\")\n",
    "    \n",
    "    # novel_yellow_square_blue_circle_coexist_shape\n",
    "    if \"yellow,square\" in example['command'] and \"blue,circle\" in example['command']:\n",
    "        id_splits_map[index].add(\"b_novel_object_coexist\")\n",
    "\n",
    "    # novel_same_shape_is_inside_coexist_relation\n",
    "    if \"same,shape\" in example['command'] and \"is,inside\" in example['command']:\n",
    "        id_splits_map[index].add(\"c_novel_relation_coexist\")\n",
    "        \n",
    "    # novel_inside_of_as_yellow_box\n",
    "    if \"is,inside,of,a,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,a,small,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,small,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,a,big,yellow,box\" in example['command'] or \\\n",
    "        \"is,inside,of,the,big,yellow,box\" in example['command']:\n",
    "        id_splits_map[index].add(\"d_novel_object_relation_pair\")\n",
    "    \n",
    "    if example['grammer_pattern'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2 & $OBJ_3\":\n",
    "        id_splits_map[index].add(\"e_novel_clause_length\")\n",
    "    \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_distribution = OrderedDict({})\n",
    "splits_assignment = OrderedDict({})\n",
    "for index, splits in id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in splits_distribution.keys():\n",
    "            splits_distribution[split] += 1\n",
    "        else:\n",
    "            splits_distribution[split] = 1\n",
    "        \n",
    "        if split in splits_assignment:\n",
    "            splits_assignment[split].append(index)\n",
    "        else:\n",
    "            splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        for split in splits:\n",
    "            if split in splits_distribution.keys():\n",
    "                splits_distribution[split] += 1\n",
    "            else:\n",
    "                splits_distribution[split] = 1\n",
    "                \n",
    "            if split in splits_assignment:\n",
    "                splits_assignment[split].append(index)\n",
    "            else:\n",
    "                splits_assignment[split] = [index]\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "all_example_id = splits_assignment[\"train\"]\n",
    "gscan_dev_size = int(len(all_example_id)*0.01)\n",
    "gscan_test_size = int(len(all_example_id)*0.052)\n",
    "random.shuffle(all_example_id)\n",
    "train_example_id = all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "dev_example_id = all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "test_example_id = all_example_id[-gscan_dev_size:]\n",
    "splits_assignment[\"train\"] = train_example_id\n",
    "splits_assignment[\"dev\"] = dev_example_id\n",
    "splits_assignment[\"test\"] = test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train split, we have 644557 examples.\n",
      "for a2_novel_color_attribute_visual split, we have 191774 examples.\n",
      "for a3_novel_size_attribute split, we have 205334 examples.\n",
      "for d_novel_object_relation_pair split, we have 33656 examples.\n",
      "for a1_novel_color_attribute split, we have 127830 examples.\n",
      "for b_novel_object_coexist split, we have 11700 examples.\n",
      "for c_novel_relation_coexist split, we have 6291 examples.\n",
      "for dev split, we have 35732 examples.\n",
      "for test split, we have 6871 examples.\n"
     ]
    }
   ],
   "source": [
    "for split, all_ids in splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional/data-compositional-splits-all.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in splits_assignment.items():\n",
    "    if split == \"train\" or split == \"dev\" or split == \"test\":\n",
    "        updated_examples[split] = []\n",
    "        for _id in all_ids:\n",
    "            updated_examples[split].append(id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files/ReaSCAN-compositional/data-compositional-splits-train.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1+P2+P3: Compositional Splits Continue\n",
    "We need to make sure novel attribute splits actually require the attribute to reason, otherwise, it becomes less meaningful, and may cause accuracy inflation afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 14:11 Reading dataset from file: ../../data-files/ReaSCAN-compositional/data-compositional-splits-all.txt...\n"
     ]
    }
   ],
   "source": [
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "path_to_data = \"../../data-files/ReaSCAN-compositional/data-compositional-splits-all.txt\"\n",
    "logger.info(f\"Reading dataset from file: {path_to_data}...\")\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 \n",
    "a1_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a1_novel_color_attribute']:\n",
    "    if example['has_attribute_distractor']:\n",
    "        for k, v in example['object_expression'].items():\n",
    "            if \"yellow square\" in v:\n",
    "                if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$COLOR\":\n",
    "                        a1_attribute_example_filtered += [example]\n",
    "                        attribute_change += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual examples for a1 = 13339\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual examples for a1 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a2\n",
    "a2_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a2_novel_color_attribute_visual']:\n",
    "    if \"yellow,square\" in example[\"command\"]:\n",
    "        if example['has_attribute_distractor']:\n",
    "            for k, v in example['object_expression'].items():\n",
    "                if \"yellow square\" in v:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                        if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$COLOR\":\n",
    "                            a2_attribute_example_filtered += [example]\n",
    "                            attribute_change += 1\n",
    "    else:\n",
    "        # this is for the visual part, we automatically added in.\n",
    "        a2_attribute_example_filtered += [example]\n",
    "        attribute_change += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual examples for a2 = 77283\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual examples for a2 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a3\n",
    "a3_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a3_novel_size_attribute']:\n",
    "    if example['has_attribute_distractor']:\n",
    "        for k, v in example['object_expression'].items():\n",
    "            if \"small\" in v and \"cylinder\" in v:\n",
    "                if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$SIZE\":\n",
    "                        a3_attribute_example_filtered += [example]\n",
    "                        attribute_change += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual examples for a3 = 20188\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual examples for a3 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['b_novel_object_coexist']:\n",
    "    b1_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['c_novel_relation_coexist']:\n",
    "    b2_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['d_novel_object_relation_pair']:\n",
    "    b3_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a1_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-a1/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a2_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-a2/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a3_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-a3/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b1_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-b1/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b2_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-b2/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b3_attribute_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-b3/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test subsplit per command pattern!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-25 22:41 Reading dataset from file: ../../data-files/ReaSCAN-compositional/data-compositional-splits-all.txt...\n"
     ]
    }
   ],
   "source": [
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "path_to_data = \"../../data-files/ReaSCAN-compositional/data-compositional-splits-all.txt\"\n",
    "logger.info(f\"Reading dataset from file: {path_to_data}...\")\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_test_example_filtered = []\n",
    "p2_test_example_filtered = []\n",
    "p3_test_example_filtered = []\n",
    "for example in data_json[\"examples\"][\"test\"]:\n",
    "    if example['derivation'] == \"$OBJ_0\":\n",
    "        p1_test_example_filtered += [example]\n",
    "    elif example['derivation'] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        p2_test_example_filtered += [example]\n",
    "    elif example['derivation'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        p3_test_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 test example count=974\n",
      "p2 test example count=2438\n",
      "p3 test example count=3459\n"
     ]
    }
   ],
   "source": [
    "print(f\"p1 test example count={len(p1_test_example_filtered)}\")\n",
    "print(f\"p2 test example count={len(p2_test_example_filtered)}\")\n",
    "print(f\"p3 test example count={len(p3_test_example_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p1_test_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p1-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p2_test_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p2-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p3_test_example_filtered\n",
    "with open(\"../../data-files/ReaSCAN-compositional-p3-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Novel Clause Length Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
